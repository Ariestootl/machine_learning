{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 Classification Model for ISL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sign_Lang_VGG(\n",
      "  (convolutional_block_1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (convolutional_block_2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (convolutional_block_3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (convolutional_block_4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (convolutional_block_5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=8192, out_features=1000, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=1000, out_features=26, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 23934714\n"
     ]
    }
   ],
   "source": [
    "#Building Model: ISL Classification Model\n",
    "class Sign_Lang_VGG(torch.nn.Module):\n",
    "  '''\n",
    "  Built a VGG16 architecture variant for  Indian Sign Language Classification\n",
    "  The dataset: https://www.kaggle.com/datasets/prathumarikeri/indian-sign-language-isl\n",
    "  '''\n",
    "  def __init__(self,\n",
    "               input_shape: int,\n",
    "               output_shape: int):\n",
    "    super().__init__()\n",
    "    self.convolutional_block_1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=input_shape,\n",
    "                  out_channels=64,\n",
    "                  kernel_size=(3,3),\n",
    "                  stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=64,\n",
    "                  out_channels=64,\n",
    "                  kernel_size=(3,3),\n",
    "                  stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "    )\n",
    "    self.convolutional_block_2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=64,\n",
    "                  out_channels=128,\n",
    "                  kernel_size=(3,3),\n",
    "                  stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=128,\n",
    "                out_channels=128,\n",
    "                kernel_size=(3,3),\n",
    "                stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2))\n",
    "    )\n",
    "    self.convolutional_block_3 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=128,\n",
    "                  out_channels=256,\n",
    "                  kernel_size=(3,3),\n",
    "                  stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=256,\n",
    "                out_channels=256,\n",
    "                kernel_size=(3,3),\n",
    "                stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=256,\n",
    "        out_channels=256,\n",
    "        kernel_size=(3,3),\n",
    "        stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2))\n",
    "    )\n",
    "    self.convolutional_block_4 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=256,\n",
    "                  out_channels=512,\n",
    "                  kernel_size=(3,3),\n",
    "                  stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512,\n",
    "                out_channels=512,\n",
    "                kernel_size=(3,3),\n",
    "                stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512,\n",
    "        out_channels=512,\n",
    "        kernel_size=(3,3),\n",
    "        stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2))\n",
    "    )\n",
    "    self.convolutional_block_5 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=512,\n",
    "                  out_channels=512,\n",
    "                  kernel_size=(3,3),\n",
    "                  stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512,\n",
    "                out_channels=512,\n",
    "                kernel_size=(3,3),\n",
    "                stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512,\n",
    "        out_channels=512,\n",
    "        kernel_size=(3,3),\n",
    "        stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2))\n",
    "    )\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(512*4*4,\n",
    "                  1000),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1000,\n",
    "                  1000),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1000,\n",
    "                  output_shape)\n",
    "    )\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    x = self.convolutional_block_1(x)\n",
    "    x = self.convolutional_block_2(x)\n",
    "    x = self.convolutional_block_3(x)\n",
    "    x = self.convolutional_block_4(x)\n",
    "    x = self.convolutional_block_5(x)\n",
    "    x = self.classifier(x)\n",
    "    return x\n",
    "\n",
    "seed = 25\n",
    "torch.manual_seed(seed)\n",
    "model= Sign_Lang_VGG(input_shape=3,\n",
    "                  output_shape=26)\n",
    "print(f\"{model}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16 = [64, 64, \"MaxPool\", 128, 128, \"MaxPool\", 256, 256, 256, \"MaxPool\", 512, 512, 512, \"MaxPool\", 512, 512, 512, \"MaxPool\"]\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, input=3, output_shape=1000):\n",
    "        super(VGG, self).__init__()\n",
    "        self.input = input\n",
    "        self.convolutional_blocks = self.create_convolutional_blocks(VGG16)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(512*7*7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, out):\n",
    "        out = self.convolutional_blocks(out)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "    \n",
    "    def create_convolutional_blocks(self, architecture):\n",
    "        layers = []\n",
    "        input = self.input\n",
    "\n",
    "        for k in architecture:\n",
    "            if type(k) == int:\n",
    "                output = k\n",
    "\n",
    "                layers += [nn.Conv2d(input, output, \n",
    "                                     kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "                                     nn.ReLU()]\n",
    "                input = k\n",
    "            elif k == \"MaxPool\":\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n",
    "        return nn.Sequential(*layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FSL VGG13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (convolutional_blocks): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU()\n",
      "    (17): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU()\n",
      "    (20): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU()\n",
      "    (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU()\n",
      "    (27): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU()\n",
      "    (30): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU()\n",
      "    (33): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=1024, out_features=2096, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=2096, out_features=2096, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.2, inplace=False)\n",
      "    (7): Linear(in_features=2096, out_features=26, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 11903866\n"
     ]
    }
   ],
   "source": [
    "chosen_model_params = [32, 32, \"MaxPool\", 64, 64, \"MaxPool\", 128, 128, 128,\"MaxPool\", 256, 256, 256, \"MaxPool\"]\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, input=3, output_shape=1000):\n",
    "        super(VGG, self).__init__()\n",
    "        self.input = input\n",
    "        self.convolutional_blocks = self.create_convolutional_blocks(chosen_model_params)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(256*2*2, 2096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2096, 2096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2096, output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, out):\n",
    "        out = self.convolutional_blocks(out)\n",
    "        # print(f\"Shape after convolutional blocks: {out.shape}\") ### --> print this to get proper shape for the fully connected layer\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "    \n",
    "    def create_convolutional_blocks(self, architecture):\n",
    "        layers = []\n",
    "        input = self.input\n",
    "\n",
    "        for k in architecture:\n",
    "            if type(k) == int:\n",
    "                output = k\n",
    "\n",
    "                layers += [nn.Conv2d(input, output, \n",
    "                                     kernel_size=(5,5), stride=(1,1), padding=(1,1)),\n",
    "                                     nn.BatchNorm2d(k),\n",
    "                                     nn.ReLU()]\n",
    "                input = k\n",
    "            elif k == \"MaxPool\":\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "seed = 25\n",
    "torch.manual_seed(25)\n",
    "# fsl_model = VGG(input=3, output_shape=len(train_data.classes)).to(device)\n",
    "fsl_model = VGG(input=3, output_shape=26)\n",
    "print(f\"{fsl_model}\")\n",
    "\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in fsl_model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=2048, out_features=17, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 23563153\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    expansion: int = 4\n",
    "    def __init__(self, input, output, identity_downsample=None, stride=(1,1)):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input, output, kernel_size=(1,1), stride=(1,1), padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(output)\n",
    "        self.conv2 = nn.Conv2d(output, output, kernel_size=(3,3), stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(output)\n",
    "        self.conv3 = nn.Conv2d(output, output*self.expansion, \n",
    "                               kernel_size=(1,1), stride=(1,1), padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(output*self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block: Block, layers, input_shape, output_shape):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.input = 64\n",
    "        self.conv1 = nn.Conv2d(input_shape, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3))\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(3,3), stride=(2,2), padding=(1,1))\n",
    "\n",
    "        ###ResNet Layers\n",
    "        self.layer1 = self.make_layer(block,layers[0], output=64, stride=(1,1))\n",
    "        self.layer2 = self.make_layer(block,layers[1], output=128, stride=(1,1))\n",
    "        self.layer3 = self.make_layer(block,layers[2], output=256, stride=(1,1))\n",
    "        self.layer4 = self.make_layer(block,layers[3], output=512, stride=(1,1))\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Sequential( \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512*block.expansion, output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def make_layer(self, block, num_residual_MyBlocks, output, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.input != output*block.expansion:\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(\n",
    "                self.input, output * block.expansion, kernel_size=(1,1), stride=stride),\n",
    "                nn.BatchNorm2d(output*4)\n",
    "            )\n",
    "        layers.append(block(self.input, output, identity_downsample, stride))\n",
    "        self.input = output*block.expansion\n",
    "\n",
    "        for i in range(num_residual_MyBlocks-1):\n",
    "            layers.append(block(self.input, output))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "seed = 25\n",
    "torch.manual_seed(25)\n",
    "model2 = ResNet(Block, [3,4,6,3], input_shape=1, output_shape=17)\n",
    "print(f\"{model2}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in model2.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=256, out_features=17, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 1424721\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    expansion: int = 2\n",
    "\n",
    "    def __init__(self, input, output, identity_downsample=None, stride=(1,1)):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input, output, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.bn1 = nn.BatchNorm2d(output)\n",
    "        self.conv2 = nn.Conv2d(output,output*self.expansion, kernel_size=(3,3), stride=stride, padding=(1,1))\n",
    "        self.bn2 = nn.BatchNorm2d(output*self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        # print(f\"{self.expansion}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block: Block, layers, input_shape, output_shape):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.input = 16\n",
    "        self.conv1 = nn.Conv2d(input_shape, 16, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2,2), stride=1, padding=0)\n",
    "\n",
    "        self.layer1 = self.create_layers(block, layers[0], output=16, stride=1)\n",
    "        self.layer2 = self.create_layers(block, layers[1], output=32, stride=2)\n",
    "        self.layer3 = self.create_layers(block, layers[2], output=64, stride=2)\n",
    "        self.layer4 = self.create_layers(block, layers[3], output=128, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential( \n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(128 * block.expansion, output_shape)\n",
    "        )\n",
    "        # print(f\"{block.expansion}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def create_layers(self, block, num_residual_blocks, output, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.input != output * block.expansion:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.input, output * block.expansion, kernel_size=(1,1), stride=stride),\n",
    "                nn.BatchNorm2d(output * block.expansion)\n",
    "            )\n",
    "\n",
    "        layers.append(block(self.input, output, identity_downsample, stride))\n",
    "        self.input = output * block.expansion\n",
    "\n",
    "        for _ in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.input, output))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "  \n",
    "\n",
    "seed = 25\n",
    "torch.manual_seed(25)\n",
    "model2 = ResNet(Block, [2,2,2,2], input_shape=1, output_shape=17)\n",
    "print(f\"{model2}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in model2.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    expansion: int = 2\n",
    "\n",
    "    def __init__(self, input, output, identity_downsample=None, stride=(1,1)):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input, output, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.bn1 = nn.BatchNorm2d(output)\n",
    "        self.conv2 = nn.Conv2d(output,output*self.expansion, kernel_size=(3,3), stride=stride, padding=(1,1))\n",
    "        self.bn2 = nn.BatchNorm2d(output*self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class Baybayin_ResNet(nn.Module):\n",
    "    def __init__(self, block: Block, layers, input_shape, output_shape):\n",
    "        super(Baybayin_ResNet, self).__init__()\n",
    "        self.input = 16\n",
    "        self.conv1 = nn.Conv2d(input_shape, 16, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2,2), stride=1, padding=0)\n",
    "\n",
    "        self.layer1 = self.create_layers(block, layers[0], output=16, stride=1)\n",
    "        self.layer2 = self.create_layers(block, layers[1], output=32, stride=2)\n",
    "        self.layer3 = self.create_layers(block, layers[2], output=64, stride=2)\n",
    "        self.layer4 = self.create_layers(block, layers[3], output=128, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential( \n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(128 * block.expansion, output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def create_layers(self, block, num_residual_blocks, output, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.input != output * block.expansion:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.input, output * block.expansion, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(output * block.expansion)\n",
    "            )\n",
    "\n",
    "        layers.append(block(self.input, output, identity_downsample, stride))\n",
    "        self.input = output * block.expansion\n",
    "\n",
    "        for _ in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.input, output))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "  \n",
    "\n",
    "def ResNet18(img_channels=1, output_shape=10):\n",
    "    return ResNet(Block, [2, 2, 2, 2], img_channels, output_shape)\n",
    "\n",
    "# Example usage:\n",
    "model = ResNet18()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FSL ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=512, out_features=26, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 10743674\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    expansion: int = 2\n",
    "    def __init__(self, input, output, identity_downsample=None, stride=(1,1)):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input, output, kernel_size=(3,3), stride=(1,1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(output)\n",
    "        self.conv2 = nn.Conv2d(output, output*self.expansion, \n",
    "                               kernel_size=(3,3), stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(output*self.expansion,)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block: Block, layers, input_shape, output_shape):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.input = 32\n",
    "        self.conv1 = nn.Conv2d(input_shape, 32, kernel_size=(5,5), stride=(2,2), padding=(2,2))\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(3,3), stride=(2,2), padding=(1,1))\n",
    "\n",
    "        ###ResNet Layers\n",
    "        self.layer1 = self.make_layer(block,layers[0], output=32, stride=(1,1))\n",
    "        self.layer2 = self.make_layer(block,layers[1], output=64, stride=(2,2))\n",
    "        self.layer3 = self.make_layer(block,layers[2], output=128, stride=(2,2))\n",
    "        self.layer4 = self.make_layer(block,layers[3], output=256, stride=(2,2))\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Sequential( \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*block.expansion, output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def make_layer(self, block, num_residual_MyBlocks, output, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.input != output*block.expansion:\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(\n",
    "                self.input, output * block.expansion, kernel_size=(1,1), stride=stride),\n",
    "                nn.BatchNorm2d(output*block.expansion)\n",
    "            )\n",
    "        layers.append(block(self.input, output, identity_downsample, stride))\n",
    "        self.input = output*block.expansion\n",
    "\n",
    "        for i in range(num_residual_MyBlocks-1):\n",
    "            layers.append(block(self.input, output))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "seed = 25\n",
    "torch.manual_seed(25)\n",
    "# fsl_model = ResNet(Block, [3,4,6,3], input_shape=3, output_shape=26).to(device)\n",
    "fsl_model = ResNet(Block, [3,4,6,3], input_shape=3, output_shape=26)\n",
    "print(f\"{fsl_model}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in fsl_model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper codes F1, Recall, Precision, and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import ConfusionMatrix\n",
    "from torchmetrics.functional import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "def infer(data_loader:torch.utils.data.DataLoader,\n",
    "           model:torch.nn.Module,\n",
    "           test_data):\n",
    "    y_preds = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in tqdm(data_loader):\n",
    "            # Send the data to the proper target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Forward pass\n",
    "            y_logit = model(X)\n",
    "            # Turn predictions into class labels\n",
    "            y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1)\n",
    "            # Put predictions on CPU for evaluation\n",
    "            y_preds.append(y_pred.cpu())\n",
    "\n",
    "    # Concatenate list of predictions into a tensor\n",
    "    y_preds = torch.cat(y_preds)  # This line ensures y_preds is a tensor\n",
    "\n",
    "    # Ensure test_data.targets is a tensor\n",
    "    test_targets_tensor = torch.tensor(test_data.targets) if not isinstance(test_data.targets, torch.Tensor) else test_data.targets\n",
    "    return test_targets_tensor, y_preds\n",
    "\n",
    "test_targets1, y_preds1 = infer(test_dataloader, model_LION, test_data)\n",
    "\n",
    "def create_confusion_matrix(y_true, y_pred, class_names, num_classes=None, figsize=(19, 10)):\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = len(class_names)\n",
    "    \n",
    "    # Create confusion matrix instance\n",
    "    confmat = ConfusionMatrix(num_classes=num_classes, task=\"multiclass\")\n",
    "    confmat_tensor = confmat(preds=y_pred, target=y_true)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plot_confusion_matrix(\n",
    "        conf_mat=confmat_tensor.numpy(),\n",
    "        class_names=class_names,\n",
    "        figsize=figsize\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    return confmat_tensor\n",
    "\n",
    "create_confusion_matrix(test_targets1, y_preds1, class_names=train_data.classes)\n",
    "f1_score1, precision_score1, recall_score1 = calculate_metric(test_targets1.cpu(), y_preds1)\n",
    "print(f\"Print F1 score: {f1_score1}\")\n",
    "print(f\"Precision Score: {precision_score1}\")\n",
    "print(f\"Recall Score: {recall_score1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "def calculate_metric(y_true, y_pred, average=\"weighted\"): ##--> Choose 'micro', 'macro', or 'weighted'\n",
    "    # Convert logits to predicted class labels if necessary\n",
    "    if y_pred.dim() > 1:\n",
    "        y_pred = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "    # Move to CPU and convert to numpy arrays if necessary\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_true, y_pred, average=average)  \n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    \n",
    "    return f1, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN KAN\n",
    "\n",
    "Convolutional Neural Network Kolgomorov Arnold Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG_KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG_KAN(\n",
      "  (convolutional_blocks): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SELU()\n",
      "    (3): MaxPool2d(kernel_size=(4, 4), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): SELU()\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): SELU()\n",
      "    (10): MaxPool2d(kernel_size=(4, 4), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (KANClassifier): Sequential(\n",
      "    (0): NaiveFourierKANLayer()\n",
      "    (1): NaiveFourierKANLayer()\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 15302556\n"
     ]
    }
   ],
   "source": [
    "# chosen_model_params = [32, 32, \"MaxPool\", 64, 64, \"MaxPool\", 128, 128, 128,\"MaxPool\", 256, 256, 256, \"MaxPool\"]\n",
    "chosen_model_params = [64, \"MaxPool\",128, 128, \"MaxPool\"]\n",
    "# chosen_model_params = [16,16,\"MaxPool\",32,\"Maxpool\"]\n",
    "\n",
    "class NaiveFourierKANLayer(nn.Module):\n",
    "    def __init__(self, inputdim, outdim, initial_gridsize, addbias=True):\n",
    "        super(NaiveFourierKANLayer, self).__init__()\n",
    "        self.addbias = addbias\n",
    "        self.inputdim = inputdim\n",
    "        self.outdim = outdim\n",
    "        self.gridsize_param = nn.Parameter(torch.tensor(initial_gridsize, dtype=torch.float32)) #adjusted during training\n",
    "        self.fouriercoeffs = nn.Parameter(torch.empty(2, outdim, inputdim, initial_gridsize)) #adjusted during training\n",
    "        nn.init.xavier_uniform_(self.fouriercoeffs)\n",
    "        if self.addbias:\n",
    "            self.bias = nn.Parameter(torch.zeros(1, outdim))\n",
    "\n",
    "    def forward(self, x): #Combines cosine/sine terms with learnable coefficents for Fourier expansion and sums them + bias\n",
    "        gridsize = torch.clamp(self.gridsize_param, min=1).round().int()\n",
    "        outshape = x.shape[:-1] + (self.outdim,)\n",
    "        x = torch.reshape(x, (-1, self.inputdim))\n",
    "        k = torch.reshape(torch.arange(1, gridsize + 1, device=x.device), (1, 1, 1, gridsize))\n",
    "        xrshp = torch.reshape(x, (x.shape[0], 1, x.shape[1], 1))\n",
    "        c = torch.cos(k * xrshp)\n",
    "        s = torch.sin(k * xrshp)\n",
    "        y = torch.sum(c * self.fouriercoeffs[0:1, :, :, :gridsize], (-2, -1))\n",
    "        y += torch.sum(s * self.fouriercoeffs[1:2, :, :, :gridsize], (-2, -1))\n",
    "        if self.addbias:\n",
    "            y += self.bias\n",
    "        y = torch.reshape(y, outshape)\n",
    "        return y\n",
    "\n",
    "class VGG_KAN(nn.Module):\n",
    "    def __init__(self, input=3, output_shape=26):\n",
    "        super(VGG_KAN, self).__init__()\n",
    "        self.input = input\n",
    "        self.convolutional_blocks = self.create_convolutional_blocks(chosen_model_params)\n",
    "\n",
    "        self.KANClassifier = nn.Sequential(\n",
    "            NaiveFourierKANLayer(128*3*3, 256, initial_gridsize=30),\n",
    "            NaiveFourierKANLayer(256, output_shape, initial_gridsize=25)\n",
    "\n",
    "        )\n",
    "    \n",
    "    def forward(self, out):\n",
    "        out = self.convolutional_blocks(out)\n",
    "        # print(f\"Shape after convolutional blocks: {out.shape}\") ### --> print this to get proper shape for the fully connected layer\n",
    "\n",
    "        # out = self.classifier(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.KANClassifier(out)\n",
    "        return out\n",
    "    \n",
    "    def create_convolutional_blocks(self, architecture):\n",
    "        layers = []\n",
    "        input = self.input\n",
    "\n",
    "        for k in architecture:\n",
    "            if type(k) == int:\n",
    "                output = k\n",
    "\n",
    "                layers += [nn.Conv2d(input, output, \n",
    "                                     kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
    "                                     nn.BatchNorm2d(k),\n",
    "                                     nn.SELU()]\n",
    "                input = k\n",
    "            elif k == \"MaxPool\":\n",
    "                layers += [nn.MaxPool2d(kernel_size=(4,4), stride=(2,2))]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "\n",
    "torch.manual_seed(42)\n",
    "fsl_model = VGG_KAN(input=3, output_shape=26)\n",
    "print(f\"{fsl_model}\")\n",
    "\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in fsl_model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128])\n",
      "Shape after convolutional blocks: torch.Size([1, 256, 7, 7])\n",
      "tensor([[-0.0244,  0.1089, -0.3710, -0.1900, -0.3575, -0.3632,  0.3811, -0.0240,\n",
      "         -0.0849, -0.0614,  0.0439, -0.0904, -0.1795, -0.0823,  0.0301,  0.2366,\n",
      "          0.3434,  0.1383,  0.1374,  0.2822,  0.1175,  0.0741,  0.1067, -0.1370,\n",
      "          0.1333, -0.2222]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn(size=(3, 128, 128))  # Single image (C, H, W)\n",
    "print(tensor1.shape) \n",
    "\n",
    "output = fsl_model(tensor1.unsqueeze(dim=0))  # Add batch dimension\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet_KAN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (KANClassifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): NaiveFourierKANLayer()\n",
      "    (2): NaiveFourierKANLayer()\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 33217756\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NaiveFourierKANLayer(nn.Module):\n",
    "    def __init__(self, inputdim, outdim, initial_gridsize, addbias=True):\n",
    "        super(NaiveFourierKANLayer, self).__init__()\n",
    "        self.addbias = addbias\n",
    "        self.inputdim = inputdim\n",
    "        self.outdim = outdim\n",
    "        self.gridsize_param = nn.Parameter(torch.tensor(initial_gridsize, dtype=torch.float32)) #adjusted during training\n",
    "        self.fouriercoeffs = nn.Parameter(torch.empty(2, outdim, inputdim, initial_gridsize))\n",
    "        nn.init.xavier_uniform_(self.fouriercoeffs)\n",
    "        if self.addbias:\n",
    "            self.bias = nn.Parameter(torch.zeros(1, outdim))\n",
    "            nn.init.kaiming_normal_(self.bias)\n",
    "\n",
    "    def forward(self, x): #Combines cosine/sine terms with learnable coefficents for Fourier expansion and sums them + bias\n",
    "        gridsize = torch.clamp(self.gridsize_param, min=1).round().int()\n",
    "        outshape = x.shape[:-1] + (self.outdim,)\n",
    "        x = torch.reshape(x, (-1, self.inputdim))\n",
    "        k = torch.reshape(torch.arange(1, gridsize + 1, device=x.device), (1, 1, 1, gridsize))\n",
    "        xrshp = torch.reshape(x, (x.shape[0], 1, x.shape[1], 1))\n",
    "        c = torch.cos(k * xrshp)\n",
    "        s = torch.sin(k * xrshp)\n",
    "        y = torch.sum(c * self.fouriercoeffs[0:1, :, :, :gridsize], (-2, -1))\n",
    "        y += torch.sum(s * self.fouriercoeffs[1:2, :, :, :gridsize], (-2, -1))\n",
    "        if self.addbias:\n",
    "            y += self.bias\n",
    "        y = torch.reshape(y, outshape)\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    expansion: int = 2\n",
    "    def __init__(self, input, output, identity_downsample=None, stride=(1,1)):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input, output, kernel_size=(3,3), stride=(1,1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(output)\n",
    "        self.conv2 = nn.Conv2d(output, output*self.expansion, \n",
    "                               kernel_size=(3,3), stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(output*self.expansion,)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet_KAN(nn.Module):\n",
    "    def __init__(self, block: Block, layers, input_shape, output_shape):\n",
    "        super(ResNet_KAN, self).__init__()\n",
    "        self.input = 32\n",
    "        self.conv1 = nn.Conv2d(input_shape, 32, kernel_size=(5,5), stride=(2,2), padding=(2,2))\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(3,3), stride=(2,2), padding=(1,1))\n",
    "\n",
    "        ###ResNet Layers\n",
    "        self.layer1 = self.make_layer(block,layers[0], output=32, stride=(1,1))\n",
    "        self.layer2 = self.make_layer(block,layers[1], output=64, stride=(2,2))\n",
    "        self.layer3 = self.make_layer(block,layers[2], output=128, stride=(2,2))\n",
    "        self.layer4 = self.make_layer(block,layers[3], output=256, stride=(2,2))\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        self.KANClassifier = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            NaiveFourierKANLayer(256*block.expansion, 512, initial_gridsize=50),\n",
    "            NaiveFourierKANLayer(512, output_shape, initial_gridsize=50)\n",
    "\n",
    "        )\n",
    "    \n",
    "    def forward(self,x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = self.KANClassifier(x)\n",
    "        return x\n",
    "    \n",
    "    def make_layer(self, block, num_residual_MyBlocks, output, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.input != output*block.expansion:\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(\n",
    "                self.input, output * block.expansion, kernel_size=(1,1), stride=stride),\n",
    "                nn.BatchNorm2d(output*block.expansion)\n",
    "            )\n",
    "        layers.append(block(self.input, output, identity_downsample, stride))\n",
    "        self.input = output*block.expansion\n",
    "\n",
    "        for i in range(num_residual_MyBlocks-1):\n",
    "            layers.append(block(self.input, output))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(23)\n",
    "fsl_model = ResNet_KAN(Block, [2,2,2,2], input_shape=3, \n",
    "                       output_shape=26)\n",
    "print(f\"{fsl_model}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in fsl_model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View initialization for ResNet KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv1.bias, Biases: tensor([-0.0781, -0.1030,  0.0394,  0.0026,  0.1062, -0.0296, -0.0620, -0.0299,\n",
      "        -0.0952,  0.0856, -0.0479,  0.0041, -0.0407, -0.0992,  0.0846,  0.1045,\n",
      "         0.0254, -0.0170,  0.1145,  0.0511,  0.0584,  0.0230,  0.1045, -0.0673,\n",
      "         0.0402,  0.0710, -0.0604, -0.0720, -0.0469, -0.0301,  0.0789,  0.0958])\n",
      "Layer: bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer1.0.conv1.bias, Biases: tensor([-0.0253, -0.0417,  0.0310, -0.0109,  0.0068, -0.0416, -0.0042,  0.0353,\n",
      "        -0.0095,  0.0414,  0.0152, -0.0585,  0.0407, -0.0427,  0.0546, -0.0252,\n",
      "        -0.0504, -0.0239,  0.0060,  0.0002, -0.0388,  0.0135, -0.0153,  0.0252,\n",
      "        -0.0422,  0.0571,  0.0250, -0.0188,  0.0241,  0.0499,  0.0254,  0.0565])\n",
      "Layer: layer1.0.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer1.0.conv2.bias, Biases: tensor([-0.0579,  0.0131, -0.0047, -0.0491,  0.0358,  0.0515,  0.0093, -0.0347,\n",
      "         0.0532, -0.0210,  0.0334,  0.0391, -0.0239, -0.0342,  0.0304,  0.0173,\n",
      "         0.0354, -0.0463,  0.0262,  0.0068, -0.0108, -0.0028, -0.0486, -0.0160,\n",
      "        -0.0064,  0.0017,  0.0349, -0.0285,  0.0188,  0.0391,  0.0585, -0.0470,\n",
      "         0.0466, -0.0488, -0.0308, -0.0361, -0.0374, -0.0398, -0.0280,  0.0141,\n",
      "        -0.0381, -0.0566,  0.0062, -0.0500,  0.0528, -0.0118,  0.0388, -0.0364,\n",
      "         0.0509, -0.0461,  0.0070, -0.0418, -0.0471, -0.0582,  0.0225,  0.0513,\n",
      "         0.0138, -0.0566,  0.0025, -0.0369,  0.0420, -0.0038,  0.0410, -0.0106])\n",
      "Layer: layer1.0.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer1.0.identity_downsample.0.bias, Biases: tensor([-0.0742,  0.0336,  0.1620,  0.1529, -0.1516,  0.1710,  0.1039,  0.0396,\n",
      "         0.0338,  0.0468,  0.1593,  0.1536,  0.1189, -0.0048,  0.0571, -0.1432,\n",
      "         0.0149, -0.1099,  0.1181, -0.0836,  0.0252,  0.0821,  0.0654, -0.1745,\n",
      "        -0.0269,  0.1432, -0.1620, -0.0229, -0.0750,  0.0087, -0.1514,  0.0647,\n",
      "         0.1117, -0.1264,  0.0184,  0.0709,  0.1040,  0.0351,  0.1153, -0.0414,\n",
      "        -0.0411, -0.1503, -0.1630,  0.0205, -0.1548,  0.1504,  0.1192, -0.1256,\n",
      "         0.1342,  0.0394,  0.0935,  0.0851, -0.1093, -0.0744,  0.1376,  0.0165,\n",
      "        -0.0892, -0.0338,  0.1607,  0.0132, -0.0976,  0.0407, -0.1030,  0.1203])\n",
      "Layer: layer1.0.identity_downsample.1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer1.1.conv1.bias, Biases: tensor([-0.0266,  0.0210,  0.0134, -0.0048,  0.0016,  0.0128, -0.0417,  0.0320,\n",
      "         0.0017,  0.0075, -0.0193, -0.0128,  0.0138,  0.0081, -0.0154, -0.0319,\n",
      "        -0.0121,  0.0070,  0.0316, -0.0076,  0.0173,  0.0046, -0.0385, -0.0344,\n",
      "         0.0317,  0.0059,  0.0411,  0.0386,  0.0108, -0.0034,  0.0261,  0.0311])\n",
      "Layer: layer1.1.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer1.1.conv2.bias, Biases: tensor([-0.0230, -0.0269,  0.0573, -0.0188,  0.0526, -0.0209, -0.0344,  0.0115,\n",
      "         0.0080,  0.0103,  0.0523, -0.0207,  0.0360,  0.0097, -0.0547, -0.0061,\n",
      "         0.0406, -0.0423, -0.0193,  0.0417, -0.0309, -0.0193, -0.0039, -0.0168,\n",
      "        -0.0444,  0.0078, -0.0013,  0.0458,  0.0217, -0.0179,  0.0349,  0.0578,\n",
      "        -0.0228, -0.0388, -0.0059,  0.0542, -0.0270, -0.0003, -0.0353,  0.0249,\n",
      "         0.0478, -0.0053, -0.0519, -0.0241, -0.0173, -0.0248, -0.0207,  0.0095,\n",
      "         0.0462, -0.0252, -0.0262,  0.0110, -0.0176,  0.0467,  0.0110,  0.0302,\n",
      "         0.0520,  0.0561,  0.0044, -0.0354, -0.0334,  0.0280, -0.0580,  0.0201])\n",
      "Layer: layer1.1.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer1.2.conv1.bias, Biases: tensor([ 0.0063, -0.0175, -0.0415,  0.0110, -0.0025,  0.0387,  0.0397,  0.0034,\n",
      "        -0.0190,  0.0402, -0.0150, -0.0051,  0.0158, -0.0075, -0.0311,  0.0028,\n",
      "        -0.0095, -0.0290,  0.0416, -0.0009,  0.0088,  0.0308,  0.0016, -0.0386,\n",
      "        -0.0079, -0.0107, -0.0353,  0.0219,  0.0114, -0.0262,  0.0165, -0.0350])\n",
      "Layer: layer1.2.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer1.2.conv2.bias, Biases: tensor([-0.0533,  0.0588, -0.0512,  0.0094, -0.0006, -0.0227,  0.0327,  0.0348,\n",
      "        -0.0120,  0.0153, -0.0319,  0.0031, -0.0033, -0.0323,  0.0272,  0.0124,\n",
      "        -0.0323, -0.0382, -0.0471, -0.0120,  0.0174, -0.0436, -0.0152,  0.0582,\n",
      "        -0.0080, -0.0030, -0.0130,  0.0114,  0.0114,  0.0584, -0.0549, -0.0393,\n",
      "         0.0047,  0.0242,  0.0366,  0.0157,  0.0273,  0.0210,  0.0126,  0.0106,\n",
      "        -0.0115,  0.0139, -0.0249,  0.0420,  0.0024, -0.0473, -0.0307,  0.0173,\n",
      "        -0.0500, -0.0207,  0.0149,  0.0063,  0.0295,  0.0477, -0.0100,  0.0423,\n",
      "         0.0345,  0.0487, -0.0576, -0.0027, -0.0319, -0.0583,  0.0315,  0.0326])\n",
      "Layer: layer1.2.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.0.conv1.bias, Biases: tensor([ 0.0155,  0.0221, -0.0409, -0.0063,  0.0402,  0.0404, -0.0035, -0.0413,\n",
      "        -0.0388,  0.0187,  0.0285, -0.0232, -0.0143, -0.0165, -0.0351, -0.0191,\n",
      "         0.0335,  0.0026, -0.0008,  0.0171, -0.0284, -0.0035,  0.0174,  0.0334,\n",
      "         0.0161, -0.0041,  0.0234,  0.0318, -0.0285, -0.0368, -0.0240, -0.0276,\n",
      "        -0.0381,  0.0315, -0.0232,  0.0382,  0.0294, -0.0064,  0.0016,  0.0162,\n",
      "         0.0312,  0.0160, -0.0176, -0.0083, -0.0106,  0.0061, -0.0196, -0.0145,\n",
      "         0.0412, -0.0081,  0.0211,  0.0058, -0.0387, -0.0033, -0.0375, -0.0312,\n",
      "        -0.0287,  0.0003,  0.0024, -0.0021,  0.0295, -0.0386, -0.0035, -0.0332])\n",
      "Layer: layer2.0.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.0.conv2.bias, Biases: tensor([ 0.0214,  0.0402, -0.0169,  0.0243, -0.0218, -0.0274,  0.0192, -0.0011,\n",
      "         0.0204, -0.0385, -0.0161,  0.0140,  0.0341,  0.0140,  0.0076,  0.0136,\n",
      "        -0.0053,  0.0206, -0.0346, -0.0038, -0.0290,  0.0085, -0.0132, -0.0289,\n",
      "        -0.0147,  0.0320,  0.0195,  0.0280,  0.0252,  0.0376, -0.0168,  0.0174,\n",
      "        -0.0193,  0.0078,  0.0131,  0.0273, -0.0184, -0.0339, -0.0413,  0.0221,\n",
      "        -0.0109, -0.0335,  0.0062, -0.0400, -0.0059, -0.0057,  0.0253, -0.0078,\n",
      "         0.0203, -0.0303,  0.0192, -0.0231, -0.0205, -0.0151, -0.0320,  0.0135,\n",
      "        -0.0288,  0.0292,  0.0307, -0.0018, -0.0129, -0.0137, -0.0208,  0.0053,\n",
      "         0.0039, -0.0136, -0.0251,  0.0053, -0.0089,  0.0367,  0.0300,  0.0127,\n",
      "         0.0260,  0.0267,  0.0197, -0.0200, -0.0173,  0.0152, -0.0303,  0.0102,\n",
      "         0.0118,  0.0064,  0.0068,  0.0285, -0.0196,  0.0219, -0.0300, -0.0237,\n",
      "         0.0082,  0.0239, -0.0298, -0.0319, -0.0072, -0.0026, -0.0235, -0.0069,\n",
      "        -0.0359,  0.0319,  0.0065,  0.0183,  0.0301, -0.0368,  0.0286, -0.0047,\n",
      "         0.0099,  0.0370, -0.0205, -0.0384,  0.0208,  0.0216,  0.0239,  0.0108,\n",
      "        -0.0165,  0.0110,  0.0148, -0.0015,  0.0403, -0.0312, -0.0115, -0.0223,\n",
      "        -0.0172, -0.0280, -0.0019, -0.0352,  0.0235,  0.0088, -0.0203,  0.0042])\n",
      "Layer: layer2.0.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.0.identity_downsample.0.bias, Biases: tensor([-0.1131, -0.0382,  0.0616,  0.0380,  0.1079, -0.1070, -0.1026, -0.0917,\n",
      "         0.0716,  0.0611, -0.0389,  0.0396,  0.0697,  0.0747,  0.0907, -0.1225,\n",
      "        -0.1056, -0.1094,  0.0083,  0.0785,  0.1084,  0.0099,  0.0972, -0.1044,\n",
      "         0.0224, -0.0084,  0.0702, -0.0702,  0.0487, -0.0670,  0.0411, -0.1032,\n",
      "        -0.0008,  0.0687, -0.0725, -0.0318, -0.0395,  0.0526, -0.0486,  0.0089,\n",
      "         0.0066,  0.1137, -0.0384, -0.1069, -0.0856, -0.0801,  0.1209, -0.0460,\n",
      "         0.0811, -0.0552,  0.1013,  0.1055, -0.0485,  0.1060,  0.0453,  0.0425,\n",
      "         0.0633, -0.0498, -0.0889, -0.0426, -0.0506, -0.1059, -0.1247,  0.0692,\n",
      "         0.1098,  0.0906, -0.0581, -0.1130,  0.0886, -0.0435,  0.0433,  0.0239,\n",
      "        -0.0223,  0.0187, -0.0364,  0.0461, -0.1240, -0.0160, -0.0341, -0.1035,\n",
      "         0.0727, -0.0469, -0.1125, -0.0611, -0.1242, -0.0850, -0.0330,  0.1193,\n",
      "         0.0050, -0.0027, -0.0422,  0.0449,  0.0248,  0.0921,  0.1107, -0.0745,\n",
      "         0.0424,  0.0431,  0.0879,  0.0484, -0.0470,  0.0603,  0.0614, -0.0410,\n",
      "        -0.0232, -0.0256, -0.0533,  0.0508,  0.0967,  0.0895,  0.0001, -0.0229,\n",
      "         0.0670,  0.0557,  0.0089, -0.0882,  0.0875,  0.0449,  0.0031, -0.0845,\n",
      "        -0.1183, -0.0552,  0.0294,  0.0720, -0.0420, -0.0247, -0.0089,  0.1233])\n",
      "Layer: layer2.0.identity_downsample.1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.1.conv1.bias, Biases: tensor([ 0.0083,  0.0270,  0.0128, -0.0107,  0.0079,  0.0293, -0.0113,  0.0054,\n",
      "        -0.0259,  0.0079,  0.0236,  0.0109, -0.0131,  0.0030,  0.0128,  0.0209,\n",
      "         0.0088, -0.0095,  0.0198,  0.0067,  0.0130,  0.0073,  0.0176, -0.0006,\n",
      "        -0.0262,  0.0192, -0.0222,  0.0125, -0.0041, -0.0162,  0.0259,  0.0199,\n",
      "        -0.0219,  0.0018,  0.0288,  0.0248,  0.0204, -0.0004, -0.0158,  0.0130,\n",
      "        -0.0159,  0.0160,  0.0123,  0.0232,  0.0072,  0.0056, -0.0084,  0.0032,\n",
      "        -0.0105,  0.0083,  0.0089, -0.0081, -0.0241, -0.0141,  0.0120,  0.0057,\n",
      "        -0.0230,  0.0029, -0.0021,  0.0026, -0.0254, -0.0153, -0.0097, -0.0178])\n",
      "Layer: layer2.1.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.1.conv2.bias, Biases: tensor([-2.7402e-02, -2.8701e-02, -7.1458e-03,  1.7150e-02, -1.7895e-02,\n",
      "         2.1894e-02,  1.3199e-02,  1.0164e-02,  3.2119e-02,  1.4204e-02,\n",
      "        -3.8552e-02,  3.3037e-02,  1.4362e-03, -8.6687e-04,  6.4318e-03,\n",
      "        -8.8756e-03, -3.0279e-02,  4.1473e-02, -3.2564e-02, -2.8058e-02,\n",
      "        -1.5399e-03,  4.4659e-03,  2.3707e-02, -9.6472e-03, -3.9053e-02,\n",
      "         1.2131e-02, -6.3007e-03,  2.5474e-02,  8.4298e-03,  1.7180e-02,\n",
      "         3.9763e-02,  6.2213e-03,  2.5117e-02,  2.2619e-02,  2.3351e-02,\n",
      "        -6.0141e-03, -2.2042e-02, -2.7533e-02, -3.1604e-02, -3.3967e-02,\n",
      "        -2.5664e-02, -3.0472e-02,  2.1978e-02,  7.6379e-03,  1.2030e-02,\n",
      "        -2.5449e-02,  4.0974e-02,  3.3303e-02, -8.5857e-03,  2.7892e-02,\n",
      "        -3.9309e-02,  5.5265e-03, -1.0358e-02,  1.7033e-02,  1.4574e-02,\n",
      "        -2.7127e-02, -1.5209e-02,  8.2604e-03, -2.8666e-02, -3.6104e-02,\n",
      "        -6.0780e-03, -2.2477e-02, -4.1329e-03, -6.1940e-03,  1.0558e-02,\n",
      "         3.0424e-02, -3.6584e-02,  3.9446e-02, -2.4939e-03,  3.7158e-02,\n",
      "         3.1748e-02,  6.1116e-03,  3.6340e-02,  3.2801e-02, -3.9910e-02,\n",
      "        -1.5549e-02,  1.1180e-02, -7.0300e-03, -3.2492e-02,  1.5288e-02,\n",
      "         1.6081e-02,  2.3418e-02,  1.3691e-02, -9.5657e-03, -3.9085e-02,\n",
      "         2.0433e-03, -3.6073e-02,  4.1011e-02, -9.3769e-03, -2.5564e-02,\n",
      "        -2.9350e-02, -2.0500e-02, -1.4935e-02,  4.0364e-02,  2.5077e-02,\n",
      "         1.4864e-02,  2.0576e-02,  2.7961e-02,  1.5780e-02,  2.3471e-02,\n",
      "         2.9070e-03, -2.3799e-02,  7.9345e-04, -9.8144e-05, -2.5033e-02,\n",
      "         1.5488e-02,  6.3496e-03,  2.5464e-02, -1.7638e-03, -2.0237e-02,\n",
      "        -1.6639e-02,  1.9822e-02, -2.8968e-02,  4.0800e-02, -8.7613e-03,\n",
      "         2.3207e-02,  3.4511e-02,  1.7708e-02,  3.9059e-02,  2.7187e-02,\n",
      "         3.9759e-02,  3.3287e-02, -3.4237e-03, -3.8550e-02,  9.1774e-03,\n",
      "         8.5172e-03,  1.7245e-03, -4.2171e-03])\n",
      "Layer: layer2.1.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.2.conv1.bias, Biases: tensor([-0.0226,  0.0202, -0.0143,  0.0027,  0.0020,  0.0090,  0.0223,  0.0134,\n",
      "        -0.0094, -0.0148,  0.0200,  0.0152,  0.0268,  0.0285, -0.0030,  0.0134,\n",
      "        -0.0101, -0.0029,  0.0277,  0.0284,  0.0018,  0.0043,  0.0105, -0.0266,\n",
      "        -0.0210,  0.0066, -0.0109,  0.0036, -0.0101, -0.0274, -0.0116, -0.0223,\n",
      "        -0.0113, -0.0011, -0.0260, -0.0289,  0.0026, -0.0238,  0.0227, -0.0227,\n",
      "        -0.0202, -0.0202, -0.0149, -0.0101,  0.0289,  0.0176, -0.0013,  0.0019,\n",
      "         0.0070,  0.0187,  0.0209, -0.0276, -0.0215, -0.0125,  0.0086,  0.0194,\n",
      "        -0.0011, -0.0212, -0.0179, -0.0007, -0.0050,  0.0161, -0.0094, -0.0086])\n",
      "Layer: layer2.2.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.2.conv2.bias, Biases: tensor([ 0.0401, -0.0278,  0.0152, -0.0321, -0.0023, -0.0275,  0.0378,  0.0152,\n",
      "         0.0340,  0.0324, -0.0076, -0.0145,  0.0256,  0.0063, -0.0194, -0.0283,\n",
      "         0.0107,  0.0084, -0.0317, -0.0093, -0.0181, -0.0032,  0.0233, -0.0236,\n",
      "         0.0094, -0.0269,  0.0362,  0.0126, -0.0285, -0.0164, -0.0227,  0.0056,\n",
      "        -0.0174, -0.0138, -0.0170, -0.0296,  0.0280,  0.0185,  0.0200,  0.0026,\n",
      "        -0.0353, -0.0333, -0.0370, -0.0278,  0.0319,  0.0257, -0.0215, -0.0241,\n",
      "         0.0065, -0.0399,  0.0015, -0.0172,  0.0144, -0.0337,  0.0240, -0.0135,\n",
      "        -0.0312,  0.0416, -0.0363, -0.0374,  0.0356, -0.0085,  0.0139,  0.0001,\n",
      "         0.0220,  0.0142,  0.0345, -0.0082,  0.0058,  0.0289, -0.0295,  0.0024,\n",
      "         0.0044,  0.0037, -0.0128,  0.0233, -0.0126, -0.0294,  0.0369,  0.0319,\n",
      "        -0.0035, -0.0366,  0.0032,  0.0313, -0.0106, -0.0100,  0.0284, -0.0199,\n",
      "        -0.0242,  0.0242,  0.0017, -0.0332, -0.0088,  0.0218,  0.0275,  0.0362,\n",
      "         0.0309, -0.0355,  0.0228,  0.0276, -0.0266,  0.0295,  0.0402,  0.0283,\n",
      "        -0.0184, -0.0268,  0.0240,  0.0065,  0.0117, -0.0324,  0.0244, -0.0117,\n",
      "        -0.0387, -0.0402,  0.0018, -0.0238, -0.0277, -0.0063,  0.0265,  0.0024,\n",
      "         0.0057,  0.0166, -0.0129, -0.0357,  0.0111,  0.0208,  0.0346, -0.0403])\n",
      "Layer: layer2.2.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.3.conv1.bias, Biases: tensor([ 0.0139,  0.0205, -0.0269,  0.0283,  0.0144, -0.0135,  0.0073,  0.0096,\n",
      "        -0.0184, -0.0111,  0.0273, -0.0128,  0.0095,  0.0096, -0.0013,  0.0100,\n",
      "        -0.0163,  0.0116,  0.0253,  0.0008,  0.0163,  0.0176,  0.0285,  0.0041,\n",
      "        -0.0054,  0.0174,  0.0202, -0.0151,  0.0266,  0.0077, -0.0052,  0.0160,\n",
      "         0.0117, -0.0219,  0.0061, -0.0147,  0.0256, -0.0265, -0.0266,  0.0188,\n",
      "        -0.0250,  0.0152, -0.0285, -0.0006, -0.0178, -0.0230, -0.0061,  0.0047,\n",
      "         0.0210,  0.0057,  0.0004, -0.0277,  0.0078, -0.0169,  0.0081, -0.0270,\n",
      "         0.0289,  0.0034,  0.0247,  0.0148,  0.0078, -0.0292, -0.0269, -0.0162])\n",
      "Layer: layer2.3.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.3.conv2.bias, Biases: tensor([-0.0037,  0.0072,  0.0227, -0.0335, -0.0111,  0.0076,  0.0071, -0.0338,\n",
      "         0.0320, -0.0207, -0.0350,  0.0188, -0.0039, -0.0156, -0.0372,  0.0098,\n",
      "        -0.0191,  0.0367, -0.0258,  0.0008, -0.0181,  0.0251, -0.0385,  0.0218,\n",
      "         0.0126, -0.0143, -0.0091, -0.0263, -0.0222, -0.0210,  0.0021, -0.0143,\n",
      "        -0.0049, -0.0175, -0.0356, -0.0045, -0.0041, -0.0223, -0.0364, -0.0141,\n",
      "         0.0101,  0.0243,  0.0385,  0.0122,  0.0138,  0.0248, -0.0287, -0.0156,\n",
      "        -0.0352,  0.0221,  0.0323, -0.0271,  0.0047, -0.0274,  0.0095, -0.0368,\n",
      "         0.0074,  0.0273, -0.0371,  0.0042,  0.0319,  0.0228, -0.0329, -0.0192,\n",
      "        -0.0268,  0.0303, -0.0373, -0.0384, -0.0058,  0.0106,  0.0243,  0.0124,\n",
      "        -0.0171,  0.0218,  0.0387, -0.0032,  0.0162, -0.0309,  0.0093,  0.0084,\n",
      "         0.0054,  0.0209, -0.0379, -0.0006, -0.0171,  0.0367,  0.0371, -0.0195,\n",
      "        -0.0157,  0.0246, -0.0018,  0.0078, -0.0239,  0.0006, -0.0126,  0.0226,\n",
      "        -0.0038,  0.0395, -0.0072, -0.0212,  0.0384,  0.0278,  0.0095,  0.0074,\n",
      "        -0.0080, -0.0122, -0.0006, -0.0312,  0.0334, -0.0330,  0.0326, -0.0254,\n",
      "        -0.0108, -0.0283, -0.0104, -0.0184,  0.0111, -0.0236,  0.0165, -0.0129,\n",
      "         0.0406,  0.0317,  0.0091, -0.0083, -0.0137,  0.0374, -0.0408,  0.0157])\n",
      "Layer: layer2.3.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.0.conv1.bias, Biases: tensor([ 0.0244, -0.0034, -0.0243, -0.0115,  0.0184,  0.0098, -0.0075,  0.0006,\n",
      "        -0.0091, -0.0109,  0.0170,  0.0017,  0.0037,  0.0115,  0.0144, -0.0162,\n",
      "        -0.0258, -0.0167, -0.0083, -0.0259,  0.0140,  0.0071,  0.0197, -0.0085,\n",
      "         0.0084, -0.0262,  0.0068, -0.0174, -0.0003, -0.0191, -0.0184, -0.0287,\n",
      "        -0.0268,  0.0054, -0.0214,  0.0081, -0.0234,  0.0160,  0.0187, -0.0269,\n",
      "         0.0264,  0.0035,  0.0210,  0.0005,  0.0051,  0.0014,  0.0250, -0.0110,\n",
      "         0.0037,  0.0157,  0.0287,  0.0273,  0.0163,  0.0275,  0.0041,  0.0063,\n",
      "        -0.0225, -0.0166, -0.0172, -0.0151, -0.0195, -0.0129, -0.0189,  0.0285,\n",
      "        -0.0213,  0.0262, -0.0083,  0.0268, -0.0287, -0.0046,  0.0246,  0.0274,\n",
      "        -0.0287,  0.0072, -0.0121,  0.0245, -0.0270, -0.0261, -0.0198,  0.0234,\n",
      "         0.0164, -0.0275,  0.0172, -0.0115, -0.0076,  0.0156,  0.0044, -0.0239,\n",
      "        -0.0278, -0.0098,  0.0207,  0.0242, -0.0145,  0.0037, -0.0220, -0.0189,\n",
      "        -0.0078,  0.0091,  0.0272, -0.0032, -0.0183,  0.0146, -0.0253,  0.0148,\n",
      "         0.0234,  0.0263, -0.0228, -0.0070,  0.0233,  0.0096, -0.0167,  0.0044,\n",
      "         0.0041, -0.0099,  0.0133,  0.0011,  0.0234, -0.0019,  0.0241, -0.0133,\n",
      "        -0.0224,  0.0036, -0.0269,  0.0090, -0.0229, -0.0234,  0.0020, -0.0054])\n",
      "Layer: layer3.0.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.0.conv2.bias, Biases: tensor([ 1.9217e-02, -1.2952e-02,  6.4348e-04,  8.0655e-03, -1.3457e-02,\n",
      "         2.1581e-02, -1.3174e-02, -2.6985e-03, -6.6829e-03, -1.4424e-03,\n",
      "        -2.5283e-02, -8.1989e-03, -1.4486e-02, -2.1362e-02, -1.4849e-02,\n",
      "        -6.8669e-03, -1.9572e-02,  7.8376e-03,  1.6778e-02, -7.8095e-03,\n",
      "        -1.8065e-02, -2.1958e-02,  2.6917e-02,  8.7550e-04,  2.6927e-02,\n",
      "         1.2681e-02, -9.2622e-03, -2.1977e-03, -1.9089e-02, -9.0176e-03,\n",
      "        -1.6459e-02,  2.7840e-02,  1.3590e-02, -1.4744e-02,  2.7633e-02,\n",
      "        -1.6788e-02,  2.2660e-02,  6.1549e-03, -2.4703e-02,  2.5910e-02,\n",
      "        -2.6504e-02,  2.6362e-03, -1.6271e-02, -2.2910e-02, -1.4423e-02,\n",
      "        -1.9651e-02, -2.3816e-02,  1.5660e-02,  2.8761e-02,  1.9884e-02,\n",
      "        -3.8235e-03, -2.0849e-02,  2.8569e-02, -4.9626e-03, -1.8407e-02,\n",
      "        -6.3223e-04, -1.3280e-02,  6.6046e-03, -1.6764e-02, -2.5523e-02,\n",
      "         2.3858e-02,  1.2951e-02, -1.9977e-03, -1.5614e-02, -1.6340e-02,\n",
      "         2.7536e-02, -1.0744e-02,  1.7502e-02,  3.6566e-03,  1.7103e-02,\n",
      "         1.0009e-02,  8.7332e-03,  3.0518e-03,  8.2746e-03,  2.4154e-02,\n",
      "        -2.4777e-02,  1.9506e-02,  1.4876e-02, -5.8033e-05,  2.0781e-02,\n",
      "        -3.8245e-03,  1.7788e-02, -7.3896e-03, -2.4749e-02, -2.0189e-02,\n",
      "         2.2940e-02, -6.7330e-03, -2.6732e-02,  1.2933e-02,  1.9955e-02,\n",
      "        -5.7996e-03, -1.2584e-02, -4.2228e-03, -1.7485e-02,  1.6158e-02,\n",
      "        -1.1305e-02, -2.6280e-02,  2.7063e-02,  2.8184e-02,  1.5073e-02,\n",
      "         1.0625e-03, -2.0408e-02,  1.4364e-02, -2.4512e-02, -2.3156e-02,\n",
      "         6.5199e-03,  7.4635e-03,  1.7964e-02,  1.3142e-02,  2.8740e-02,\n",
      "        -7.3732e-04,  1.2338e-02, -1.6213e-02, -2.6042e-02,  2.2052e-02,\n",
      "         3.1544e-03,  7.9855e-03, -1.3899e-03, -1.6060e-02,  2.9431e-02,\n",
      "         2.2176e-02,  1.4870e-02, -1.7797e-02,  1.2433e-03,  2.9071e-02,\n",
      "        -4.5046e-03, -2.3568e-02,  2.2444e-02, -2.2293e-02, -1.7914e-03,\n",
      "        -6.5618e-03,  1.1628e-03, -1.7988e-02,  2.1342e-02,  1.4648e-02,\n",
      "         1.3320e-03,  7.2057e-03,  9.0131e-05, -1.0864e-02, -2.4661e-02,\n",
      "        -2.9266e-02,  1.9619e-02,  1.0112e-02,  9.2686e-03, -4.1141e-03,\n",
      "        -7.6994e-03, -2.1292e-02, -2.6317e-02,  2.0611e-03,  1.6970e-02,\n",
      "         1.0590e-02, -1.0262e-02, -2.0218e-02,  2.4084e-02,  6.2313e-03,\n",
      "        -4.7803e-03,  1.6677e-02, -2.8904e-02, -1.8564e-02,  1.2840e-02,\n",
      "        -1.2806e-02,  1.9955e-02,  6.2593e-03, -1.4482e-03, -4.3429e-03,\n",
      "         2.0089e-02,  1.3173e-02, -1.8683e-02,  1.4023e-02, -1.2482e-02,\n",
      "         2.0886e-02,  2.6313e-02,  2.6190e-02,  1.9663e-02, -1.5168e-02,\n",
      "         1.6448e-02,  1.6473e-02,  8.2779e-03,  1.9856e-02,  2.1652e-02,\n",
      "        -2.7343e-02,  2.6186e-02,  7.2435e-03,  7.6838e-04,  2.7045e-02,\n",
      "        -1.4026e-02,  2.0281e-02,  2.7137e-02, -1.8776e-02, -1.7910e-03,\n",
      "         2.5496e-03,  5.9284e-03,  1.4682e-02,  1.7080e-02,  3.5507e-03,\n",
      "         1.9755e-02, -2.2026e-02,  2.0101e-03, -2.9030e-02,  2.3887e-02,\n",
      "         2.5307e-02, -1.8222e-02, -1.7897e-04,  2.1504e-02,  2.9235e-02,\n",
      "        -2.1406e-02,  1.0870e-02,  2.8254e-02,  2.3991e-02,  2.0570e-02,\n",
      "         2.0347e-02, -1.8731e-02, -2.0207e-02, -1.9834e-02, -1.9532e-02,\n",
      "        -5.4675e-05,  2.2173e-02,  5.4499e-03,  2.6013e-02,  1.7608e-03,\n",
      "         2.7652e-02,  1.9449e-02,  1.9916e-02, -1.6369e-02, -2.6686e-02,\n",
      "         3.6879e-03,  6.1294e-03, -7.1309e-03, -1.2414e-02,  7.0200e-03,\n",
      "        -1.3814e-02, -8.6097e-03,  1.1109e-02, -2.8441e-02, -9.9081e-03,\n",
      "         2.5732e-02, -2.8650e-02, -2.3117e-02,  4.9077e-04, -9.7225e-03,\n",
      "        -2.6266e-03,  2.3549e-02, -1.1260e-02, -1.3719e-02,  2.8835e-02,\n",
      "        -2.2360e-02,  4.3215e-03,  1.2406e-02, -2.8769e-02,  8.7059e-03,\n",
      "        -2.0980e-02, -2.2414e-02,  2.0009e-02,  2.1428e-02, -1.5902e-02,\n",
      "         2.1021e-02])\n",
      "Layer: layer3.0.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.0.identity_downsample.0.bias, Biases: tensor([ 0.0585,  0.0350, -0.0115, -0.0048, -0.0155,  0.0181, -0.0163,  0.0436,\n",
      "        -0.0198, -0.0639, -0.0647, -0.0132,  0.0047, -0.0514, -0.0211,  0.0091,\n",
      "        -0.0053,  0.0814, -0.0436,  0.0870, -0.0380, -0.0808,  0.0600, -0.0232,\n",
      "         0.0872, -0.0048,  0.0185,  0.0136, -0.0472, -0.0533, -0.0789, -0.0500,\n",
      "        -0.0114, -0.0755,  0.0845,  0.0845, -0.0491,  0.0155,  0.0418, -0.0605,\n",
      "        -0.0720, -0.0752,  0.0261,  0.0796,  0.0781, -0.0114,  0.0065,  0.0834,\n",
      "         0.0520,  0.0024,  0.0604,  0.0343,  0.0290, -0.0793,  0.0620, -0.0413,\n",
      "         0.0123, -0.0248, -0.0684,  0.0407, -0.0299,  0.0329, -0.0256,  0.0806,\n",
      "        -0.0028,  0.0875, -0.0347,  0.0837,  0.0088,  0.0833, -0.0421,  0.0517,\n",
      "         0.0297,  0.0845, -0.0040,  0.0491,  0.0014,  0.0444,  0.0571, -0.0687,\n",
      "        -0.0506, -0.0273,  0.0477, -0.0833,  0.0207,  0.0059,  0.0158,  0.0687,\n",
      "        -0.0357,  0.0398, -0.0102, -0.0447, -0.0574, -0.0102, -0.0136, -0.0244,\n",
      "        -0.0448,  0.0405, -0.0592, -0.0307, -0.0550, -0.0794,  0.0730,  0.0599,\n",
      "         0.0447,  0.0695, -0.0661, -0.0871,  0.0617, -0.0537,  0.0264,  0.0707,\n",
      "         0.0019,  0.0533,  0.0549,  0.0569,  0.0817,  0.0059,  0.0239,  0.0130,\n",
      "        -0.0725,  0.0219,  0.0841, -0.0205, -0.0583, -0.0035, -0.0249, -0.0030,\n",
      "         0.0136, -0.0593,  0.0056, -0.0872, -0.0515, -0.0028, -0.0114, -0.0136,\n",
      "         0.0725,  0.0589, -0.0403, -0.0389,  0.0423,  0.0649, -0.0106,  0.0257,\n",
      "        -0.0464, -0.0407, -0.0587, -0.0594, -0.0658,  0.0010, -0.0500,  0.0349,\n",
      "        -0.0474,  0.0053,  0.0097,  0.0421, -0.0803, -0.0194,  0.0501,  0.0092,\n",
      "        -0.0744, -0.0409,  0.0478,  0.0468,  0.0624, -0.0209, -0.0820, -0.0210,\n",
      "        -0.0574, -0.0463, -0.0772, -0.0814,  0.0340, -0.0803, -0.0682, -0.0588,\n",
      "        -0.0363, -0.0158,  0.0026,  0.0153,  0.0710, -0.0110, -0.0057, -0.0676,\n",
      "        -0.0526,  0.0499, -0.0403,  0.0840, -0.0244, -0.0611,  0.0318,  0.0870,\n",
      "         0.0427, -0.0756,  0.0659, -0.0827, -0.0823, -0.0214,  0.0007, -0.0194,\n",
      "         0.0101, -0.0323,  0.0547, -0.0070, -0.0725, -0.0175,  0.0063,  0.0155,\n",
      "        -0.0531,  0.0862,  0.0594, -0.0318,  0.0416,  0.0148,  0.0847,  0.0510,\n",
      "         0.0288,  0.0590,  0.0712,  0.0420,  0.0353, -0.0606,  0.0407, -0.0179,\n",
      "         0.0566,  0.0862,  0.0656,  0.0146,  0.0505,  0.0810,  0.0467, -0.0117,\n",
      "        -0.0733, -0.0587, -0.0313, -0.0848, -0.0723,  0.0851,  0.0646, -0.0872,\n",
      "        -0.0878, -0.0539, -0.0513, -0.0324,  0.0001, -0.0196,  0.0555, -0.0289,\n",
      "         0.0016,  0.0256, -0.0113,  0.0163, -0.0882,  0.0368, -0.0475,  0.0100])\n",
      "Layer: layer3.0.identity_downsample.1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.1.conv1.bias, Biases: tensor([-0.0012,  0.0020, -0.0003, -0.0153,  0.0149, -0.0157, -0.0191, -0.0101,\n",
      "        -0.0151,  0.0132,  0.0106,  0.0011, -0.0072, -0.0072,  0.0046, -0.0100,\n",
      "        -0.0179,  0.0122, -0.0076,  0.0036, -0.0064, -0.0150,  0.0188,  0.0095,\n",
      "         0.0029,  0.0005,  0.0111, -0.0015,  0.0052, -0.0034,  0.0182,  0.0010,\n",
      "         0.0097, -0.0178, -0.0110,  0.0079,  0.0116, -0.0113,  0.0041, -0.0173,\n",
      "         0.0058, -0.0074, -0.0081,  0.0074, -0.0008, -0.0032,  0.0197, -0.0186,\n",
      "        -0.0064,  0.0050,  0.0007, -0.0205,  0.0081,  0.0067,  0.0019,  0.0015,\n",
      "         0.0031, -0.0042, -0.0073, -0.0082,  0.0119, -0.0202, -0.0039,  0.0067,\n",
      "        -0.0192,  0.0178,  0.0109,  0.0197, -0.0088, -0.0036,  0.0087,  0.0157,\n",
      "         0.0094, -0.0155,  0.0171,  0.0127,  0.0138, -0.0063,  0.0049, -0.0050,\n",
      "         0.0067,  0.0114,  0.0030,  0.0183,  0.0045, -0.0148,  0.0114, -0.0154,\n",
      "         0.0029, -0.0112, -0.0154, -0.0139,  0.0127,  0.0130, -0.0095,  0.0036,\n",
      "         0.0086, -0.0167, -0.0020,  0.0056,  0.0140,  0.0088, -0.0088, -0.0119,\n",
      "        -0.0204, -0.0189, -0.0141,  0.0088,  0.0103, -0.0110,  0.0075, -0.0166,\n",
      "         0.0190,  0.0124,  0.0110,  0.0157, -0.0096,  0.0031,  0.0177, -0.0136,\n",
      "        -0.0194, -0.0043,  0.0126,  0.0117,  0.0112,  0.0084,  0.0098,  0.0011])\n",
      "Layer: layer3.1.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.1.conv2.bias, Biases: tensor([-0.0033, -0.0018,  0.0060,  0.0017,  0.0134, -0.0220, -0.0190, -0.0144,\n",
      "        -0.0113, -0.0263,  0.0209,  0.0219,  0.0055,  0.0221,  0.0171,  0.0273,\n",
      "        -0.0009, -0.0121, -0.0098,  0.0257,  0.0014, -0.0151, -0.0067, -0.0174,\n",
      "         0.0256, -0.0159,  0.0217, -0.0108, -0.0021,  0.0117, -0.0239,  0.0089,\n",
      "         0.0265, -0.0101,  0.0005, -0.0075, -0.0007,  0.0085, -0.0058,  0.0144,\n",
      "        -0.0240,  0.0288,  0.0025, -0.0140,  0.0152,  0.0157, -0.0293,  0.0166,\n",
      "         0.0262, -0.0179,  0.0219, -0.0124,  0.0103,  0.0207, -0.0218, -0.0258,\n",
      "        -0.0121, -0.0054, -0.0214, -0.0017,  0.0242, -0.0124,  0.0249, -0.0282,\n",
      "        -0.0265, -0.0246, -0.0148,  0.0271, -0.0105, -0.0118,  0.0085, -0.0036,\n",
      "         0.0027,  0.0218, -0.0041,  0.0018, -0.0230,  0.0258,  0.0109, -0.0044,\n",
      "         0.0143, -0.0122, -0.0078,  0.0092, -0.0177, -0.0217, -0.0170, -0.0119,\n",
      "         0.0287, -0.0226, -0.0002,  0.0280,  0.0020,  0.0052,  0.0073, -0.0072,\n",
      "        -0.0102,  0.0224,  0.0203,  0.0004, -0.0209, -0.0295, -0.0055, -0.0050,\n",
      "         0.0117,  0.0284,  0.0062, -0.0165, -0.0101, -0.0120, -0.0280,  0.0040,\n",
      "        -0.0165,  0.0279,  0.0265, -0.0075,  0.0229,  0.0240,  0.0207,  0.0033,\n",
      "         0.0250, -0.0289,  0.0165,  0.0258, -0.0006, -0.0100, -0.0027,  0.0146,\n",
      "        -0.0153,  0.0168,  0.0052, -0.0187, -0.0110, -0.0165, -0.0101,  0.0229,\n",
      "         0.0274,  0.0162, -0.0030, -0.0259, -0.0108,  0.0077,  0.0253,  0.0220,\n",
      "        -0.0041,  0.0163, -0.0122, -0.0224,  0.0283,  0.0015, -0.0220,  0.0202,\n",
      "        -0.0004, -0.0286,  0.0085, -0.0100,  0.0188, -0.0039,  0.0002, -0.0151,\n",
      "         0.0163, -0.0266, -0.0268,  0.0022,  0.0056, -0.0115, -0.0114, -0.0287,\n",
      "         0.0288, -0.0270,  0.0051,  0.0115, -0.0058,  0.0085,  0.0265, -0.0193,\n",
      "        -0.0131, -0.0068,  0.0102, -0.0004,  0.0049, -0.0193, -0.0192, -0.0105,\n",
      "         0.0123,  0.0142,  0.0276,  0.0274,  0.0199, -0.0178, -0.0278,  0.0101,\n",
      "        -0.0281,  0.0256,  0.0081, -0.0183,  0.0218,  0.0204, -0.0255, -0.0085,\n",
      "        -0.0161, -0.0186, -0.0271, -0.0277,  0.0097, -0.0224, -0.0279, -0.0052,\n",
      "         0.0276,  0.0288,  0.0004,  0.0262,  0.0045, -0.0227, -0.0050, -0.0246,\n",
      "         0.0223, -0.0209,  0.0242,  0.0075, -0.0144,  0.0278, -0.0199, -0.0062,\n",
      "         0.0151, -0.0061, -0.0105,  0.0270, -0.0026,  0.0026, -0.0033, -0.0167,\n",
      "        -0.0234, -0.0139, -0.0139, -0.0291,  0.0004, -0.0203, -0.0235,  0.0050,\n",
      "         0.0060, -0.0175,  0.0097,  0.0239,  0.0283,  0.0080, -0.0045, -0.0078,\n",
      "         0.0121,  0.0102, -0.0181,  0.0294,  0.0051, -0.0014, -0.0104, -0.0020])\n",
      "Layer: layer3.1.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.2.conv1.bias, Biases: tensor([ 0.0063, -0.0142, -0.0088,  0.0059,  0.0146, -0.0172,  0.0208,  0.0090,\n",
      "         0.0058, -0.0123,  0.0098,  0.0185,  0.0169, -0.0175,  0.0054, -0.0171,\n",
      "        -0.0061, -0.0116,  0.0207,  0.0160, -0.0008, -0.0097,  0.0179, -0.0193,\n",
      "        -0.0161, -0.0010,  0.0139, -0.0203, -0.0129,  0.0038,  0.0185, -0.0090,\n",
      "         0.0149,  0.0091, -0.0066, -0.0006,  0.0008,  0.0007, -0.0145, -0.0172,\n",
      "        -0.0204, -0.0080,  0.0134, -0.0171,  0.0124, -0.0189, -0.0058,  0.0025,\n",
      "         0.0158,  0.0053, -0.0043,  0.0065, -0.0087, -0.0180, -0.0107,  0.0200,\n",
      "         0.0110,  0.0139,  0.0043,  0.0206,  0.0075,  0.0101, -0.0180,  0.0029,\n",
      "         0.0121, -0.0166, -0.0020, -0.0060,  0.0156,  0.0038, -0.0156, -0.0188,\n",
      "         0.0136,  0.0204, -0.0176, -0.0115, -0.0172, -0.0179,  0.0040,  0.0136,\n",
      "        -0.0100, -0.0007,  0.0083,  0.0187, -0.0127, -0.0013, -0.0028, -0.0202,\n",
      "         0.0152, -0.0085,  0.0107, -0.0033, -0.0033, -0.0022,  0.0122, -0.0075,\n",
      "         0.0128,  0.0155,  0.0058,  0.0164,  0.0032,  0.0141, -0.0001, -0.0154,\n",
      "        -0.0008, -0.0075,  0.0001, -0.0185, -0.0009,  0.0153,  0.0196, -0.0135,\n",
      "        -0.0106, -0.0151, -0.0076,  0.0070,  0.0119, -0.0058,  0.0185, -0.0195,\n",
      "        -0.0019, -0.0044, -0.0168,  0.0139,  0.0074,  0.0175,  0.0101,  0.0008])\n",
      "Layer: layer3.2.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.2.conv2.bias, Biases: tensor([-1.8489e-03,  8.3289e-03,  1.3770e-02, -2.5661e-02,  2.8029e-02,\n",
      "         1.3477e-02,  1.4729e-02, -2.8907e-02,  2.6529e-02,  6.1265e-03,\n",
      "         8.2403e-04, -2.9858e-04, -6.1158e-03,  1.7627e-02,  9.9658e-03,\n",
      "        -1.6321e-02,  2.4232e-02, -2.2133e-02, -1.1298e-02, -2.7753e-02,\n",
      "         2.5365e-02,  4.0102e-03, -7.1178e-03,  1.9279e-02,  7.8858e-03,\n",
      "         2.0888e-02,  2.0664e-02, -6.4864e-03, -1.3769e-02, -9.3945e-04,\n",
      "        -5.7553e-03,  2.0156e-02,  2.6558e-02,  2.8230e-02,  1.0059e-02,\n",
      "        -1.0448e-02,  2.2242e-02, -7.9468e-03, -3.5168e-03,  1.3787e-02,\n",
      "        -2.2882e-05,  8.2362e-03, -1.9494e-02, -1.4736e-02, -2.5144e-02,\n",
      "        -2.6387e-02,  1.5399e-02,  2.1363e-02,  1.5117e-02,  6.0031e-03,\n",
      "        -1.5084e-02,  7.7442e-03, -1.4743e-02,  1.0004e-02, -5.7136e-03,\n",
      "        -1.2219e-03, -9.8095e-03,  1.7645e-02,  5.4840e-03,  2.7961e-02,\n",
      "        -1.8513e-03, -1.7405e-02, -2.5968e-02, -1.8084e-02,  8.2368e-03,\n",
      "         2.5872e-02,  2.1292e-02,  1.4811e-02,  1.4979e-02,  1.9004e-02,\n",
      "        -7.0486e-03,  2.5989e-02,  7.6762e-03, -4.8754e-03,  4.1097e-03,\n",
      "        -8.0489e-03,  2.2102e-02, -6.4605e-03, -2.5794e-02,  8.4313e-03,\n",
      "        -1.5375e-02, -2.9238e-02, -1.5671e-02,  2.2440e-02, -4.0199e-03,\n",
      "         2.7478e-02, -4.0874e-03,  2.3216e-03,  2.0723e-03,  2.9018e-02,\n",
      "        -2.0919e-03, -2.0802e-02, -4.4550e-03, -2.6224e-02, -7.0260e-03,\n",
      "        -3.7325e-03, -1.7374e-02, -6.2183e-03, -1.9805e-02, -2.1355e-02,\n",
      "         1.7103e-03, -2.1406e-02,  2.4094e-03,  1.8228e-02, -3.3027e-03,\n",
      "        -1.8558e-02, -2.8752e-02,  1.5215e-02,  1.7580e-02, -1.1705e-02,\n",
      "        -1.1683e-02, -2.1773e-02, -2.7174e-03, -9.8019e-03, -2.2848e-02,\n",
      "         2.8043e-02,  1.4435e-02,  2.3943e-02, -4.7200e-03,  2.1813e-03,\n",
      "         8.1392e-03,  7.2298e-04,  1.9096e-02,  2.2857e-02, -9.5660e-03,\n",
      "         3.1541e-03,  2.7485e-02,  1.0515e-02,  2.0035e-02, -2.2413e-02,\n",
      "         1.8967e-02,  1.3619e-02, -1.7120e-02, -2.8670e-02,  1.6296e-02,\n",
      "        -3.1174e-03, -1.1046e-02,  8.2844e-03, -2.1382e-02,  2.8822e-02,\n",
      "         2.5264e-02, -1.8261e-02,  1.8926e-02,  2.2553e-02,  5.4060e-03,\n",
      "         2.4865e-02,  1.0691e-02, -1.0704e-02,  7.5696e-03,  1.1880e-02,\n",
      "        -3.8151e-03, -1.8807e-02,  2.8204e-02, -1.6149e-02,  1.5395e-02,\n",
      "        -1.1949e-02,  1.5437e-02,  1.8619e-02,  1.9398e-02,  1.1125e-02,\n",
      "        -2.6271e-02, -2.4747e-04, -1.6115e-02,  2.6085e-02, -1.2488e-04,\n",
      "         2.6610e-02,  5.8859e-03, -2.7755e-02, -6.1050e-03, -2.3927e-03,\n",
      "        -5.0576e-03,  2.1743e-02, -7.8943e-03, -2.3034e-02, -1.4646e-02,\n",
      "         1.0279e-02, -2.0387e-02, -6.5540e-03,  9.4973e-03,  1.2647e-02,\n",
      "         1.7252e-02,  1.9507e-05,  2.5382e-02,  2.3675e-02,  1.0666e-02,\n",
      "        -7.7628e-03, -1.1144e-03, -6.4877e-03, -2.5056e-02, -6.6541e-03,\n",
      "         1.5307e-03,  2.9031e-02,  2.8460e-02, -2.3570e-02,  2.7016e-02,\n",
      "        -1.2003e-02, -2.8246e-02,  2.5732e-02, -9.1369e-03, -2.8314e-02,\n",
      "        -1.9650e-02,  1.4143e-02, -1.6415e-03,  2.3710e-02,  1.3214e-02,\n",
      "         2.5191e-02, -2.1721e-02, -1.8879e-02,  1.0472e-02,  1.2356e-02,\n",
      "         1.9835e-02, -2.2473e-02,  9.4357e-03, -2.1480e-02, -7.0338e-03,\n",
      "         1.3991e-02,  2.4208e-02,  2.6652e-03,  5.9952e-03,  1.5626e-02,\n",
      "        -6.0489e-03,  2.6639e-02, -1.2808e-02,  2.1433e-04, -2.1462e-02,\n",
      "         1.1200e-02, -1.3678e-02, -1.3049e-02,  9.2765e-03, -5.6076e-03,\n",
      "         2.1714e-02, -1.6158e-02,  1.7349e-03, -4.1397e-03, -1.6410e-02,\n",
      "        -2.4828e-02,  1.1146e-03,  9.2454e-03,  4.8945e-03,  2.5547e-02,\n",
      "        -4.2373e-03, -1.8942e-02, -2.4033e-02, -2.7259e-03, -2.0040e-02,\n",
      "         1.8305e-02, -6.7265e-03, -8.8825e-03, -2.7135e-02,  2.4401e-02,\n",
      "         2.6840e-02,  2.1609e-03,  1.0158e-02, -1.9271e-02,  8.2183e-03,\n",
      "         4.3217e-03])\n",
      "Layer: layer3.2.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.3.conv1.bias, Biases: tensor([-0.0070,  0.0132, -0.0035, -0.0198,  0.0006,  0.0020,  0.0200, -0.0046,\n",
      "        -0.0178, -0.0140,  0.0031, -0.0124,  0.0198, -0.0050, -0.0078,  0.0199,\n",
      "         0.0171, -0.0176, -0.0096, -0.0148, -0.0154,  0.0114,  0.0157,  0.0160,\n",
      "         0.0067,  0.0191, -0.0039,  0.0116, -0.0020,  0.0085, -0.0191, -0.0136,\n",
      "         0.0202, -0.0099, -0.0095,  0.0188, -0.0143,  0.0046,  0.0052, -0.0022,\n",
      "        -0.0008, -0.0132, -0.0034,  0.0008,  0.0122,  0.0125, -0.0016,  0.0007,\n",
      "         0.0121,  0.0053, -0.0115,  0.0039, -0.0186,  0.0152,  0.0001,  0.0191,\n",
      "         0.0054, -0.0059, -0.0054, -0.0168, -0.0192,  0.0043,  0.0123, -0.0125,\n",
      "         0.0182,  0.0017, -0.0058, -0.0140, -0.0094, -0.0050, -0.0183,  0.0155,\n",
      "         0.0201,  0.0191,  0.0084, -0.0008, -0.0141,  0.0085, -0.0041,  0.0051,\n",
      "        -0.0081,  0.0002, -0.0021, -0.0050, -0.0043,  0.0007,  0.0055, -0.0136,\n",
      "        -0.0078,  0.0176,  0.0005,  0.0182,  0.0030,  0.0119,  0.0024,  0.0112,\n",
      "        -0.0032, -0.0145, -0.0161,  0.0131,  0.0112,  0.0166, -0.0101, -0.0198,\n",
      "         0.0192, -0.0093, -0.0147, -0.0153,  0.0114,  0.0182,  0.0137, -0.0035,\n",
      "         0.0051, -0.0196, -0.0188,  0.0148, -0.0051,  0.0031,  0.0159,  0.0192,\n",
      "        -0.0115, -0.0001,  0.0145,  0.0117, -0.0176, -0.0122,  0.0123,  0.0142])\n",
      "Layer: layer3.3.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.3.conv2.bias, Biases: tensor([-2.7354e-03, -4.2174e-03,  7.9183e-03,  2.5703e-02, -2.0967e-03,\n",
      "        -2.3633e-02,  2.0961e-02,  2.6765e-02, -7.4346e-03, -1.8476e-04,\n",
      "         2.2642e-02,  2.1319e-02, -1.0629e-02, -5.3697e-03, -4.1364e-03,\n",
      "         1.1091e-02, -2.5857e-02, -2.7536e-02,  2.3938e-02,  2.4116e-02,\n",
      "        -5.4529e-03, -1.2432e-02,  1.0193e-02, -1.4404e-02, -2.9058e-03,\n",
      "        -2.3085e-02, -5.1626e-03, -2.8224e-02,  2.3402e-02,  1.6822e-03,\n",
      "         2.9821e-03, -7.6848e-03,  2.4564e-02,  2.4308e-02,  2.3903e-02,\n",
      "         6.4862e-03, -9.0059e-04, -2.8426e-03,  2.2286e-02,  2.4632e-02,\n",
      "         2.1740e-02, -5.6710e-03,  1.6905e-02,  6.8242e-04,  2.3572e-03,\n",
      "         2.8267e-02,  2.0694e-02,  2.9353e-02, -5.8759e-03, -2.2264e-03,\n",
      "        -1.2173e-03, -2.0970e-02,  1.8516e-02, -1.1540e-02,  4.6296e-03,\n",
      "        -1.7172e-02,  1.7455e-02, -2.6123e-02, -5.4077e-03, -8.9614e-03,\n",
      "         1.7720e-02, -4.9175e-03,  2.9094e-02, -9.8950e-03, -4.6723e-03,\n",
      "         2.4302e-02,  8.8695e-04, -1.6526e-02,  1.2572e-02,  8.8181e-03,\n",
      "         1.2400e-02,  1.2726e-02,  1.1309e-02,  2.0482e-02,  1.3949e-03,\n",
      "         1.7433e-02,  2.8430e-02, -9.8461e-03,  2.0207e-02,  1.9377e-02,\n",
      "        -3.0806e-03,  1.4498e-02, -5.2316e-03,  1.2201e-02, -1.9954e-03,\n",
      "        -4.4635e-04,  1.5115e-02, -2.3641e-02,  4.5105e-03,  2.0148e-02,\n",
      "        -1.9003e-02, -2.7716e-02, -1.8619e-02, -2.7900e-03, -1.9616e-02,\n",
      "        -2.1890e-02,  1.3086e-02,  6.1766e-03, -1.5928e-02, -2.6975e-02,\n",
      "         9.9666e-03, -3.0202e-03,  9.9564e-03, -2.2384e-02, -1.0682e-02,\n",
      "         2.8875e-02, -8.4323e-03,  2.0530e-03,  4.9630e-03,  9.9219e-03,\n",
      "         5.9550e-03, -1.8177e-03, -1.6625e-02,  1.9632e-04, -2.7388e-02,\n",
      "         1.3976e-02,  1.4269e-02,  1.2343e-02,  2.1495e-02, -1.5608e-02,\n",
      "        -1.9856e-02, -1.6213e-02,  2.6120e-02,  1.2114e-02,  5.9336e-03,\n",
      "         9.1145e-03,  1.8139e-02,  2.3547e-03,  1.6971e-02, -2.2413e-02,\n",
      "         4.6644e-03, -2.6408e-03, -2.7449e-02,  2.3044e-02, -5.4446e-03,\n",
      "        -5.7732e-03,  5.5906e-03, -1.4898e-02,  9.5419e-03,  7.8637e-04,\n",
      "        -1.3895e-02, -1.6651e-02, -2.6464e-03,  2.1922e-02,  2.0776e-02,\n",
      "        -1.9699e-02, -1.9392e-02, -1.0143e-02, -2.2892e-02,  2.2308e-02,\n",
      "        -2.4630e-02,  5.5225e-03,  8.4559e-03, -6.8636e-03,  2.9242e-02,\n",
      "        -3.5635e-03,  2.7998e-02, -1.1872e-04,  2.1220e-02,  2.9110e-02,\n",
      "        -1.3965e-02, -8.7880e-05,  1.7132e-02, -2.2901e-02,  1.2582e-02,\n",
      "        -1.6309e-02, -1.5670e-02, -1.8794e-02, -7.0875e-03, -1.1599e-02,\n",
      "         2.3625e-02, -7.4163e-03, -2.2712e-03, -9.3598e-03,  2.4704e-02,\n",
      "        -1.9275e-02,  9.4633e-03, -3.0419e-03, -7.3000e-03,  3.4429e-04,\n",
      "        -2.3935e-02, -6.6705e-03,  2.0456e-03, -2.5124e-02,  4.1741e-04,\n",
      "         2.8386e-02, -2.0940e-02, -1.7405e-02, -2.2308e-02, -2.1355e-03,\n",
      "         5.5934e-03, -9.8659e-03,  6.3629e-03,  2.2984e-02, -1.8412e-02,\n",
      "        -1.0484e-02,  1.3510e-02,  8.9754e-03,  9.5765e-03, -1.6449e-02,\n",
      "         2.8784e-02, -1.6723e-02,  1.4953e-03, -4.7030e-03, -4.4704e-03,\n",
      "         1.4961e-02,  2.8467e-02, -1.8319e-03,  8.2325e-03, -2.7813e-02,\n",
      "         2.9444e-03, -2.9058e-02,  7.4087e-03,  1.4807e-02,  1.2581e-02,\n",
      "        -7.8193e-03, -2.0883e-02,  1.4021e-02, -9.6066e-03, -1.3131e-02,\n",
      "        -2.0382e-02,  2.3240e-02, -1.2124e-02,  2.1959e-02, -1.4407e-02,\n",
      "         2.2019e-02,  1.7974e-02, -1.3969e-02, -1.8820e-03,  7.0736e-03,\n",
      "        -7.5145e-03,  1.4475e-02, -1.9817e-02,  1.6552e-03, -1.1523e-03,\n",
      "         2.2392e-02,  8.1441e-03,  2.2189e-02, -6.7894e-03,  2.5230e-02,\n",
      "        -2.3812e-02, -2.3333e-02,  1.5639e-02,  2.4739e-02,  6.9350e-03,\n",
      "         1.6345e-02, -2.8391e-02,  2.5532e-02,  2.2870e-02,  1.3270e-02,\n",
      "        -1.8362e-02, -8.2886e-03, -9.3726e-03, -1.4750e-03,  2.3966e-02,\n",
      "         1.5015e-02])\n",
      "Layer: layer3.3.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.4.conv1.bias, Biases: tensor([ 0.0081,  0.0043, -0.0046, -0.0117,  0.0131, -0.0023, -0.0031, -0.0027,\n",
      "         0.0032,  0.0110, -0.0057,  0.0115, -0.0056,  0.0077,  0.0062,  0.0072,\n",
      "        -0.0191,  0.0122,  0.0023, -0.0097, -0.0167, -0.0143, -0.0095, -0.0161,\n",
      "        -0.0163,  0.0170,  0.0193, -0.0009, -0.0122,  0.0056,  0.0171, -0.0144,\n",
      "         0.0061,  0.0149, -0.0152,  0.0130,  0.0174,  0.0103,  0.0165,  0.0043,\n",
      "        -0.0088,  0.0096,  0.0176,  0.0041,  0.0012, -0.0113, -0.0141,  0.0111,\n",
      "        -0.0092, -0.0172, -0.0186, -0.0138, -0.0193,  0.0093,  0.0069,  0.0190,\n",
      "        -0.0195,  0.0134, -0.0054,  0.0028,  0.0025, -0.0172, -0.0167, -0.0149,\n",
      "         0.0129, -0.0129,  0.0011,  0.0016,  0.0186, -0.0109, -0.0147,  0.0115,\n",
      "        -0.0045, -0.0037,  0.0159, -0.0012,  0.0169, -0.0151, -0.0036, -0.0165,\n",
      "         0.0045, -0.0147,  0.0112,  0.0114,  0.0087,  0.0122, -0.0108,  0.0014,\n",
      "         0.0006, -0.0105,  0.0023, -0.0112,  0.0103, -0.0029, -0.0019,  0.0198,\n",
      "         0.0106,  0.0195,  0.0136,  0.0159,  0.0035, -0.0048,  0.0119, -0.0128,\n",
      "        -0.0028, -0.0110, -0.0083,  0.0137, -0.0126, -0.0187,  0.0035, -0.0049,\n",
      "         0.0013, -0.0155,  0.0042,  0.0046,  0.0186,  0.0121, -0.0019,  0.0065,\n",
      "         0.0152,  0.0193, -0.0097,  0.0146, -0.0164, -0.0141,  0.0171,  0.0195])\n",
      "Layer: layer3.4.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.4.conv2.bias, Biases: tensor([ 1.8430e-02, -1.2469e-02,  1.6399e-02,  2.1813e-02, -2.3068e-02,\n",
      "        -2.1222e-03,  2.9350e-02,  9.8871e-03, -3.1246e-03,  7.3674e-03,\n",
      "         1.2213e-02, -4.0401e-04,  1.8777e-03, -1.4541e-02,  1.8509e-02,\n",
      "        -1.8383e-02, -7.8497e-03, -1.3905e-03, -1.7624e-02,  2.1894e-02,\n",
      "        -2.2931e-02,  1.8447e-02,  2.7771e-02, -9.2368e-03, -1.3428e-02,\n",
      "        -1.5641e-02, -2.2998e-02, -1.6364e-02, -6.1851e-03, -1.1079e-02,\n",
      "        -2.6338e-02, -2.6333e-04,  2.6791e-02, -2.6737e-02, -2.4664e-03,\n",
      "        -3.2464e-04,  2.0378e-02,  3.5755e-06,  1.1109e-02,  2.8028e-02,\n",
      "        -9.4376e-03, -7.2041e-03,  2.8239e-02, -2.8126e-02,  5.3766e-03,\n",
      "        -2.6406e-02, -2.4632e-02, -1.7640e-02,  3.6428e-03,  5.6460e-03,\n",
      "        -2.0646e-02, -1.8777e-02,  1.5648e-03, -1.5833e-02, -1.0497e-02,\n",
      "        -2.5660e-02, -2.9726e-03, -4.5590e-03,  1.4621e-02, -2.9237e-02,\n",
      "        -1.3054e-02, -1.6160e-02, -4.0842e-03,  2.0224e-02,  3.5360e-03,\n",
      "        -3.9052e-03,  2.9405e-02,  2.4456e-02,  1.8885e-02,  2.4969e-02,\n",
      "        -2.5755e-02,  2.4370e-02,  2.2785e-02,  3.1068e-03,  1.7571e-02,\n",
      "        -2.1941e-02, -2.8416e-02,  2.7816e-03,  2.5079e-02,  8.7867e-03,\n",
      "         1.2236e-02,  4.2582e-03, -8.5954e-03, -1.8495e-02, -5.8579e-03,\n",
      "         2.8638e-02, -2.1928e-02, -2.3692e-02,  2.5211e-03,  2.3953e-02,\n",
      "        -2.3957e-02, -1.9839e-03, -8.3991e-04,  1.9624e-02,  2.5636e-02,\n",
      "         1.3267e-02, -2.8702e-02,  6.2092e-03, -1.4767e-03,  1.6739e-02,\n",
      "        -2.8597e-02, -2.6731e-02, -1.6137e-02,  9.9336e-03, -2.2425e-02,\n",
      "        -1.3996e-02,  2.3945e-02,  2.0453e-02, -2.3605e-02, -2.1914e-02,\n",
      "        -4.0890e-03, -9.2448e-03,  1.1038e-02,  1.7306e-02, -1.3045e-02,\n",
      "        -8.5569e-03,  2.6887e-04,  9.0889e-03,  1.0003e-02, -2.0334e-02,\n",
      "         1.9733e-02, -2.1366e-03, -2.4662e-03,  2.7191e-02, -2.3422e-02,\n",
      "         2.1886e-02, -5.5953e-03, -2.5846e-02, -2.6605e-02, -1.5093e-02,\n",
      "        -1.1160e-02,  1.6853e-02, -8.9109e-03, -9.1804e-03,  2.8191e-02,\n",
      "        -7.4474e-03, -1.5733e-02, -1.0338e-02,  3.8678e-03,  1.1914e-03,\n",
      "        -5.9911e-03,  9.6193e-03, -4.8825e-03,  1.8162e-02, -7.5954e-04,\n",
      "         2.6088e-02,  2.2709e-02,  2.5063e-02,  2.1163e-02,  3.5743e-03,\n",
      "         1.8160e-02, -2.2503e-02, -1.9750e-03, -2.4769e-02,  2.7051e-02,\n",
      "        -2.7333e-02,  1.1141e-03,  2.8888e-02,  1.7231e-02, -1.8198e-03,\n",
      "        -8.1553e-03, -2.8065e-02, -1.4948e-02,  2.1834e-03,  2.3764e-02,\n",
      "        -2.9449e-02,  9.1197e-03, -1.9015e-02,  1.5191e-02,  7.5210e-03,\n",
      "         2.5235e-02,  1.8064e-02, -2.4156e-02, -5.9394e-03,  4.5072e-03,\n",
      "        -8.4966e-03, -1.9683e-02, -6.9152e-03, -2.1514e-02, -9.0678e-03,\n",
      "        -6.4310e-04, -5.2589e-03,  1.4151e-02,  9.3762e-03, -1.1623e-02,\n",
      "        -2.5445e-02, -1.7140e-02, -7.7443e-04, -2.0637e-02, -2.3389e-02,\n",
      "         7.5152e-03, -2.4929e-02, -1.2597e-02,  1.5459e-02, -1.6712e-02,\n",
      "        -5.0284e-03, -6.3438e-03,  1.1665e-02, -2.7737e-02, -5.3423e-03,\n",
      "        -1.4440e-02, -1.5744e-02,  3.7921e-03, -1.3385e-02,  1.6190e-02,\n",
      "        -2.1804e-02, -2.0627e-02, -6.9023e-03,  1.2876e-02,  2.9009e-02,\n",
      "         2.2724e-02,  7.0328e-03, -2.7086e-02,  2.6843e-02, -2.6427e-02,\n",
      "         2.8565e-02,  1.7473e-02,  1.6223e-02, -1.2340e-02, -5.1101e-03,\n",
      "        -6.7025e-03, -2.3678e-02, -1.1149e-02,  2.7140e-02,  1.0997e-04,\n",
      "        -3.9306e-03,  2.2272e-02, -1.1792e-02,  6.0275e-03,  2.5344e-02,\n",
      "         6.2027e-03, -1.3758e-02, -2.4231e-02, -9.5310e-03, -5.6836e-03,\n",
      "         1.3306e-02, -2.8491e-02,  2.1278e-02,  1.7222e-02,  1.1346e-02,\n",
      "        -2.4081e-03,  1.1813e-02,  1.4579e-02,  9.4646e-03, -2.5648e-02,\n",
      "        -1.9296e-02, -3.4292e-03,  4.1486e-03, -2.2994e-02,  7.6908e-03,\n",
      "        -1.5128e-02,  2.6733e-02, -1.0946e-02,  1.8928e-02,  2.6724e-02,\n",
      "        -1.7327e-02])\n",
      "Layer: layer3.4.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.5.conv1.bias, Biases: tensor([ 0.0057,  0.0029,  0.0080, -0.0198, -0.0082, -0.0137,  0.0199, -0.0086,\n",
      "         0.0185,  0.0071, -0.0150, -0.0132, -0.0033,  0.0129, -0.0163,  0.0117,\n",
      "         0.0149,  0.0005, -0.0197, -0.0167,  0.0123, -0.0204,  0.0176, -0.0171,\n",
      "         0.0132, -0.0003, -0.0024, -0.0040,  0.0149, -0.0189,  0.0148,  0.0113,\n",
      "        -0.0088,  0.0019, -0.0040,  0.0176,  0.0067,  0.0094,  0.0117, -0.0057,\n",
      "        -0.0116, -0.0042,  0.0043,  0.0201, -0.0185, -0.0165,  0.0005,  0.0195,\n",
      "        -0.0177,  0.0019,  0.0096, -0.0189, -0.0058,  0.0189,  0.0024,  0.0026,\n",
      "        -0.0058,  0.0193,  0.0114, -0.0063, -0.0049,  0.0205,  0.0005,  0.0097,\n",
      "        -0.0147, -0.0155,  0.0019,  0.0150, -0.0057, -0.0083, -0.0071, -0.0058,\n",
      "        -0.0046, -0.0119, -0.0022,  0.0048,  0.0203, -0.0069,  0.0129,  0.0181,\n",
      "         0.0186, -0.0198,  0.0037, -0.0123,  0.0088,  0.0072,  0.0058,  0.0050,\n",
      "        -0.0113,  0.0196,  0.0006,  0.0051, -0.0171, -0.0019, -0.0060, -0.0022,\n",
      "        -0.0019, -0.0159, -0.0029,  0.0166, -0.0035, -0.0197, -0.0172, -0.0103,\n",
      "        -0.0181,  0.0112,  0.0169,  0.0081,  0.0049,  0.0147,  0.0125, -0.0098,\n",
      "        -0.0180, -0.0001,  0.0087,  0.0079,  0.0204, -0.0115,  0.0094, -0.0133,\n",
      "        -0.0046,  0.0163, -0.0098, -0.0191,  0.0009, -0.0036,  0.0161,  0.0202])\n",
      "Layer: layer3.5.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.5.conv2.bias, Biases: tensor([ 0.0171,  0.0058,  0.0121, -0.0219, -0.0244, -0.0100, -0.0202,  0.0177,\n",
      "        -0.0170,  0.0051, -0.0052, -0.0163, -0.0060, -0.0200,  0.0185,  0.0161,\n",
      "         0.0027, -0.0064,  0.0266, -0.0272, -0.0249, -0.0127,  0.0037, -0.0279,\n",
      "        -0.0040,  0.0057, -0.0099, -0.0064,  0.0277, -0.0051,  0.0004, -0.0037,\n",
      "         0.0086,  0.0184,  0.0088, -0.0291,  0.0294,  0.0268,  0.0150, -0.0063,\n",
      "        -0.0281,  0.0185, -0.0104,  0.0245, -0.0237, -0.0184,  0.0220,  0.0277,\n",
      "        -0.0040,  0.0283,  0.0147,  0.0139, -0.0229,  0.0192,  0.0039, -0.0215,\n",
      "         0.0102, -0.0173,  0.0023,  0.0256,  0.0116, -0.0228,  0.0028,  0.0215,\n",
      "        -0.0010,  0.0168,  0.0091, -0.0101,  0.0057, -0.0174,  0.0016, -0.0096,\n",
      "        -0.0063, -0.0294,  0.0101,  0.0185,  0.0023, -0.0071, -0.0211, -0.0229,\n",
      "         0.0198, -0.0036,  0.0095, -0.0021, -0.0140,  0.0141, -0.0242,  0.0123,\n",
      "         0.0086, -0.0218,  0.0183,  0.0144,  0.0009, -0.0111,  0.0133, -0.0066,\n",
      "        -0.0035,  0.0188, -0.0106,  0.0011,  0.0039,  0.0245, -0.0289,  0.0063,\n",
      "         0.0048, -0.0190,  0.0044,  0.0170,  0.0087, -0.0258, -0.0091,  0.0082,\n",
      "        -0.0142,  0.0242,  0.0059, -0.0026,  0.0024, -0.0202,  0.0138,  0.0200,\n",
      "        -0.0241,  0.0047,  0.0050, -0.0006,  0.0100,  0.0031, -0.0160,  0.0187,\n",
      "        -0.0234,  0.0208,  0.0165, -0.0083, -0.0025, -0.0179,  0.0240, -0.0167,\n",
      "        -0.0201,  0.0203,  0.0186,  0.0259,  0.0202,  0.0029, -0.0166, -0.0059,\n",
      "         0.0071, -0.0043, -0.0017,  0.0282, -0.0247, -0.0014, -0.0022, -0.0009,\n",
      "         0.0044, -0.0218, -0.0143,  0.0063, -0.0225,  0.0140,  0.0079, -0.0153,\n",
      "        -0.0177,  0.0084,  0.0097,  0.0169, -0.0121, -0.0163,  0.0049, -0.0204,\n",
      "         0.0096, -0.0040,  0.0110, -0.0199,  0.0128, -0.0223, -0.0013,  0.0108,\n",
      "        -0.0003, -0.0208,  0.0196,  0.0172,  0.0159, -0.0106, -0.0017,  0.0138,\n",
      "        -0.0262, -0.0148, -0.0104, -0.0084, -0.0177,  0.0057,  0.0262, -0.0147,\n",
      "         0.0107,  0.0173, -0.0247,  0.0177, -0.0078, -0.0241,  0.0135,  0.0090,\n",
      "        -0.0184,  0.0191,  0.0100, -0.0172, -0.0106,  0.0290, -0.0078,  0.0013,\n",
      "        -0.0047, -0.0039, -0.0175, -0.0282, -0.0008, -0.0158, -0.0293,  0.0119,\n",
      "         0.0233,  0.0071, -0.0144, -0.0251,  0.0017,  0.0002,  0.0146, -0.0039,\n",
      "        -0.0169,  0.0242, -0.0228,  0.0183,  0.0090,  0.0093,  0.0057,  0.0123,\n",
      "         0.0015,  0.0199, -0.0171, -0.0271, -0.0257,  0.0077, -0.0092, -0.0069,\n",
      "         0.0132,  0.0235, -0.0004, -0.0044,  0.0195, -0.0158,  0.0225,  0.0068,\n",
      "        -0.0275,  0.0189, -0.0060, -0.0128,  0.0171, -0.0044,  0.0050, -0.0048])\n",
      "Layer: layer3.5.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer4.0.conv1.bias, Biases: tensor([-0.0156,  0.0146,  0.0081,  0.0203, -0.0193,  0.0080, -0.0174,  0.0094,\n",
      "         0.0109,  0.0078, -0.0054, -0.0111,  0.0012,  0.0193,  0.0132, -0.0151,\n",
      "        -0.0036,  0.0182,  0.0087,  0.0063, -0.0054,  0.0073,  0.0126, -0.0029,\n",
      "        -0.0140, -0.0113, -0.0061, -0.0049,  0.0100, -0.0065,  0.0035, -0.0100,\n",
      "        -0.0102,  0.0155,  0.0019, -0.0094,  0.0155, -0.0090,  0.0206,  0.0144,\n",
      "         0.0077, -0.0056,  0.0054, -0.0013, -0.0184,  0.0019, -0.0199, -0.0027,\n",
      "        -0.0145, -0.0205, -0.0124, -0.0112,  0.0120,  0.0129, -0.0073, -0.0120,\n",
      "        -0.0076,  0.0169,  0.0152, -0.0092,  0.0200,  0.0148,  0.0142,  0.0170,\n",
      "         0.0050,  0.0157,  0.0150,  0.0142, -0.0168,  0.0162, -0.0197, -0.0185,\n",
      "         0.0087, -0.0109, -0.0145,  0.0206, -0.0132, -0.0189, -0.0129,  0.0189,\n",
      "         0.0155, -0.0110, -0.0120, -0.0175, -0.0061, -0.0163, -0.0099,  0.0117,\n",
      "        -0.0012,  0.0022,  0.0094, -0.0069, -0.0053,  0.0028, -0.0068, -0.0173,\n",
      "         0.0135,  0.0071, -0.0122,  0.0075, -0.0115,  0.0160,  0.0089, -0.0018,\n",
      "         0.0026,  0.0100,  0.0105,  0.0079, -0.0184, -0.0021, -0.0030,  0.0201,\n",
      "         0.0029, -0.0096, -0.0173, -0.0053, -0.0186,  0.0196,  0.0203,  0.0109,\n",
      "         0.0048,  0.0151,  0.0132,  0.0204,  0.0047,  0.0191,  0.0195,  0.0163,\n",
      "         0.0011,  0.0005,  0.0014,  0.0082, -0.0084,  0.0079, -0.0093, -0.0106,\n",
      "         0.0112,  0.0201, -0.0173,  0.0191,  0.0159,  0.0145,  0.0136, -0.0131,\n",
      "        -0.0125, -0.0141, -0.0208, -0.0096,  0.0178, -0.0077, -0.0096, -0.0056,\n",
      "        -0.0088,  0.0030,  0.0039, -0.0124,  0.0185, -0.0084, -0.0081, -0.0174,\n",
      "         0.0146,  0.0036, -0.0017,  0.0072,  0.0115,  0.0011,  0.0001, -0.0114,\n",
      "        -0.0192, -0.0152,  0.0015,  0.0013,  0.0108,  0.0136,  0.0107,  0.0048,\n",
      "        -0.0073, -0.0082,  0.0203,  0.0049,  0.0109, -0.0111,  0.0143, -0.0159,\n",
      "         0.0051,  0.0135,  0.0064,  0.0090, -0.0031, -0.0073, -0.0041, -0.0095,\n",
      "        -0.0189, -0.0190,  0.0046,  0.0163, -0.0142, -0.0113,  0.0068,  0.0029,\n",
      "         0.0112, -0.0115, -0.0146, -0.0040, -0.0165, -0.0052, -0.0045, -0.0133,\n",
      "         0.0012,  0.0022, -0.0129,  0.0060,  0.0074, -0.0028,  0.0088,  0.0025,\n",
      "         0.0018, -0.0192, -0.0120, -0.0049, -0.0130, -0.0171, -0.0114,  0.0139,\n",
      "        -0.0049,  0.0122, -0.0004, -0.0006,  0.0179,  0.0134, -0.0132,  0.0062,\n",
      "         0.0014,  0.0140,  0.0165,  0.0006,  0.0104, -0.0129, -0.0180,  0.0050,\n",
      "         0.0048,  0.0189,  0.0129, -0.0092, -0.0159,  0.0157, -0.0153, -0.0140,\n",
      "         0.0054, -0.0099,  0.0128,  0.0119, -0.0148,  0.0019, -0.0183, -0.0047])\n",
      "Layer: layer4.0.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer4.0.conv2.bias, Biases: tensor([ 1.3765e-02,  1.4740e-02, -1.0612e-02,  5.3405e-03, -1.4006e-02,\n",
      "        -1.4072e-02, -1.4589e-02,  5.5985e-03, -1.0747e-02,  1.2954e-02,\n",
      "         1.4810e-02,  8.5028e-03, -1.0144e-02, -8.8501e-04,  1.8904e-02,\n",
      "         4.1408e-04,  1.9543e-02,  6.7378e-03,  1.1489e-02, -6.0997e-03,\n",
      "         1.2529e-02,  6.0398e-03, -8.6779e-03,  9.1714e-03, -1.6724e-02,\n",
      "         1.3025e-02,  8.3447e-03, -1.2607e-02, -2.4949e-03, -1.8692e-03,\n",
      "        -3.3616e-03, -9.9822e-03, -2.5778e-03, -8.5697e-03, -1.9821e-02,\n",
      "         1.4874e-02,  1.0860e-02, -1.0898e-02, -1.0420e-03, -9.0418e-03,\n",
      "         9.3423e-03, -2.0206e-02, -5.9144e-03, -1.2887e-03, -6.0565e-03,\n",
      "        -1.4179e-02,  7.6591e-03,  1.1822e-02,  1.6661e-02, -1.5591e-03,\n",
      "        -1.4037e-02, -2.4817e-04, -1.1394e-02,  3.6062e-03,  1.1334e-02,\n",
      "         2.0711e-04, -7.0387e-03, -4.3614e-03, -1.9107e-02, -6.6943e-04,\n",
      "        -1.3076e-02,  8.4202e-03, -1.0331e-02, -1.5092e-02,  2.4391e-03,\n",
      "        -2.0625e-02,  1.5745e-02, -6.5826e-03,  1.1383e-03, -1.2469e-02,\n",
      "         7.2087e-03, -6.7719e-03,  1.3967e-02,  6.8879e-03, -1.6473e-02,\n",
      "        -2.0548e-03,  1.5653e-02, -8.4883e-03, -1.2644e-03,  1.5236e-02,\n",
      "         1.8748e-02, -1.8904e-02,  5.3334e-03,  9.3238e-03,  2.0443e-02,\n",
      "         1.3596e-02,  1.7373e-02, -1.7306e-02,  2.0198e-02,  9.0604e-04,\n",
      "        -9.7101e-03,  8.9334e-03,  6.5540e-04,  7.7206e-03, -9.5750e-03,\n",
      "         4.8307e-03,  1.9209e-03, -1.0782e-02, -3.5669e-03,  1.5785e-02,\n",
      "        -4.7239e-03, -8.0004e-04, -8.8003e-03,  5.4633e-03,  1.2802e-02,\n",
      "        -1.4958e-02,  3.9261e-03,  1.0355e-02, -1.9185e-02, -5.1981e-03,\n",
      "        -9.1720e-03, -5.1231e-04,  4.1761e-03, -1.5050e-02, -1.9658e-02,\n",
      "        -1.1443e-02, -4.8362e-03, -2.3485e-03,  7.0310e-03, -8.1487e-04,\n",
      "        -1.9056e-02, -1.4284e-02,  9.0242e-03,  8.1498e-03, -4.7087e-03,\n",
      "        -3.6681e-03,  1.2781e-03, -1.2114e-02, -1.4573e-02,  1.7561e-02,\n",
      "         7.1350e-04, -2.0094e-02, -4.9356e-04, -4.5779e-03, -1.0728e-02,\n",
      "         1.3131e-02,  6.3346e-03,  1.9654e-02,  3.9554e-03, -1.6532e-02,\n",
      "        -8.4765e-03, -6.4274e-03, -1.7248e-02, -7.1218e-03,  8.2186e-03,\n",
      "         1.0673e-03, -1.1483e-02, -1.8431e-02, -1.2295e-02, -1.0556e-02,\n",
      "         9.8414e-03,  2.3929e-04,  2.0489e-02, -1.6819e-02, -1.6616e-02,\n",
      "        -1.7102e-02,  8.6025e-03, -5.6181e-03,  1.1124e-02,  1.1401e-02,\n",
      "         1.5858e-02, -1.9234e-02, -9.5521e-03, -1.8900e-02,  1.9348e-02,\n",
      "        -1.6560e-02, -1.8506e-03, -1.1400e-02, -4.9161e-03,  6.2820e-03,\n",
      "         1.5493e-02,  1.2441e-02,  1.9347e-02, -4.3297e-03, -4.8346e-03,\n",
      "        -1.6049e-02,  1.0056e-02, -1.6035e-02, -1.4795e-02, -3.8736e-03,\n",
      "        -1.1239e-02,  1.8005e-03, -1.0707e-02,  1.8687e-02,  1.1702e-02,\n",
      "        -4.5258e-03, -1.9768e-02, -7.2835e-03, -8.0871e-03, -1.5942e-02,\n",
      "         5.8234e-05,  1.5770e-02,  4.7803e-03, -1.1392e-02, -8.0352e-03,\n",
      "        -1.5413e-02,  1.8295e-02,  4.7079e-03, -5.1080e-03,  1.3183e-03,\n",
      "        -1.3373e-02, -1.9240e-02, -1.8885e-02,  3.0935e-03,  1.5318e-02,\n",
      "         1.0021e-03, -2.7508e-03,  4.7382e-03, -3.3127e-03, -3.4358e-03,\n",
      "        -2.0104e-02,  4.9270e-03, -1.3446e-02,  1.1774e-02,  1.6282e-02,\n",
      "         1.1243e-02,  1.2479e-03, -4.3122e-03, -4.8551e-03, -1.4516e-02,\n",
      "        -2.0036e-02, -1.6356e-02,  1.6837e-02, -2.0584e-02, -1.7420e-02,\n",
      "        -1.8508e-02, -9.7048e-03, -9.4348e-03, -1.8065e-02,  6.4259e-04,\n",
      "         6.4040e-03,  9.2064e-03,  4.4181e-04, -1.9832e-02,  1.7110e-02,\n",
      "         1.8496e-02,  7.9995e-03,  6.6219e-03, -1.2371e-02, -1.8167e-02,\n",
      "         1.3801e-03,  1.6743e-03, -1.0333e-03, -1.8637e-02,  9.2196e-03,\n",
      "         9.2114e-03, -1.8730e-02, -1.8822e-02,  7.4687e-03, -1.8595e-02,\n",
      "         3.1122e-03, -1.2381e-02,  1.3572e-02, -1.3799e-02, -1.1581e-02,\n",
      "        -1.2192e-02,  5.5247e-03, -3.8770e-03,  6.3422e-03, -1.1541e-02,\n",
      "        -7.4007e-03, -2.1756e-03,  8.5100e-03,  8.9742e-03, -8.8919e-03,\n",
      "        -1.8095e-02,  5.4255e-03,  1.0158e-03, -8.6299e-03, -2.0527e-02,\n",
      "         2.3395e-03,  2.5950e-03,  5.3275e-03, -5.3729e-03, -1.3664e-02,\n",
      "        -5.1580e-03, -1.3276e-02,  1.5484e-03, -1.2227e-02,  4.4610e-03,\n",
      "        -3.2442e-03,  2.0710e-02, -4.9940e-03, -1.4479e-02, -2.0231e-02,\n",
      "         2.6924e-03,  2.2443e-03,  3.9226e-03, -1.9882e-02, -1.4446e-02,\n",
      "         7.2234e-03, -1.0117e-02, -1.0697e-02, -2.0337e-02, -1.1067e-02,\n",
      "        -2.6405e-03, -1.3873e-02, -1.2675e-02, -2.7979e-03,  5.3264e-03,\n",
      "         1.6696e-02, -1.7699e-02, -1.7335e-02,  5.0459e-03, -9.6493e-03,\n",
      "        -1.5406e-02,  3.7970e-03,  1.2076e-02, -8.7230e-03, -1.9069e-02,\n",
      "         1.7447e-02, -1.1538e-02, -1.1205e-02, -4.0315e-03, -1.0695e-02,\n",
      "         1.0876e-02,  9.9132e-03, -1.5872e-02,  5.0351e-03,  1.5353e-02,\n",
      "        -1.1342e-02, -1.5305e-02, -2.4841e-03, -1.8439e-02,  1.0181e-02,\n",
      "        -7.0469e-03, -1.9387e-02, -9.8514e-03,  7.2191e-03, -1.2505e-02,\n",
      "         5.0574e-03,  1.0519e-02, -6.4575e-03,  6.9265e-04,  1.0941e-02,\n",
      "        -1.2686e-02,  1.9460e-02,  1.7205e-02,  4.2942e-03,  8.9667e-03,\n",
      "         6.1975e-03,  7.7496e-03,  6.3748e-03, -1.1436e-03, -1.7614e-02,\n",
      "        -1.4556e-02,  7.5850e-03,  1.1666e-02,  1.4619e-02,  1.7430e-02,\n",
      "         2.5961e-04, -1.8887e-02,  8.4324e-03,  4.5902e-03,  1.2051e-02,\n",
      "        -2.1837e-03,  3.9792e-03,  9.0464e-03, -1.2149e-02,  1.8697e-02,\n",
      "        -6.3611e-03, -2.6573e-03,  1.2327e-03,  3.8743e-03,  8.1947e-03,\n",
      "        -6.4972e-03, -1.4597e-02,  1.2999e-02, -3.4712e-04, -1.3620e-02,\n",
      "         1.4964e-02,  1.8796e-02,  9.7886e-03,  5.4402e-03,  9.1265e-03,\n",
      "        -2.2813e-03,  1.8548e-02,  1.2258e-03,  1.2513e-02, -1.0208e-02,\n",
      "        -2.0377e-02,  5.6791e-03, -7.2058e-03, -6.8761e-03, -1.3989e-02,\n",
      "        -1.5121e-02, -6.1496e-03, -5.2970e-03,  1.0164e-02,  1.8080e-03,\n",
      "        -7.0349e-03,  3.6639e-03,  6.2537e-03, -2.0776e-02, -4.9341e-03,\n",
      "        -1.9391e-02, -1.2467e-02, -4.2863e-03, -5.7746e-03, -1.4545e-02,\n",
      "         3.6721e-04, -2.0372e-02,  7.6001e-05,  1.4328e-02,  1.1458e-02,\n",
      "        -4.4046e-03, -8.6268e-03, -1.9175e-02,  1.6897e-02, -1.4049e-02,\n",
      "         1.8070e-02, -8.3619e-03,  1.0985e-02, -2.4514e-04,  4.8579e-03,\n",
      "        -9.9065e-03,  9.5098e-03, -4.7014e-03,  1.2842e-02,  5.5555e-03,\n",
      "         9.9394e-03, -2.9825e-03, -1.8072e-02,  2.9496e-03,  5.7335e-03,\n",
      "         1.4346e-02, -1.8048e-03,  4.7216e-03, -2.6316e-03,  1.7022e-02,\n",
      "         1.5616e-02,  9.0634e-03, -1.7934e-02,  5.9353e-03,  1.6176e-02,\n",
      "        -3.7825e-03, -2.6396e-03,  1.6078e-02,  1.5053e-02, -1.3446e-02,\n",
      "        -6.5521e-03,  1.6458e-02,  5.5307e-03, -2.0663e-02, -1.8515e-02,\n",
      "        -8.2555e-03, -2.2021e-03,  2.9561e-03,  8.4843e-03,  1.6016e-02,\n",
      "        -3.8868e-03, -6.1778e-03,  7.1046e-04,  1.8611e-02, -3.1721e-03,\n",
      "         2.0423e-02,  9.4763e-03, -1.2274e-02,  1.8492e-02, -1.3232e-02,\n",
      "         4.6243e-03,  9.0784e-03,  1.0567e-02,  1.2425e-02,  2.3199e-03,\n",
      "         1.8075e-02,  9.3749e-04, -3.1953e-03,  1.1383e-03,  1.6535e-02,\n",
      "        -1.6662e-02, -7.4809e-04,  1.6739e-02,  1.9643e-02,  4.0353e-03,\n",
      "        -1.3952e-02,  7.0739e-03, -1.3923e-03,  6.8560e-03, -1.5295e-02,\n",
      "        -1.2426e-02, -1.9489e-03,  1.8579e-02, -1.1944e-03,  1.7120e-03,\n",
      "         4.2446e-03,  3.4032e-03,  1.8927e-02,  6.2192e-03, -1.6324e-02,\n",
      "        -1.1113e-02, -9.9293e-03, -2.2351e-04,  7.9249e-03, -1.7175e-03,\n",
      "        -1.7140e-02, -1.7385e-02, -1.6873e-02, -1.8768e-02, -2.3811e-03,\n",
      "         7.5900e-03,  8.0644e-03,  3.7940e-03,  1.2882e-02,  6.9170e-03,\n",
      "        -1.6810e-02, -1.2771e-02, -4.5806e-03,  1.7705e-02,  3.9141e-03,\n",
      "         2.3223e-03,  1.0286e-02])\n",
      "Layer: layer4.0.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer4.0.identity_downsample.0.bias, Biases: tensor([ 0.0603, -0.0548,  0.0147,  0.0522, -0.0479,  0.0449, -0.0147,  0.0071,\n",
      "        -0.0032,  0.0532,  0.0164, -0.0021, -0.0357,  0.0175,  0.0190, -0.0453,\n",
      "         0.0170,  0.0273,  0.0145, -0.0319,  0.0348,  0.0008, -0.0535,  0.0025,\n",
      "        -0.0167,  0.0188,  0.0382,  0.0105, -0.0586,  0.0452, -0.0223,  0.0226,\n",
      "         0.0209, -0.0422,  0.0113, -0.0510,  0.0437, -0.0311,  0.0518,  0.0439,\n",
      "         0.0335,  0.0601,  0.0019, -0.0443,  0.0268, -0.0338,  0.0113,  0.0313,\n",
      "        -0.0426, -0.0356,  0.0242, -0.0236, -0.0085,  0.0364, -0.0416, -0.0091,\n",
      "         0.0469,  0.0549, -0.0138,  0.0573,  0.0444,  0.0548, -0.0149,  0.0345,\n",
      "        -0.0416,  0.0456,  0.0619, -0.0080, -0.0168, -0.0287, -0.0252,  0.0084,\n",
      "        -0.0054, -0.0462,  0.0473,  0.0206,  0.0477,  0.0418, -0.0516,  0.0602,\n",
      "         0.0012,  0.0566, -0.0191,  0.0105,  0.0325,  0.0502, -0.0607, -0.0448,\n",
      "         0.0414,  0.0233, -0.0217, -0.0536, -0.0600,  0.0304, -0.0609, -0.0043,\n",
      "         0.0004,  0.0329, -0.0615,  0.0270,  0.0392, -0.0187,  0.0306, -0.0206,\n",
      "         0.0124, -0.0070,  0.0230, -0.0603,  0.0043,  0.0590,  0.0505,  0.0537,\n",
      "        -0.0465,  0.0587,  0.0243,  0.0610, -0.0208, -0.0426,  0.0614,  0.0565,\n",
      "         0.0100, -0.0625, -0.0519,  0.0433,  0.0227, -0.0121, -0.0416, -0.0440,\n",
      "         0.0205,  0.0625,  0.0112, -0.0207,  0.0253,  0.0163, -0.0239, -0.0323,\n",
      "         0.0357, -0.0096, -0.0450, -0.0374,  0.0343, -0.0342,  0.0259,  0.0600,\n",
      "        -0.0570,  0.0520,  0.0121, -0.0612, -0.0004, -0.0464, -0.0584,  0.0037,\n",
      "         0.0265, -0.0582, -0.0008,  0.0486, -0.0345,  0.0242,  0.0327, -0.0583,\n",
      "        -0.0091, -0.0256, -0.0195,  0.0550, -0.0555,  0.0387,  0.0325, -0.0307,\n",
      "        -0.0426, -0.0249, -0.0333, -0.0533, -0.0071, -0.0141,  0.0131, -0.0170,\n",
      "         0.0184, -0.0577,  0.0604,  0.0456,  0.0183,  0.0600, -0.0575,  0.0212,\n",
      "         0.0570, -0.0024, -0.0539, -0.0157, -0.0331,  0.0194,  0.0162, -0.0576,\n",
      "         0.0597, -0.0393,  0.0607,  0.0124,  0.0426,  0.0045,  0.0335, -0.0466,\n",
      "         0.0390, -0.0374,  0.0146, -0.0613,  0.0365,  0.0085, -0.0520,  0.0299,\n",
      "         0.0151,  0.0432, -0.0222, -0.0008,  0.0212, -0.0574,  0.0485,  0.0516,\n",
      "        -0.0068, -0.0485, -0.0594, -0.0336,  0.0594,  0.0517,  0.0025,  0.0292,\n",
      "        -0.0559,  0.0182,  0.0310, -0.0337,  0.0208, -0.0088,  0.0013,  0.0215,\n",
      "         0.0126,  0.0424, -0.0200, -0.0370, -0.0007,  0.0321,  0.0111,  0.0549,\n",
      "        -0.0051, -0.0565,  0.0446,  0.0596,  0.0463, -0.0415, -0.0586, -0.0546,\n",
      "         0.0552, -0.0618,  0.0460, -0.0009,  0.0511,  0.0167, -0.0391,  0.0524,\n",
      "         0.0554,  0.0490,  0.0064, -0.0154,  0.0540, -0.0141,  0.0430, -0.0018,\n",
      "        -0.0578, -0.0475,  0.0099,  0.0547, -0.0309,  0.0104,  0.0068,  0.0131,\n",
      "        -0.0594, -0.0407, -0.0437,  0.0204,  0.0073, -0.0569,  0.0137,  0.0112,\n",
      "         0.0209,  0.0456,  0.0013, -0.0132,  0.0103,  0.0223, -0.0019, -0.0168,\n",
      "        -0.0531,  0.0072,  0.0420,  0.0239,  0.0153, -0.0027,  0.0055, -0.0618,\n",
      "         0.0476,  0.0523,  0.0324,  0.0453, -0.0558, -0.0407, -0.0536,  0.0291,\n",
      "         0.0501, -0.0076, -0.0527, -0.0173,  0.0189,  0.0141,  0.0284, -0.0156,\n",
      "         0.0439, -0.0272,  0.0004, -0.0084, -0.0548,  0.0289, -0.0054,  0.0140,\n",
      "        -0.0289, -0.0473,  0.0552,  0.0033, -0.0320, -0.0064,  0.0532,  0.0514,\n",
      "         0.0274,  0.0066, -0.0009, -0.0290, -0.0364,  0.0180, -0.0619, -0.0100,\n",
      "        -0.0475,  0.0578,  0.0279,  0.0326, -0.0035,  0.0249,  0.0565, -0.0395,\n",
      "        -0.0344,  0.0065,  0.0239, -0.0023, -0.0037, -0.0435, -0.0011, -0.0216,\n",
      "        -0.0175,  0.0056,  0.0364,  0.0191,  0.0364, -0.0310, -0.0069, -0.0204,\n",
      "         0.0464,  0.0544,  0.0297,  0.0422,  0.0356,  0.0194, -0.0619,  0.0384,\n",
      "         0.0341, -0.0563,  0.0224, -0.0079,  0.0538, -0.0333, -0.0524, -0.0222,\n",
      "        -0.0179,  0.0071, -0.0565, -0.0612,  0.0571,  0.0202,  0.0489,  0.0073,\n",
      "        -0.0330, -0.0330, -0.0002,  0.0175, -0.0311,  0.0597,  0.0448, -0.0421,\n",
      "        -0.0507, -0.0218, -0.0273, -0.0519, -0.0325, -0.0586, -0.0167,  0.0573,\n",
      "        -0.0255,  0.0270,  0.0325, -0.0311, -0.0572,  0.0568,  0.0250,  0.0084,\n",
      "         0.0378,  0.0492,  0.0487,  0.0275, -0.0215, -0.0467,  0.0058, -0.0122,\n",
      "        -0.0512, -0.0440, -0.0158, -0.0232, -0.0053, -0.0564,  0.0354,  0.0542,\n",
      "         0.0528, -0.0549,  0.0155, -0.0573,  0.0500, -0.0435,  0.0503, -0.0021,\n",
      "         0.0554,  0.0134,  0.0236,  0.0404, -0.0240,  0.0079, -0.0136,  0.0590,\n",
      "         0.0005, -0.0204, -0.0422, -0.0561, -0.0192, -0.0289,  0.0005,  0.0495,\n",
      "        -0.0149, -0.0102,  0.0116, -0.0167,  0.0035,  0.0456,  0.0166,  0.0117,\n",
      "         0.0122,  0.0099,  0.0198,  0.0150, -0.0009,  0.0579,  0.0333,  0.0597,\n",
      "         0.0097, -0.0289, -0.0467,  0.0433,  0.0400,  0.0361,  0.0239,  0.0489,\n",
      "         0.0489,  0.0434, -0.0267,  0.0295,  0.0150, -0.0231,  0.0112,  0.0381,\n",
      "         0.0090,  0.0203,  0.0235, -0.0619, -0.0411,  0.0244, -0.0162, -0.0076,\n",
      "         0.0386,  0.0182, -0.0524,  0.0357,  0.0244,  0.0457,  0.0566, -0.0383,\n",
      "         0.0468,  0.0549,  0.0560,  0.0065, -0.0105, -0.0489,  0.0360,  0.0479,\n",
      "        -0.0333,  0.0434,  0.0098, -0.0235,  0.0334, -0.0608,  0.0010, -0.0156])\n",
      "Layer: layer4.0.identity_downsample.1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer4.1.conv1.bias, Biases: tensor([-0.0014, -0.0105,  0.0009, -0.0062,  0.0098, -0.0077,  0.0079, -0.0006,\n",
      "         0.0083, -0.0031,  0.0128,  0.0117,  0.0026, -0.0085, -0.0028, -0.0135,\n",
      "         0.0138,  0.0061,  0.0129, -0.0093, -0.0079,  0.0050, -0.0031,  0.0039,\n",
      "         0.0011, -0.0064,  0.0129,  0.0123, -0.0054,  0.0133, -0.0145,  0.0034,\n",
      "        -0.0141, -0.0130, -0.0125,  0.0014,  0.0134,  0.0053, -0.0098,  0.0140,\n",
      "        -0.0089, -0.0047,  0.0076, -0.0049,  0.0106,  0.0076,  0.0054,  0.0094,\n",
      "         0.0037, -0.0106, -0.0129,  0.0110, -0.0069, -0.0084, -0.0080,  0.0088,\n",
      "         0.0062, -0.0063, -0.0113, -0.0049,  0.0135, -0.0116, -0.0073,  0.0124,\n",
      "         0.0039,  0.0135, -0.0066, -0.0068,  0.0130, -0.0039,  0.0046,  0.0133,\n",
      "        -0.0032, -0.0115, -0.0039, -0.0058,  0.0130,  0.0052, -0.0050,  0.0072,\n",
      "        -0.0030, -0.0003,  0.0120, -0.0130, -0.0116,  0.0037, -0.0016, -0.0017,\n",
      "         0.0035,  0.0121, -0.0008,  0.0059, -0.0010, -0.0068, -0.0083, -0.0016,\n",
      "        -0.0036,  0.0051,  0.0085, -0.0080, -0.0054,  0.0085, -0.0101, -0.0145,\n",
      "        -0.0109, -0.0132,  0.0038, -0.0067, -0.0136,  0.0141, -0.0071,  0.0048,\n",
      "         0.0094,  0.0057, -0.0051, -0.0078,  0.0011, -0.0074,  0.0116, -0.0041,\n",
      "        -0.0057,  0.0118, -0.0038,  0.0040,  0.0104,  0.0088, -0.0119,  0.0080,\n",
      "        -0.0122,  0.0114, -0.0136,  0.0047,  0.0130, -0.0099, -0.0138,  0.0137,\n",
      "         0.0141,  0.0033,  0.0012,  0.0094, -0.0079,  0.0039,  0.0139, -0.0008,\n",
      "        -0.0063,  0.0015, -0.0035,  0.0033, -0.0072, -0.0122, -0.0070,  0.0002,\n",
      "         0.0061,  0.0074,  0.0093, -0.0001, -0.0143,  0.0026,  0.0100,  0.0048,\n",
      "        -0.0059,  0.0016, -0.0083, -0.0138, -0.0090, -0.0015,  0.0007, -0.0064,\n",
      "         0.0125, -0.0141, -0.0110,  0.0113,  0.0081,  0.0004, -0.0065,  0.0040,\n",
      "         0.0135,  0.0094,  0.0077, -0.0025,  0.0064,  0.0030,  0.0031,  0.0008,\n",
      "         0.0029,  0.0128,  0.0084, -0.0124,  0.0032, -0.0087, -0.0049, -0.0020,\n",
      "        -0.0044,  0.0090, -0.0029, -0.0002, -0.0141, -0.0091, -0.0119, -0.0060,\n",
      "         0.0080,  0.0030, -0.0124, -0.0068, -0.0007,  0.0072,  0.0050,  0.0005,\n",
      "         0.0133,  0.0021,  0.0035,  0.0014, -0.0097, -0.0066, -0.0139,  0.0066,\n",
      "        -0.0136, -0.0021,  0.0044, -0.0092, -0.0006, -0.0092, -0.0058, -0.0022,\n",
      "         0.0117, -0.0098, -0.0009, -0.0117, -0.0062, -0.0122, -0.0099, -0.0134,\n",
      "        -0.0007,  0.0113,  0.0044, -0.0065,  0.0090, -0.0037, -0.0022,  0.0107,\n",
      "        -0.0069,  0.0123, -0.0074,  0.0003,  0.0082, -0.0096, -0.0103,  0.0102,\n",
      "        -0.0107, -0.0105,  0.0013,  0.0142,  0.0037, -0.0050, -0.0098,  0.0084])\n",
      "Layer: layer4.1.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer4.1.conv2.bias, Biases: tensor([ 1.5199e-02, -1.7845e-02, -1.2302e-03,  1.8389e-02,  1.4679e-02,\n",
      "         1.0126e-02, -6.7330e-03,  2.8789e-03, -5.2023e-04, -1.1249e-02,\n",
      "        -2.0630e-02, -1.2865e-02,  1.4928e-02, -1.0698e-02,  2.7326e-03,\n",
      "         1.7858e-02,  1.0339e-02,  1.3588e-03, -8.8313e-03,  1.8634e-05,\n",
      "         1.8681e-02, -7.6699e-04,  2.0378e-02,  5.5464e-03,  4.7132e-03,\n",
      "         1.6310e-02,  2.8418e-03,  1.8339e-02, -4.6599e-03, -1.0583e-02,\n",
      "        -2.8947e-03, -1.6340e-02,  7.7409e-04,  9.4357e-03,  1.2128e-02,\n",
      "         1.6681e-02, -4.7698e-04, -1.3928e-02,  7.8701e-03, -9.9335e-03,\n",
      "        -1.2024e-02,  4.7494e-03,  6.8651e-03, -1.4287e-02, -2.0388e-02,\n",
      "        -9.9072e-03, -5.3231e-03,  2.9970e-04,  1.5850e-02, -1.1193e-02,\n",
      "         3.7410e-03,  3.4584e-03,  2.0587e-02, -1.9989e-02,  1.9618e-02,\n",
      "         1.9853e-02, -5.5679e-03, -8.0877e-03, -8.4240e-03, -2.0530e-02,\n",
      "         1.9877e-02,  1.8411e-02, -4.6636e-03, -1.8490e-02,  6.5969e-03,\n",
      "        -7.6987e-03, -7.7523e-03, -1.4087e-02,  1.3799e-02, -6.9336e-03,\n",
      "        -4.6651e-03, -1.0068e-02, -8.8951e-04,  1.8533e-02, -3.7944e-03,\n",
      "        -7.0824e-03, -8.8618e-03,  1.7237e-02, -1.5750e-02,  5.7479e-03,\n",
      "         1.6544e-02,  1.8505e-02, -1.9949e-02, -1.8009e-02,  2.0401e-02,\n",
      "        -3.5286e-03, -1.9622e-02, -1.7467e-02,  9.7061e-03, -4.7585e-03,\n",
      "         2.0529e-02,  1.0820e-03,  1.2405e-02,  6.2519e-03,  7.6276e-03,\n",
      "        -1.7221e-02,  5.7093e-03, -2.0010e-03, -1.7947e-02,  2.0675e-02,\n",
      "         2.1743e-03, -1.0266e-02, -2.0273e-02, -2.0319e-02, -1.6587e-02,\n",
      "        -1.4293e-02, -1.9216e-02,  1.8505e-02, -8.5688e-03, -1.7458e-02,\n",
      "        -1.2263e-02,  8.4034e-03,  1.6635e-02, -1.1107e-02, -1.0465e-02,\n",
      "         2.0372e-02,  1.4297e-02,  1.3502e-02, -7.0105e-03, -1.2388e-02,\n",
      "        -1.1002e-03,  1.0740e-02,  1.6941e-02,  1.5103e-02,  1.7817e-03,\n",
      "        -5.5391e-03, -7.5897e-04, -1.8044e-02,  6.6154e-03, -5.6459e-03,\n",
      "        -1.1337e-02, -6.6461e-03,  7.8663e-03, -1.6224e-02,  7.4371e-03,\n",
      "         4.6686e-03, -3.9692e-04,  5.3003e-03, -1.3528e-02,  1.9497e-02,\n",
      "        -9.6083e-03, -1.4149e-02, -1.8910e-03,  1.2719e-02,  1.9770e-03,\n",
      "         1.0183e-02, -2.0108e-03, -1.0149e-02,  1.3280e-02, -1.4485e-02,\n",
      "         5.8515e-03, -3.1844e-04,  1.1515e-02, -1.6692e-02,  7.8074e-03,\n",
      "        -1.8231e-02,  2.0677e-02, -1.1843e-02, -3.8705e-03, -1.6911e-02,\n",
      "        -1.1030e-02,  1.7143e-02, -6.0158e-03,  1.3801e-02, -6.4536e-03,\n",
      "         1.4956e-02,  3.7527e-03,  4.0629e-03, -1.0985e-02,  1.1386e-02,\n",
      "         9.4957e-03,  9.1982e-03,  2.0232e-03, -8.2831e-03, -1.2837e-02,\n",
      "         1.6699e-02, -1.1197e-02, -9.1491e-03,  1.5976e-02,  1.9699e-02,\n",
      "         5.8728e-03, -7.1714e-03, -1.4590e-02,  9.6196e-03,  5.6582e-03,\n",
      "         6.6573e-03, -1.5361e-02, -1.2008e-02,  8.6295e-03,  1.5349e-02,\n",
      "         1.7532e-02,  2.2412e-03,  6.8870e-03, -7.8333e-05,  6.6079e-03,\n",
      "         1.6502e-02, -5.2655e-03, -8.7834e-03,  1.3932e-02,  7.9928e-03,\n",
      "         8.6016e-03,  1.7758e-02, -7.6458e-03, -6.3599e-03, -6.5547e-03,\n",
      "        -1.8251e-02, -6.2944e-03,  1.3595e-02,  1.5262e-02, -1.4109e-02,\n",
      "         1.3104e-02, -1.0481e-02, -8.8311e-03,  1.9870e-02, -1.6765e-02,\n",
      "         1.5260e-02,  7.9959e-03, -6.2018e-04,  1.2449e-02,  9.0802e-03,\n",
      "        -4.2137e-03,  1.2244e-02, -1.2982e-02,  1.5426e-02,  1.9602e-02,\n",
      "         1.6253e-02,  1.9391e-02, -8.4033e-03,  1.6260e-02,  9.4408e-03,\n",
      "        -8.7211e-03, -1.1923e-02, -6.5543e-03, -8.4319e-03, -1.9171e-02,\n",
      "        -1.3858e-03, -4.3631e-03, -2.7452e-03, -9.3483e-03, -1.4289e-02,\n",
      "        -1.1547e-02, -8.6914e-04,  1.9869e-02, -6.5116e-03,  1.0400e-02,\n",
      "        -1.9542e-02,  5.2907e-03,  1.8921e-02, -1.1704e-02, -1.4470e-02,\n",
      "        -5.3121e-03, -4.1770e-03, -1.3433e-02,  1.1781e-02, -1.1512e-02,\n",
      "        -2.0543e-02, -1.5300e-03, -5.6021e-03,  4.1037e-03,  2.0811e-02,\n",
      "         1.3331e-03, -1.3048e-02,  1.7437e-02, -6.7229e-03, -1.8654e-02,\n",
      "         2.8644e-03, -1.0481e-02,  9.7219e-03, -6.7284e-03, -5.1352e-03,\n",
      "        -1.3436e-02,  1.7542e-02, -8.5321e-03,  1.6516e-02,  1.4134e-02,\n",
      "        -1.9874e-03,  1.8233e-02,  1.6429e-04,  8.3448e-03, -8.2851e-03,\n",
      "         1.5333e-02, -1.7760e-02,  9.6508e-03, -2.8475e-03, -2.0492e-02,\n",
      "         1.7869e-02,  4.5285e-03,  1.1881e-02,  2.2716e-03,  1.6855e-03,\n",
      "         1.8153e-03,  5.7342e-03, -1.1716e-02,  1.6625e-03, -1.9461e-02,\n",
      "        -4.0353e-03, -2.4885e-03,  1.3999e-02, -1.4506e-02,  3.3930e-03,\n",
      "        -1.4246e-02, -1.0809e-02, -1.9184e-02,  1.3646e-02,  1.0632e-02,\n",
      "        -1.6535e-02,  1.2548e-02, -1.6266e-02,  2.0248e-02, -1.1516e-02,\n",
      "         6.9100e-03,  4.3763e-03, -1.8814e-02, -7.2331e-03,  6.9754e-03,\n",
      "        -1.9591e-02,  1.1927e-02, -1.8668e-02, -6.5083e-04, -1.6375e-02,\n",
      "        -1.8479e-02, -1.5096e-02, -2.0408e-02, -2.1758e-03, -1.1270e-02,\n",
      "        -1.3373e-02,  1.2201e-02, -6.5582e-03,  1.6726e-02,  1.7532e-03,\n",
      "        -5.9435e-03,  8.4382e-03,  3.3490e-03, -3.4680e-03, -1.0848e-02,\n",
      "         1.7291e-03, -1.4651e-02,  7.6517e-03,  8.1926e-03, -3.4837e-03,\n",
      "        -4.9623e-03, -3.2728e-03,  7.5832e-04,  1.9316e-02, -1.6669e-02,\n",
      "        -3.3885e-03, -1.9707e-02, -2.0064e-02, -2.0260e-02, -4.9156e-03,\n",
      "         1.4530e-02, -9.8136e-03, -1.6802e-03,  6.5839e-03, -7.0433e-03,\n",
      "         7.3269e-03,  1.1118e-02, -1.5859e-02,  1.0063e-02, -5.5207e-03,\n",
      "        -1.2525e-02, -2.0790e-02, -1.3203e-02, -1.1214e-02, -4.0928e-03,\n",
      "         2.0122e-02,  1.6512e-02, -3.9849e-03, -9.8084e-03,  1.5220e-02,\n",
      "         4.7219e-03, -1.9072e-03, -1.0791e-02, -3.2901e-04,  1.0105e-02,\n",
      "         5.2218e-03,  1.8927e-02, -7.9182e-03,  1.9001e-02, -8.9973e-03,\n",
      "         9.7786e-03,  2.0818e-02, -1.3551e-03, -1.7642e-02, -1.7639e-02,\n",
      "         9.6187e-03,  1.0727e-02, -6.7381e-03, -1.5057e-02, -2.0757e-02,\n",
      "         1.3374e-02, -1.9689e-02,  7.4301e-03,  5.5712e-03, -1.8396e-02,\n",
      "         4.5863e-03, -1.4855e-02, -2.5962e-03, -7.1699e-03, -1.9862e-02,\n",
      "        -5.1535e-03, -1.7162e-02,  5.9075e-03, -5.1983e-03,  1.6623e-02,\n",
      "        -1.2050e-02, -5.7831e-03, -2.7077e-03,  7.4264e-03,  1.4043e-02,\n",
      "        -2.9895e-03,  6.6679e-03, -1.2698e-02,  4.0192e-03, -1.0580e-02,\n",
      "         2.6095e-04, -3.7442e-03, -1.6151e-02, -1.8532e-03, -1.8198e-02,\n",
      "         4.1131e-03,  1.1213e-02,  1.9046e-02,  1.6770e-03,  6.6138e-04,\n",
      "        -3.0662e-03, -1.7538e-02, -9.3687e-03,  4.1212e-04,  1.7972e-02,\n",
      "        -4.7957e-03, -1.5760e-02, -1.4787e-02,  2.0536e-02,  3.1961e-03,\n",
      "        -2.0786e-02, -1.9294e-02,  1.9608e-02,  4.8551e-03,  6.1610e-03,\n",
      "        -1.8318e-02, -1.9648e-02, -6.7418e-03,  1.3237e-02, -7.5655e-03,\n",
      "        -7.0309e-03,  1.8014e-02,  1.7237e-02, -1.3669e-02, -2.0375e-02,\n",
      "        -1.0827e-02,  7.8010e-04, -8.0985e-03, -1.9246e-02,  1.5032e-02,\n",
      "         4.8546e-03, -6.0320e-03, -4.7077e-04,  5.2280e-03, -3.1147e-03,\n",
      "        -6.1512e-03, -1.7492e-02,  7.4064e-03,  1.0628e-02, -1.4206e-02,\n",
      "        -1.9203e-02,  8.0680e-04,  1.9552e-02,  1.6971e-02,  1.0413e-02,\n",
      "        -1.3105e-02,  3.5203e-03, -1.1537e-02,  5.3479e-04, -1.0533e-02,\n",
      "         1.3296e-02, -2.2163e-03,  3.6777e-03, -1.3687e-02,  6.7510e-03,\n",
      "        -3.7010e-03, -1.6627e-02, -1.4447e-02, -1.3303e-02,  2.0143e-02,\n",
      "         2.3178e-03,  1.2011e-03, -5.0032e-03,  1.1172e-02, -1.5335e-02,\n",
      "         1.2193e-02,  5.6855e-04,  6.0827e-03, -1.5148e-02, -1.7229e-03,\n",
      "         6.1980e-03,  1.6827e-02, -1.5083e-02, -4.4956e-03,  5.1165e-03,\n",
      "         1.4178e-02,  1.5819e-02,  3.5476e-03,  7.1251e-03, -5.3680e-03,\n",
      "        -1.2167e-02, -3.3101e-03,  2.0484e-03,  5.8315e-03,  6.0849e-03,\n",
      "        -6.4637e-03,  2.0779e-03])\n",
      "Layer: layer4.1.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer4.2.conv1.bias, Biases: tensor([ 9.3682e-03, -4.1722e-03,  1.1581e-02, -6.5032e-03,  8.5188e-03,\n",
      "        -1.4465e-02,  2.7675e-03,  1.7784e-03, -3.5415e-03, -4.4809e-03,\n",
      "        -1.2476e-02,  4.7020e-03,  4.8121e-03, -9.0379e-03, -5.3337e-03,\n",
      "        -1.1648e-04,  8.8740e-03,  2.5221e-03, -6.0579e-03, -7.8028e-03,\n",
      "        -1.4678e-02,  4.1180e-04,  5.7607e-04,  1.0646e-02, -1.0732e-02,\n",
      "         5.1202e-03,  1.3185e-02,  5.9123e-03, -6.9511e-03, -9.9030e-03,\n",
      "         3.7117e-03, -6.4812e-03, -5.0738e-04,  5.4789e-03, -1.2654e-02,\n",
      "         6.4852e-03,  8.2620e-04,  6.7317e-03,  8.8991e-03, -2.6900e-03,\n",
      "        -9.8081e-03,  1.0107e-02,  3.5049e-03, -6.6572e-03, -5.4737e-03,\n",
      "        -5.2805e-03,  1.0772e-02, -9.6076e-03, -4.8382e-03, -4.4192e-03,\n",
      "        -2.0151e-03,  9.3656e-03, -4.1239e-04, -3.0010e-03, -1.2432e-02,\n",
      "        -7.6654e-03,  1.0297e-02,  1.0828e-02,  8.5451e-04,  8.5574e-03,\n",
      "         2.2586e-03,  9.7317e-03, -3.5670e-03,  5.3622e-03,  1.4701e-03,\n",
      "        -1.3225e-02,  6.5341e-04,  5.4426e-03,  6.5854e-03,  1.3513e-02,\n",
      "        -1.2973e-02,  5.5535e-03,  1.1064e-02, -8.8265e-03, -1.8307e-03,\n",
      "        -8.9820e-03,  1.1772e-02, -7.2158e-03,  1.0078e-02, -1.4559e-02,\n",
      "         6.6436e-03,  5.9174e-03, -6.5715e-03, -1.0725e-02, -6.9630e-03,\n",
      "         1.0035e-02, -2.3657e-03, -1.0964e-02,  1.0779e-02,  9.4368e-04,\n",
      "         1.4720e-02,  4.6343e-03,  1.4466e-02, -1.2860e-02, -8.8880e-03,\n",
      "         1.4172e-02,  1.4126e-02,  9.6772e-03,  5.9545e-03,  2.6833e-03,\n",
      "         2.8268e-03,  1.1961e-02,  3.2284e-03, -1.2752e-02, -1.2962e-02,\n",
      "         1.1049e-02, -1.0915e-02,  7.0900e-04, -5.1425e-03, -5.1972e-03,\n",
      "         1.3245e-02,  5.6160e-03, -4.3190e-03,  6.2175e-03,  4.0538e-03,\n",
      "         2.4936e-03,  8.7042e-05,  2.0318e-05, -1.2433e-02, -1.0175e-02,\n",
      "        -7.7498e-03, -8.5263e-03, -1.0648e-02,  9.9828e-03, -5.7858e-03,\n",
      "         3.8584e-03,  8.6061e-03,  1.3306e-02, -1.1310e-02, -1.0412e-02,\n",
      "         1.0194e-02,  6.5201e-03,  1.4601e-02, -4.2934e-04, -1.4431e-02,\n",
      "         1.0284e-02, -1.9305e-03,  9.9015e-03,  4.1064e-03, -7.4343e-03,\n",
      "         1.2795e-02,  6.6004e-03,  7.7797e-03,  1.2748e-02,  1.3024e-02,\n",
      "         1.1349e-02, -4.8070e-03,  9.9124e-03,  1.2213e-02, -1.4706e-02,\n",
      "        -1.0974e-02, -1.3671e-02, -8.4236e-03, -8.3290e-03,  1.3509e-02,\n",
      "        -6.3432e-03, -1.4146e-02,  6.7011e-03, -6.8542e-03,  8.3136e-03,\n",
      "         7.9543e-03, -8.0232e-03, -8.4805e-03,  7.5118e-03,  1.0186e-02,\n",
      "        -1.0073e-03,  9.6844e-03,  4.4169e-03,  8.4589e-03,  1.1427e-02,\n",
      "        -1.1419e-03, -1.3404e-02, -4.7945e-04, -2.0723e-03, -2.7359e-03,\n",
      "         1.1756e-02,  7.9305e-03, -1.0805e-02,  6.1801e-03,  8.6720e-03,\n",
      "         5.4760e-03, -3.0642e-03,  9.2983e-03,  2.1435e-03,  9.3125e-03,\n",
      "         1.5753e-03, -2.8755e-03,  7.8167e-03, -8.1654e-03, -3.3734e-04,\n",
      "        -6.8223e-04, -5.5195e-03, -8.9993e-03, -1.0867e-02, -7.1313e-03,\n",
      "        -1.1492e-02, -1.1427e-02,  6.6959e-03, -1.0872e-02, -4.4127e-03,\n",
      "        -1.4020e-02,  1.0864e-03,  1.3827e-02, -1.1915e-02, -1.1127e-02,\n",
      "        -2.9082e-03,  3.5561e-03, -3.3063e-03,  8.5146e-03,  9.9396e-06,\n",
      "         7.9865e-03,  9.7709e-03, -7.4984e-03, -8.1169e-03, -8.6347e-03,\n",
      "         3.1856e-03,  6.6530e-03, -1.3582e-02, -1.2583e-02,  9.6625e-03,\n",
      "        -6.0676e-04,  2.9404e-03,  1.2988e-02, -1.3035e-03, -1.2481e-02,\n",
      "         6.6326e-03,  1.4235e-02,  1.9370e-04,  2.7450e-03,  4.3707e-03,\n",
      "        -1.3169e-02, -1.0045e-02,  5.4926e-03, -8.0911e-03, -1.4583e-02,\n",
      "        -1.1954e-02,  1.0369e-02, -4.4241e-03, -1.1244e-02,  1.0844e-02,\n",
      "         2.6647e-03,  4.2774e-03, -3.0725e-03, -1.0878e-02, -9.9914e-03,\n",
      "         2.3674e-03,  1.6288e-03,  7.6080e-03,  5.5499e-03,  4.8731e-03,\n",
      "         8.4742e-03, -7.2862e-03, -1.4564e-02,  1.1443e-02,  1.1796e-02,\n",
      "        -7.8289e-03])\n",
      "Layer: layer4.2.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer4.2.conv2.bias, Biases: tensor([ 1.9676e-02, -1.9132e-02,  1.1069e-02,  2.0015e-02,  1.8590e-03,\n",
      "         1.6306e-03,  1.5938e-03,  1.8004e-02,  7.7140e-03, -1.2875e-02,\n",
      "        -1.8989e-02,  1.6305e-02, -8.9685e-03,  1.2839e-02,  5.9523e-03,\n",
      "         4.6983e-03, -5.5649e-03,  6.9436e-03,  2.7493e-04,  2.0112e-02,\n",
      "        -1.9753e-02, -1.6472e-02,  9.0152e-04,  1.3232e-02, -5.3037e-03,\n",
      "        -4.6682e-03,  1.7773e-02, -1.7708e-02, -1.0816e-02, -1.2850e-02,\n",
      "        -4.5491e-03,  2.0809e-03,  7.1818e-03,  5.3426e-04,  1.2996e-02,\n",
      "         5.0701e-03,  2.0380e-03, -7.0342e-03,  8.2998e-04, -3.9411e-03,\n",
      "        -1.2916e-02,  1.6457e-02,  9.5291e-04,  7.2171e-03,  1.0698e-02,\n",
      "        -1.5331e-02,  2.0496e-03, -1.5722e-02, -1.1192e-02,  1.9181e-02,\n",
      "         1.6571e-02, -7.6390e-03, -1.7936e-03,  1.4673e-02,  1.6150e-02,\n",
      "         6.3749e-04,  3.2057e-03, -1.8343e-03, -1.9373e-02, -1.4114e-02,\n",
      "         1.0970e-02, -1.7302e-03,  1.2638e-02, -4.7656e-05,  2.2526e-03,\n",
      "        -7.0429e-03,  2.3888e-03, -6.0247e-03, -1.4658e-02, -1.4050e-02,\n",
      "        -9.2675e-03,  1.7701e-02,  2.4060e-03,  2.0544e-02, -1.6972e-02,\n",
      "        -1.0368e-02, -8.3638e-03, -6.4179e-03, -1.5351e-02,  3.6817e-03,\n",
      "         4.5902e-03,  1.8467e-02, -3.6965e-03, -1.4005e-02, -1.6961e-02,\n",
      "         8.2940e-03,  1.4960e-02, -2.3740e-03,  8.4103e-03,  1.1498e-02,\n",
      "         1.9593e-02,  6.1878e-03,  4.6711e-03, -1.4239e-02, -1.1959e-02,\n",
      "         9.1427e-03, -7.1469e-03,  7.6384e-03,  1.8201e-03, -1.1583e-03,\n",
      "         1.2150e-02, -2.0178e-02, -4.3117e-03,  7.5883e-03,  1.1834e-02,\n",
      "        -1.1640e-02,  1.1544e-02, -1.1933e-02, -1.9020e-02,  9.7383e-03,\n",
      "        -2.0133e-03,  1.9665e-02, -3.5888e-03,  2.0390e-03, -4.3218e-03,\n",
      "        -1.5398e-02,  1.6221e-02, -1.1678e-02, -9.3182e-03, -1.0004e-02,\n",
      "         1.1601e-02, -4.1868e-03,  9.2863e-03,  9.4348e-03, -7.1209e-03,\n",
      "         2.6140e-03,  4.7186e-03,  4.8087e-03,  5.2264e-03,  2.2278e-03,\n",
      "        -1.9203e-03, -2.0582e-02, -6.7119e-03, -1.2860e-05,  1.1684e-02,\n",
      "        -2.0154e-02,  5.6391e-03, -1.6779e-02,  3.5336e-03,  1.7969e-02,\n",
      "        -6.2027e-03,  1.7830e-02, -5.4155e-03, -1.0870e-02,  5.5656e-03,\n",
      "         1.5803e-02, -3.8117e-04,  8.9901e-03,  1.6352e-02,  4.5267e-03,\n",
      "         1.5811e-02,  1.9460e-02,  7.7137e-03,  1.2920e-02, -1.8580e-03,\n",
      "        -5.9031e-03,  3.2656e-03,  1.9296e-02, -6.5503e-04,  9.6576e-03,\n",
      "         1.2409e-02, -4.8426e-03,  2.7610e-03, -8.4659e-03,  4.2251e-03,\n",
      "        -1.7675e-02, -1.6851e-02,  1.9318e-03,  1.6925e-02, -1.8659e-02,\n",
      "         6.3621e-03,  1.0350e-02, -4.8945e-03,  1.3244e-02,  5.7373e-03,\n",
      "         1.4152e-02,  1.3590e-02, -1.5692e-03, -1.3938e-02,  3.0832e-03,\n",
      "        -9.7224e-03, -5.5564e-03, -1.6785e-02, -7.5485e-03, -7.2226e-03,\n",
      "        -1.8881e-02, -1.1846e-02,  1.9528e-02, -1.4358e-02,  6.1887e-03,\n",
      "         1.9481e-02, -9.7865e-03, -1.9353e-03, -1.7763e-02,  1.1641e-02,\n",
      "         1.0573e-02, -1.3732e-02,  1.3516e-02, -1.9373e-02, -1.6821e-02,\n",
      "        -6.1225e-03, -1.2213e-02, -7.3546e-03,  9.4954e-03,  1.1378e-03,\n",
      "        -1.3351e-03, -6.8740e-04, -1.3542e-02, -2.0438e-02,  1.5994e-02,\n",
      "        -1.7932e-02, -5.6519e-04,  1.3931e-02,  1.9589e-02,  5.8870e-03,\n",
      "        -1.5626e-02, -1.0555e-02,  9.5421e-03,  1.1102e-02, -1.4226e-02,\n",
      "         1.4018e-02, -4.4831e-03,  1.1314e-02,  1.9425e-02,  1.5914e-03,\n",
      "         1.4511e-02, -8.4878e-03,  7.7654e-03, -1.9972e-02,  4.4482e-03,\n",
      "        -1.3090e-02,  8.4041e-03,  1.3050e-02, -1.0565e-02, -4.1750e-03,\n",
      "         8.4134e-03,  2.0193e-02, -1.8733e-02, -1.3014e-02,  1.8955e-02,\n",
      "        -8.7161e-03,  6.9519e-03, -1.8161e-02, -1.8615e-02, -1.4363e-02,\n",
      "        -2.0188e-02, -3.9084e-03, -1.1382e-02,  1.3458e-02,  8.0871e-03,\n",
      "        -1.7825e-02, -1.3859e-02, -1.0042e-02,  6.8084e-03,  1.0842e-02,\n",
      "        -1.2485e-02, -1.8214e-02,  1.2897e-02, -1.8917e-02,  1.5330e-02,\n",
      "        -6.2920e-03, -3.2482e-04,  8.8173e-03, -9.2277e-03,  1.0951e-02,\n",
      "         2.0217e-02, -3.3509e-03, -1.8839e-02, -8.6774e-03,  1.9889e-02,\n",
      "         8.8177e-03, -7.2710e-03,  1.2150e-02,  1.7553e-03,  8.5406e-03,\n",
      "         2.0480e-02, -1.2960e-02, -1.5171e-02,  1.1974e-02,  1.2412e-02,\n",
      "         1.2388e-02, -1.0781e-02,  1.1342e-02,  1.2026e-02,  7.7685e-03,\n",
      "         5.2827e-03,  7.9564e-03,  1.5134e-03, -1.0119e-02,  2.4227e-03,\n",
      "         1.9521e-02,  1.9712e-02,  1.0870e-02,  5.3188e-03,  8.7529e-03,\n",
      "         1.6915e-02,  1.1224e-02, -4.1335e-03,  1.1307e-02, -8.9113e-03,\n",
      "         2.0265e-02, -1.3325e-02, -2.6089e-03,  1.1797e-02, -9.3126e-03,\n",
      "        -9.8561e-03,  1.1771e-02, -2.0627e-02,  4.2191e-03, -1.8053e-02,\n",
      "        -2.0277e-02,  8.1203e-03,  1.9934e-02, -8.1301e-03,  1.2967e-02,\n",
      "        -1.2033e-02,  9.0368e-03, -1.8620e-02, -1.5313e-02, -2.0548e-02,\n",
      "        -1.7864e-02, -1.9037e-03,  1.0928e-02, -1.2836e-02, -1.4980e-02,\n",
      "        -9.5913e-03,  1.1349e-03,  1.4689e-02,  9.3633e-03,  1.7616e-02,\n",
      "        -6.0883e-03,  6.0395e-04,  4.1950e-03, -9.1472e-03, -1.4024e-02,\n",
      "        -1.2507e-02, -5.3172e-03,  1.8124e-02,  1.3813e-02,  8.5361e-03,\n",
      "         1.7052e-02,  1.0071e-02,  2.0208e-02, -2.3965e-03,  1.7137e-02,\n",
      "         9.7978e-03, -9.1545e-03,  5.3260e-03, -3.9308e-03,  1.9629e-02,\n",
      "        -4.3704e-03, -4.9021e-03, -2.0209e-02,  8.1870e-03,  1.7602e-02,\n",
      "         9.6315e-03,  3.4292e-03, -1.7346e-02,  1.5424e-02, -9.5810e-04,\n",
      "         3.8639e-03,  7.2184e-03, -1.0248e-02,  5.1627e-04,  2.8095e-04,\n",
      "         1.4008e-02, -4.0049e-03,  1.8819e-02,  1.3732e-02,  1.0278e-02,\n",
      "        -1.9338e-02,  7.1009e-03,  1.2691e-04,  6.0651e-03,  1.9580e-02,\n",
      "        -1.7147e-02, -7.8351e-03, -1.4333e-02, -1.6092e-02,  3.6434e-03,\n",
      "         7.8077e-03,  5.6623e-03, -1.9759e-02, -1.3051e-02,  8.6858e-03,\n",
      "         1.7776e-02,  4.2477e-04, -3.5984e-03,  7.8574e-04, -2.0703e-02,\n",
      "        -2.6894e-03, -5.0318e-04,  1.6213e-02,  1.4290e-02,  2.0771e-03,\n",
      "         7.0437e-03,  1.0242e-02, -1.4219e-02,  1.5681e-02,  2.0438e-02,\n",
      "         3.2626e-03, -1.6024e-02,  6.0750e-03,  1.9675e-02, -1.8078e-03,\n",
      "        -2.0110e-02, -5.3582e-03,  2.0422e-02,  1.2561e-02,  1.2273e-02,\n",
      "         2.0426e-02, -1.1573e-02, -1.7763e-02, -2.0797e-02,  8.9059e-03,\n",
      "         8.9929e-03, -1.4221e-02,  7.9976e-04, -1.6091e-02, -6.4666e-03,\n",
      "        -7.5963e-03, -3.8970e-03,  1.9371e-02,  1.4322e-02,  1.7688e-02,\n",
      "         1.2319e-02,  8.5255e-03, -1.5005e-02, -5.0069e-03,  5.0722e-03,\n",
      "         1.1086e-03,  1.0181e-02, -5.4396e-03,  9.4542e-03, -3.5443e-03,\n",
      "         1.6708e-03, -6.8464e-03,  8.5191e-03, -1.1110e-02,  1.2166e-02,\n",
      "         9.2774e-03, -1.9123e-02, -9.0813e-03, -2.7920e-03, -2.0394e-02,\n",
      "         9.2952e-03, -1.3572e-02,  2.0720e-02,  1.1542e-02, -7.7217e-04,\n",
      "         3.9156e-03,  1.4967e-02, -4.7084e-03, -9.4954e-03,  1.9929e-02,\n",
      "         1.1252e-04,  9.1381e-03,  9.8932e-03, -3.7219e-03,  1.8853e-02,\n",
      "         9.0345e-03,  1.4199e-02, -1.6056e-02,  8.5559e-03,  7.9940e-05,\n",
      "        -7.7756e-03, -1.0710e-02,  1.7670e-02, -8.7790e-03, -6.6371e-03,\n",
      "         7.6036e-03,  1.3414e-02, -1.0780e-03,  9.6259e-03, -1.3659e-02,\n",
      "         5.1409e-03, -1.9122e-02,  5.6131e-04,  1.9030e-02,  1.4086e-02,\n",
      "        -1.4515e-02,  1.4358e-02, -1.8574e-02, -2.0166e-02, -1.6251e-03,\n",
      "         3.8393e-03, -8.5676e-03, -1.7384e-02,  1.8879e-02,  1.6862e-02,\n",
      "         5.5954e-03, -1.0047e-02, -1.0624e-02,  2.0056e-02, -8.3059e-03,\n",
      "         1.0325e-02, -1.0289e-02,  6.3456e-03,  1.8034e-02, -1.2089e-02,\n",
      "         1.0874e-02, -9.4900e-03, -1.1823e-02, -1.5061e-02,  1.9269e-02,\n",
      "        -1.1768e-02, -1.6916e-02,  1.9984e-02,  2.8954e-03,  1.6810e-02,\n",
      "         1.9568e-02, -1.9695e-02])\n",
      "Layer: layer4.2.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: KANClassifier.1.fouriercoeffs, Fourier Coefficients:\n",
      "tensor([[[[ 6.3978e-05,  2.3308e-04, -9.8701e-05,  ...,  9.4420e-05,\n",
      "            1.8240e-04, -1.1672e-04],\n",
      "          [ 1.3392e-04, -1.2945e-04,  5.5385e-04,  ..., -6.5121e-05,\n",
      "           -5.5909e-04, -1.1596e-04],\n",
      "          [-2.8131e-04, -4.4702e-04,  2.5195e-05,  ..., -3.2260e-04,\n",
      "            1.5400e-04, -6.4543e-04],\n",
      "          ...,\n",
      "          [ 3.1799e-04, -4.5881e-05,  3.6122e-04,  ..., -1.8465e-04,\n",
      "            2.4467e-04, -4.3425e-04],\n",
      "          [-6.2980e-04, -1.4404e-05, -3.9743e-05,  ...,  1.5673e-04,\n",
      "           -2.3028e-04, -5.8721e-05],\n",
      "          [ 4.3111e-04, -4.4426e-04,  2.6224e-04,  ...,  1.5621e-04,\n",
      "           -3.2919e-04, -5.6566e-04]],\n",
      "\n",
      "         [[ 4.9611e-04, -3.6887e-04,  3.2105e-04,  ...,  6.7401e-04,\n",
      "            5.3724e-04, -2.9038e-04],\n",
      "          [ 3.0975e-04,  3.2453e-04,  5.2635e-04,  ..., -5.4016e-04,\n",
      "           -6.4446e-04, -6.5777e-04],\n",
      "          [-6.6132e-05, -6.3281e-04, -4.7097e-04,  ...,  2.1075e-04,\n",
      "           -1.1257e-04,  4.3876e-04],\n",
      "          ...,\n",
      "          [-5.9296e-04, -5.6050e-04,  1.1641e-04,  ...,  3.6884e-04,\n",
      "            2.9400e-04, -6.2494e-04],\n",
      "          [ 2.3180e-04, -3.5406e-05, -8.3096e-05,  ..., -3.7477e-04,\n",
      "            4.2182e-04,  9.5085e-05],\n",
      "          [ 3.9024e-04, -1.5111e-04, -5.1184e-04,  ...,  1.8754e-04,\n",
      "            6.1071e-04,  1.2653e-04]],\n",
      "\n",
      "         [[ 5.8536e-04, -3.4328e-04,  6.0701e-04,  ...,  5.9528e-05,\n",
      "           -1.2853e-04,  2.0627e-04],\n",
      "          [ 2.8984e-05, -1.9066e-04, -2.0959e-05,  ...,  2.2694e-04,\n",
      "           -3.8272e-04, -4.8148e-04],\n",
      "          [-1.5598e-04,  6.1480e-04,  5.6431e-04,  ..., -3.5748e-04,\n",
      "           -5.3492e-05,  5.1331e-04],\n",
      "          ...,\n",
      "          [-5.2646e-04,  5.6417e-04, -4.7598e-04,  ..., -1.8709e-04,\n",
      "            6.7250e-04, -1.4324e-04],\n",
      "          [ 1.2058e-04,  5.7980e-04,  3.2335e-04,  ..., -4.2249e-04,\n",
      "            4.8160e-04, -2.7869e-04],\n",
      "          [-3.7331e-04, -6.3885e-04, -6.6226e-04,  ..., -5.8061e-05,\n",
      "           -5.9209e-04,  5.0145e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7509e-04,  2.7499e-04, -1.7304e-04,  ...,  6.4367e-04,\n",
      "           -2.4284e-04,  6.4076e-04],\n",
      "          [-2.7080e-04,  4.5046e-04,  6.4417e-04,  ...,  4.1192e-04,\n",
      "            1.1821e-04, -4.9468e-04],\n",
      "          [-2.7057e-04, -1.3019e-04, -3.6537e-05,  ..., -5.5924e-04,\n",
      "            4.1405e-04, -1.9050e-05],\n",
      "          ...,\n",
      "          [ 1.3379e-04, -2.7424e-04, -5.8416e-04,  ..., -4.7216e-04,\n",
      "           -2.3165e-04, -1.4166e-04],\n",
      "          [-3.1121e-04, -5.6394e-04, -4.5055e-07,  ...,  4.6593e-04,\n",
      "            1.6324e-04,  4.3871e-04],\n",
      "          [-2.0093e-04,  1.1646e-04,  2.1191e-04,  ..., -8.4016e-05,\n",
      "           -3.4356e-05, -1.2219e-04]],\n",
      "\n",
      "         [[-6.4735e-04, -3.5905e-04,  1.0860e-04,  ...,  5.6365e-04,\n",
      "           -3.4209e-04,  6.4380e-04],\n",
      "          [-2.0044e-04,  4.4684e-04,  3.1241e-04,  ..., -3.6885e-04,\n",
      "            6.6229e-04, -6.5454e-06],\n",
      "          [ 2.1440e-04,  5.6698e-04, -6.2749e-04,  ..., -6.5724e-05,\n",
      "           -2.9382e-04,  4.5778e-04],\n",
      "          ...,\n",
      "          [-3.1274e-04, -1.3124e-04, -6.4122e-04,  ..., -5.8382e-04,\n",
      "           -5.1082e-04, -1.2183e-04],\n",
      "          [ 2.4910e-04,  1.1692e-04, -6.3411e-04,  ..., -1.7068e-04,\n",
      "           -6.2492e-04,  1.4144e-04],\n",
      "          [ 5.3576e-04, -5.9994e-04,  4.5176e-04,  ..., -2.4376e-04,\n",
      "            4.9680e-04,  4.1812e-04]],\n",
      "\n",
      "         [[ 4.3484e-04,  4.0708e-04, -4.4372e-05,  ...,  8.4906e-05,\n",
      "           -9.4596e-05,  8.9246e-05],\n",
      "          [ 6.0124e-04, -7.1734e-05, -5.3099e-04,  ...,  6.1247e-04,\n",
      "            2.1987e-04, -1.0428e-04],\n",
      "          [-5.4162e-04, -4.9251e-05, -2.8000e-04,  ..., -6.5655e-04,\n",
      "            2.2631e-04, -3.9827e-04],\n",
      "          ...,\n",
      "          [-3.6330e-04,  5.4121e-04,  4.6446e-04,  ...,  3.7555e-04,\n",
      "            5.4723e-04, -6.3963e-04],\n",
      "          [-2.4627e-04, -9.4020e-05,  6.3640e-04,  ...,  3.2385e-04,\n",
      "           -3.2253e-05, -1.8732e-04],\n",
      "          [ 3.9729e-04,  6.9838e-05, -5.3346e-04,  ..., -1.7785e-04,\n",
      "           -2.4302e-04, -2.4889e-05]]],\n",
      "\n",
      "\n",
      "        [[[-6.6866e-04, -2.2606e-04,  4.3154e-04,  ..., -1.6320e-04,\n",
      "            1.1883e-04,  5.9185e-04],\n",
      "          [-2.6806e-04,  3.5762e-04,  4.2095e-04,  ...,  4.9412e-04,\n",
      "            2.8365e-04,  5.4495e-04],\n",
      "          [ 3.3020e-04, -5.0262e-04,  5.0745e-04,  ...,  4.8513e-04,\n",
      "            4.6908e-04, -4.9786e-04],\n",
      "          ...,\n",
      "          [-5.2886e-04,  5.9009e-04,  3.6193e-04,  ...,  4.1052e-04,\n",
      "           -3.6436e-04,  6.7166e-04],\n",
      "          [-5.5136e-04, -4.2658e-04,  5.3271e-05,  ...,  2.2488e-04,\n",
      "            6.6602e-04,  6.6599e-04],\n",
      "          [-4.8954e-05, -6.6455e-04, -6.0025e-04,  ...,  3.5366e-05,\n",
      "            3.1346e-04,  4.4194e-04]],\n",
      "\n",
      "         [[-8.9616e-06,  5.3512e-04, -4.0650e-04,  ..., -7.2851e-05,\n",
      "            1.9309e-05,  5.3098e-04],\n",
      "          [ 2.7624e-04, -2.0725e-04, -5.3757e-05,  ..., -1.8412e-04,\n",
      "            3.4977e-04, -5.2076e-04],\n",
      "          [-2.3514e-04,  4.3317e-04, -5.2021e-04,  ...,  3.4043e-04,\n",
      "            1.8995e-04,  6.0746e-04],\n",
      "          ...,\n",
      "          [-6.3680e-04, -5.8644e-04, -4.5065e-04,  ..., -4.2386e-05,\n",
      "           -3.2916e-04,  3.5397e-05],\n",
      "          [ 5.2510e-04, -5.6667e-04,  2.1614e-04,  ..., -5.6965e-04,\n",
      "            2.7662e-05,  5.2557e-04],\n",
      "          [-4.1360e-04, -6.2944e-04,  1.8772e-05,  ...,  8.7325e-05,\n",
      "            3.3468e-04,  2.8983e-04]],\n",
      "\n",
      "         [[ 1.2027e-04, -5.4781e-05, -6.6811e-04,  ...,  2.1439e-04,\n",
      "            1.9955e-04,  4.5803e-04],\n",
      "          [-1.0527e-04,  2.3150e-04, -3.1792e-04,  ...,  3.2250e-04,\n",
      "            1.2219e-04,  6.2686e-04],\n",
      "          [-4.3287e-04, -1.0488e-04, -6.2663e-04,  ..., -9.9573e-05,\n",
      "           -3.5598e-04, -4.6297e-04],\n",
      "          ...,\n",
      "          [ 3.9579e-05,  6.5774e-04, -1.8289e-04,  ..., -6.0231e-04,\n",
      "           -6.3206e-04,  5.6951e-04],\n",
      "          [-2.4247e-04,  8.9606e-05,  1.4905e-06,  ...,  2.4967e-04,\n",
      "            1.7666e-04,  1.4960e-04],\n",
      "          [ 3.2977e-04,  4.3391e-04, -1.9410e-04,  ..., -1.9758e-04,\n",
      "            3.0377e-04, -6.2353e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.4455e-04, -4.5616e-04, -4.5219e-04,  ...,  6.8155e-06,\n",
      "            3.7631e-04,  1.1513e-04],\n",
      "          [-6.2376e-04,  4.8252e-04,  4.3154e-04,  ...,  5.1337e-04,\n",
      "           -5.6500e-05, -2.8315e-04],\n",
      "          [ 2.8528e-04, -3.5591e-04, -2.9911e-04,  ...,  8.7182e-05,\n",
      "            1.9612e-04,  4.9840e-04],\n",
      "          ...,\n",
      "          [ 3.8054e-04,  5.6263e-04, -8.6080e-05,  ..., -4.8665e-04,\n",
      "            5.5413e-05,  1.9101e-04],\n",
      "          [ 2.3737e-04, -3.9530e-04,  4.6587e-04,  ..., -6.5418e-04,\n",
      "           -1.8492e-04,  5.5093e-05],\n",
      "          [ 7.8088e-05,  2.0966e-04, -4.3057e-04,  ..., -4.1414e-04,\n",
      "            2.9659e-04, -6.0115e-05]],\n",
      "\n",
      "         [[ 5.7259e-05, -4.5442e-06, -4.5211e-04,  ..., -6.0786e-04,\n",
      "           -4.0574e-04,  3.3679e-04],\n",
      "          [-3.9212e-04,  3.7185e-04,  3.0759e-04,  ...,  4.7378e-04,\n",
      "            1.8712e-04,  1.9966e-04],\n",
      "          [ 6.4649e-05,  3.5609e-04,  5.4497e-05,  ..., -1.0952e-04,\n",
      "            2.9492e-04,  6.1537e-04],\n",
      "          ...,\n",
      "          [ 1.8113e-04, -2.0548e-04,  4.1656e-04,  ..., -1.4405e-04,\n",
      "           -2.4698e-04, -3.2382e-04],\n",
      "          [-9.1607e-05,  5.3634e-04,  4.2993e-04,  ..., -4.5429e-04,\n",
      "           -6.6248e-04, -2.4402e-04],\n",
      "          [-7.8688e-05, -8.5138e-05, -5.3537e-04,  ...,  5.8810e-04,\n",
      "            5.6498e-04, -3.6014e-04]],\n",
      "\n",
      "         [[ 3.2668e-04,  2.6296e-05,  3.6885e-04,  ...,  2.9996e-04,\n",
      "           -2.2319e-04, -4.2542e-05],\n",
      "          [ 4.4677e-04, -2.2727e-04, -2.4142e-04,  ...,  5.7291e-04,\n",
      "            8.7014e-05,  3.1593e-04],\n",
      "          [ 4.2042e-04, -1.1396e-04,  5.0536e-04,  ..., -2.8122e-04,\n",
      "            2.5336e-04,  5.7375e-04],\n",
      "          ...,\n",
      "          [-4.9490e-07, -6.3284e-04, -1.1826e-04,  ..., -4.6002e-05,\n",
      "            1.7219e-06, -1.8045e-04],\n",
      "          [ 6.1537e-05, -2.5966e-04,  6.1978e-04,  ...,  9.2962e-05,\n",
      "            4.8513e-04, -3.2694e-04],\n",
      "          [-6.4580e-04,  1.4274e-04, -5.0458e-04,  ..., -3.9644e-06,\n",
      "           -1.5495e-04,  5.7504e-04]]]])\n",
      "\n",
      "Layer: KANClassifier.1.bias, Biases: tensor([[ 2.9345e-02,  4.5927e-02,  1.8174e-02, -3.4845e-02,  3.8175e-02,\n",
      "         -4.5270e-02,  2.7121e-02, -1.3747e-01, -2.0378e-02, -8.5368e-02,\n",
      "         -2.9900e-02, -6.4528e-02, -8.4944e-03, -9.7197e-02,  2.0515e-02,\n",
      "         -3.7362e-02,  1.3610e-02,  5.2184e-02, -7.0279e-02, -3.2340e-03,\n",
      "          6.3943e-02,  2.6575e-02, -2.1395e-02, -3.6436e-02,  8.4798e-03,\n",
      "         -5.3878e-02,  9.0245e-02,  6.0223e-03, -1.1406e-01,  4.5293e-02,\n",
      "         -5.5604e-02, -2.8784e-02, -7.3753e-02, -2.6227e-02, -3.0062e-02,\n",
      "          8.5129e-02, -5.4364e-02,  7.2302e-03,  4.5771e-03,  8.7095e-03,\n",
      "         -2.8587e-02,  3.2481e-02,  4.0841e-02, -2.1795e-02,  2.2172e-02,\n",
      "         -1.2222e-01,  6.9286e-02,  1.9375e-02,  5.8190e-02,  6.1677e-02,\n",
      "         -3.0314e-02,  4.8765e-02,  9.9441e-02, -2.9069e-02, -2.0435e-02,\n",
      "          5.9093e-02,  2.5902e-02,  3.0480e-02, -2.3986e-02,  6.6327e-02,\n",
      "          1.1667e-01, -8.4043e-02,  6.3448e-02, -4.2561e-02, -1.0809e-01,\n",
      "          4.4825e-03,  1.2274e-01, -1.4435e-01, -6.4021e-02, -7.6348e-02,\n",
      "          4.4705e-02,  2.5559e-02,  2.2742e-02, -8.0274e-03, -6.1441e-02,\n",
      "          9.7887e-02,  8.3478e-02, -5.2031e-02,  1.1474e-01,  6.1990e-03,\n",
      "          6.4087e-03, -3.4630e-02,  3.7585e-02,  8.9173e-02,  1.8234e-02,\n",
      "         -7.4944e-02,  1.3921e-01,  7.8469e-02,  1.2300e-03, -5.7890e-02,\n",
      "         -4.1320e-02, -1.8497e-02, -4.1325e-02, -2.2524e-02,  3.3884e-02,\n",
      "         -4.4409e-02,  1.0313e-01, -8.4253e-02, -6.4802e-02,  2.7720e-03,\n",
      "         -4.4916e-02, -2.0568e-02, -9.1503e-02, -1.3777e-02, -1.8702e-02,\n",
      "          1.5963e-01,  1.2364e-02,  6.1077e-02,  8.2233e-02, -6.0106e-02,\n",
      "          2.1964e-02, -1.4016e-01, -9.5651e-03,  2.0603e-03,  1.2730e-01,\n",
      "          9.4696e-02,  5.5195e-02, -1.3497e-02, -2.7087e-03, -7.2618e-03,\n",
      "         -5.0458e-02, -6.1726e-02,  1.1409e-02, -3.1942e-02,  6.9332e-03,\n",
      "          7.5269e-02,  8.4429e-02, -3.1253e-02, -9.0497e-02, -1.5132e-02,\n",
      "          4.9475e-02,  1.9625e-02,  5.8063e-02, -1.0958e-01, -1.0123e-01,\n",
      "         -3.0826e-02,  3.1382e-02,  7.2859e-02,  8.0211e-02, -4.0762e-02,\n",
      "         -5.2809e-02,  5.3243e-02,  5.3090e-02,  1.4405e-01, -8.3589e-02,\n",
      "          2.3303e-02, -1.1849e-02, -9.5192e-02,  1.0765e-02, -4.5323e-03,\n",
      "          3.9906e-02,  5.3557e-02, -5.7552e-02, -1.4623e-02,  7.2623e-02,\n",
      "          7.7427e-02, -1.3403e-02,  6.9595e-02, -7.6972e-03, -9.3355e-02,\n",
      "         -9.3299e-03, -8.3011e-02,  4.9191e-02, -5.4260e-02, -8.1185e-02,\n",
      "         -7.6385e-02, -9.8201e-02,  8.2769e-02, -4.0024e-02, -1.7348e-01,\n",
      "          1.8204e-02,  9.5200e-03, -4.0402e-02, -1.0340e-01, -9.0174e-02,\n",
      "          1.3141e-02,  4.3798e-02,  3.6781e-02,  1.1343e-02, -6.8694e-02,\n",
      "         -4.0584e-02, -5.3160e-02,  2.6189e-02,  1.3112e-02,  6.3966e-02,\n",
      "         -1.1870e-01,  1.1940e-04, -4.3978e-03, -1.7964e-02,  6.8538e-02,\n",
      "         -1.2528e-01, -3.1572e-02,  9.1733e-02,  5.5933e-02,  8.1966e-04,\n",
      "          4.7466e-02,  1.0476e-01, -6.4039e-02, -1.1145e-03, -1.9113e-02,\n",
      "         -9.6173e-02, -6.9087e-03, -8.7138e-02, -5.1184e-02,  4.8113e-02,\n",
      "          1.0064e-02,  3.0523e-02,  7.9112e-02, -1.0308e-02, -3.5956e-02,\n",
      "         -1.7363e-01, -5.8750e-02,  2.4071e-02, -8.0685e-02,  5.1673e-02,\n",
      "          4.7619e-04, -6.4606e-02,  8.7935e-02,  7.8069e-02,  3.1253e-02,\n",
      "          2.9714e-02,  5.6935e-02, -5.0922e-02, -2.7804e-02,  2.1020e-02,\n",
      "          1.7635e-01, -2.2509e-02, -3.9153e-02, -1.6774e-02, -5.3692e-02,\n",
      "         -3.6004e-02,  8.4247e-02,  4.3878e-02,  1.3890e-02,  4.1276e-02,\n",
      "          2.9123e-02,  3.6138e-02,  1.1296e-01, -1.0378e-03,  3.7267e-02,\n",
      "          7.9759e-02,  1.5415e-02, -7.7985e-02,  2.2899e-02, -1.6872e-01,\n",
      "         -4.4854e-02, -1.3102e-02,  1.2765e-01,  7.2075e-02,  3.5592e-03,\n",
      "          9.4486e-03, -8.8053e-02, -5.4100e-02, -1.9269e-02, -1.8411e-02,\n",
      "         -3.3812e-02, -1.0123e-02,  2.6583e-03,  6.5111e-02, -1.7934e-02,\n",
      "          9.7632e-02,  4.6777e-02, -3.9026e-03,  2.6349e-02, -7.5671e-02,\n",
      "          7.4151e-02,  4.4457e-02,  1.9298e-01, -7.9133e-02, -2.8129e-02,\n",
      "         -4.5496e-02,  1.4558e-01,  8.8295e-02,  7.2369e-02,  4.1542e-02,\n",
      "          5.0963e-02,  1.5654e-01,  5.6362e-02,  4.4522e-02, -4.1116e-02,\n",
      "          6.8426e-02, -4.8655e-02, -3.2206e-02,  3.8944e-02,  3.2581e-02,\n",
      "         -6.0510e-02, -3.9915e-02, -4.8217e-02,  7.3811e-02, -1.7349e-03,\n",
      "         -7.4416e-02, -7.4600e-02, -4.5503e-02, -7.9725e-02, -2.7587e-02,\n",
      "         -1.7229e-02, -4.3888e-02,  1.3863e-02, -4.9175e-02, -1.8320e-02,\n",
      "         -7.0595e-02, -3.5696e-02, -6.7897e-02,  4.2046e-02,  6.6403e-02,\n",
      "         -4.5590e-03, -2.0511e-02, -4.6554e-02,  9.2445e-02, -3.4970e-02,\n",
      "          4.9407e-02, -1.3833e-02, -2.7854e-02, -1.8578e-01,  7.8327e-02,\n",
      "          2.9545e-02,  5.6224e-02,  6.0495e-02,  1.6652e-02,  8.5692e-03,\n",
      "         -9.3814e-02, -1.9426e-03,  1.0706e-01,  1.1214e-01, -1.0251e-01,\n",
      "         -2.1785e-02,  5.8435e-02, -4.0059e-02,  4.3032e-02,  4.7540e-02,\n",
      "          2.4509e-02, -1.0579e-01,  3.9690e-02, -1.4435e-03,  1.9559e-03,\n",
      "         -3.6301e-02,  8.9292e-02, -5.0972e-02,  7.9288e-02,  1.4508e-02,\n",
      "         -4.2885e-02, -8.3662e-04, -5.6733e-02,  3.5398e-02,  1.3654e-02,\n",
      "          7.5589e-02, -1.7096e-01,  1.0367e-01, -4.8127e-03,  5.9026e-02,\n",
      "         -4.9126e-02,  2.1688e-02, -7.9966e-02, -1.9821e-02,  9.1160e-02,\n",
      "          4.3785e-02, -1.2950e-02,  3.4965e-02, -7.0492e-02,  9.8807e-02,\n",
      "          7.0114e-02,  6.3621e-02,  5.7891e-02, -3.8800e-02, -8.6599e-02,\n",
      "         -4.9352e-02,  7.9063e-02,  1.9872e-02, -5.1656e-02,  4.2468e-02,\n",
      "         -5.7921e-04,  9.4490e-02, -1.3078e-01, -2.4700e-02, -5.6174e-02,\n",
      "         -9.4360e-03, -1.6137e-02,  3.1529e-02,  6.2451e-02,  2.7853e-02,\n",
      "          1.7787e-01,  7.1190e-02,  8.5578e-02, -2.9378e-02, -3.7037e-02,\n",
      "          2.4510e-02,  5.7540e-02,  2.7019e-03,  6.6580e-03,  4.3247e-02,\n",
      "          1.7246e-02, -4.0317e-02,  2.1970e-02,  8.1854e-02,  5.3865e-03,\n",
      "         -2.7786e-03,  7.3924e-02,  6.2093e-02, -5.6462e-02,  7.9899e-04,\n",
      "         -5.8764e-02, -1.6699e-01,  9.3763e-03, -2.6812e-02,  9.0591e-02,\n",
      "          9.4361e-03, -1.5192e-01, -1.2284e-01, -1.4658e-01, -1.2188e-02,\n",
      "          5.4110e-02,  3.7468e-02,  6.5100e-03,  1.1516e-02,  2.1465e-02,\n",
      "         -4.5362e-02,  2.2087e-03, -4.1703e-03, -1.4195e-01, -1.0166e-01,\n",
      "         -5.8586e-03,  8.8601e-02,  7.7644e-03, -6.2691e-02,  3.6711e-02,\n",
      "         -7.6034e-02, -2.6550e-02, -1.4182e-01, -1.2829e-04, -4.2682e-03,\n",
      "          7.1095e-03, -8.7476e-02,  2.4215e-02,  1.0467e-03,  2.6848e-02,\n",
      "          9.8180e-03, -8.9399e-02, -1.9000e-03,  1.8403e-02,  1.7904e-03,\n",
      "          2.8257e-02, -6.9779e-02, -3.9710e-02,  1.2547e-01, -6.8353e-02,\n",
      "          2.3740e-02, -7.3415e-02, -2.0660e-02,  2.6375e-02,  3.2113e-02,\n",
      "         -7.4222e-03,  1.2811e-01, -2.1666e-03, -6.1181e-02,  4.4989e-02,\n",
      "          4.2366e-02,  1.4421e-01, -1.1843e-01,  4.2329e-02,  1.9983e-02,\n",
      "         -3.8463e-02,  1.0780e-02,  6.5939e-02, -9.1845e-03, -3.7742e-02,\n",
      "          2.0942e-02,  3.1457e-02, -2.3460e-02,  7.9716e-02, -1.8558e-02,\n",
      "          2.8565e-02, -7.4567e-02,  7.1220e-02, -5.1303e-02,  6.7456e-02,\n",
      "          4.7114e-02,  4.9159e-02, -8.9077e-02,  2.3860e-02, -8.7548e-02,\n",
      "          2.5866e-02,  1.5081e-01,  1.0869e-01, -8.4262e-02, -7.2414e-02,\n",
      "         -6.6100e-02, -1.1661e-01,  1.1894e-02,  8.2102e-02, -7.1157e-02,\n",
      "          2.4316e-02, -1.2996e-02,  2.2831e-02, -7.0337e-02, -2.6865e-03,\n",
      "         -9.6952e-02,  4.2466e-02,  9.0263e-03, -7.8650e-02, -2.0605e-02,\n",
      "          1.4043e-01,  5.0659e-03, -7.2181e-02,  1.0737e-01,  8.5096e-02,\n",
      "          1.0082e-01,  3.8351e-02, -8.5218e-02, -1.6554e-02,  1.1954e-02,\n",
      "          8.9649e-02,  4.9449e-03]])\n",
      "Layer: KANClassifier.2.fouriercoeffs, Fourier Coefficients:\n",
      "tensor([[[[-3.7025e-05,  4.2193e-04,  1.0916e-03,  ...,  9.8317e-04,\n",
      "            1.4730e-03,  1.7936e-03],\n",
      "          [-1.1711e-03,  1.8440e-03, -2.8752e-03,  ..., -2.8652e-03,\n",
      "           -8.4842e-04,  1.2675e-04],\n",
      "          [ 2.3464e-03, -2.0497e-03, -1.4948e-03,  ..., -2.5454e-04,\n",
      "           -2.3492e-03,  2.4235e-03],\n",
      "          ...,\n",
      "          [-2.1143e-03, -7.0433e-04, -2.5486e-03,  ...,  8.4482e-04,\n",
      "           -1.9266e-03, -2.5195e-03],\n",
      "          [ 7.2254e-04, -1.6174e-03, -8.7543e-04,  ..., -1.1229e-03,\n",
      "           -1.4425e-03,  8.1729e-04],\n",
      "          [ 8.0328e-04,  2.4050e-03,  2.7386e-03,  ..., -4.0800e-04,\n",
      "           -1.8325e-03,  2.3070e-03]],\n",
      "\n",
      "         [[ 1.2693e-03,  3.3693e-04,  1.2309e-03,  ...,  1.6530e-03,\n",
      "           -2.8322e-03,  2.8147e-03],\n",
      "          [ 9.6566e-04,  1.5538e-03,  1.8957e-03,  ..., -2.1112e-03,\n",
      "           -1.5560e-03,  2.5962e-03],\n",
      "          [ 1.0886e-03, -2.7926e-03,  1.8338e-03,  ...,  1.0991e-03,\n",
      "           -1.4837e-03,  5.2438e-05],\n",
      "          ...,\n",
      "          [-7.5158e-04,  1.4736e-03,  2.3647e-03,  ..., -1.2913e-03,\n",
      "            8.2947e-04,  1.9992e-03],\n",
      "          [ 5.9014e-04,  1.9666e-04, -7.2985e-04,  ...,  2.3212e-04,\n",
      "           -6.6431e-04, -1.5550e-03],\n",
      "          [-2.5118e-03, -2.6970e-03,  1.0420e-03,  ...,  1.5113e-03,\n",
      "           -2.5736e-03, -2.3828e-03]],\n",
      "\n",
      "         [[ 1.7299e-03, -2.8828e-03,  4.1956e-04,  ..., -1.9211e-03,\n",
      "           -2.0445e-04,  6.2442e-04],\n",
      "          [ 6.1339e-04,  8.4458e-04,  1.4137e-04,  ...,  2.3703e-03,\n",
      "            2.7799e-03, -2.6379e-04],\n",
      "          [-9.7621e-04, -3.2882e-04, -1.0821e-03,  ...,  1.7894e-03,\n",
      "            1.8642e-03,  2.6878e-03],\n",
      "          ...,\n",
      "          [ 1.6497e-03,  3.9329e-04, -2.0815e-03,  ...,  2.5312e-03,\n",
      "           -2.3490e-03,  1.6308e-03],\n",
      "          [-2.7784e-03,  1.6630e-03, -1.3149e-03,  ...,  2.5564e-03,\n",
      "           -1.8634e-03, -1.2820e-03],\n",
      "          [ 1.4347e-03,  4.5415e-05,  1.1068e-04,  ..., -2.5497e-04,\n",
      "            2.4832e-04, -2.4637e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.6740e-04,  2.5613e-03, -2.5227e-03,  ..., -2.2438e-03,\n",
      "            2.2662e-03, -2.7027e-03],\n",
      "          [ 3.1225e-04,  1.2022e-03,  4.6324e-04,  ..., -2.5094e-03,\n",
      "           -6.5084e-04, -1.6402e-03],\n",
      "          [ 9.5584e-04,  2.8950e-04,  1.9001e-03,  ..., -1.3547e-03,\n",
      "           -2.2741e-03,  2.1905e-03],\n",
      "          ...,\n",
      "          [-1.8356e-03, -6.7997e-04, -1.4204e-03,  ...,  4.6023e-04,\n",
      "           -2.5933e-04,  2.1775e-03],\n",
      "          [ 1.4647e-03,  2.0820e-03, -2.8507e-03,  ..., -1.5274e-03,\n",
      "            1.8462e-03,  1.9052e-03],\n",
      "          [-1.9049e-03,  4.0393e-04,  1.2227e-03,  ...,  6.4788e-04,\n",
      "           -1.6674e-04,  2.3765e-03]],\n",
      "\n",
      "         [[ 1.9046e-03, -2.2614e-03,  5.8736e-04,  ..., -2.8249e-03,\n",
      "           -1.9768e-03,  1.8995e-03],\n",
      "          [-1.2708e-03,  2.5963e-03, -2.4375e-03,  ..., -1.1968e-03,\n",
      "            1.5143e-04, -2.0481e-03],\n",
      "          [-8.8019e-04,  1.3210e-03, -1.1469e-03,  ...,  1.4403e-03,\n",
      "           -2.6063e-03,  2.1943e-03],\n",
      "          ...,\n",
      "          [ 1.5456e-03,  2.4739e-03,  2.1160e-03,  ..., -2.0426e-03,\n",
      "            1.3188e-03, -1.4460e-03],\n",
      "          [ 4.8836e-04,  2.0623e-03, -2.0755e-03,  ...,  2.3038e-04,\n",
      "            2.5669e-03,  1.6451e-03],\n",
      "          [ 1.3551e-03,  2.8780e-03, -2.8027e-04,  ..., -2.2404e-03,\n",
      "           -1.3244e-03,  2.0700e-03]],\n",
      "\n",
      "         [[-8.5578e-04,  2.0997e-03, -1.6471e-03,  ..., -1.0593e-03,\n",
      "            7.5508e-04, -4.9342e-04],\n",
      "          [ 1.7926e-03,  2.5524e-03,  2.6851e-03,  ...,  6.3528e-04,\n",
      "            4.3616e-04, -2.7253e-03],\n",
      "          [ 1.7852e-03, -1.0447e-03, -8.4558e-04,  ...,  1.1833e-03,\n",
      "           -2.0973e-03, -2.0240e-03],\n",
      "          ...,\n",
      "          [-2.0971e-03, -1.6054e-03, -1.8843e-03,  ..., -7.2369e-04,\n",
      "           -2.5222e-03,  2.6562e-03],\n",
      "          [-2.1991e-03,  2.5946e-03,  9.4681e-04,  ..., -2.4600e-04,\n",
      "            1.8596e-03,  1.7900e-03],\n",
      "          [ 2.2311e-04,  4.7352e-04,  2.6405e-03,  ..., -1.0647e-03,\n",
      "           -1.2188e-03,  2.1102e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.8135e-03, -1.0900e-03, -2.4588e-03,  ...,  1.5376e-03,\n",
      "           -1.3284e-03, -9.7804e-04],\n",
      "          [-5.1070e-04, -1.9354e-03,  1.1703e-03,  ...,  2.2405e-03,\n",
      "           -6.0023e-04, -2.6213e-03],\n",
      "          [-1.9165e-03,  2.8470e-03,  1.5113e-03,  ...,  1.5168e-03,\n",
      "           -5.2810e-04, -6.4678e-06],\n",
      "          ...,\n",
      "          [-2.5464e-03, -1.6230e-03,  1.5267e-03,  ..., -2.3131e-04,\n",
      "            2.2528e-03, -1.5976e-03],\n",
      "          [ 2.3585e-03, -1.7105e-03, -2.4260e-03,  ..., -2.0985e-03,\n",
      "            8.0707e-04, -2.4749e-03],\n",
      "          [ 2.5997e-03, -7.9942e-04,  1.4352e-03,  ...,  2.3200e-03,\n",
      "           -1.5972e-03,  1.7265e-03]],\n",
      "\n",
      "         [[ 9.7897e-04, -2.6952e-03,  2.7174e-03,  ..., -8.1293e-04,\n",
      "           -2.8123e-03, -5.3063e-05],\n",
      "          [-2.4816e-03,  2.8723e-03,  1.8905e-03,  ..., -1.0765e-03,\n",
      "           -1.3631e-05,  1.7173e-03],\n",
      "          [ 1.6709e-03,  2.3767e-03,  8.0474e-04,  ...,  2.4551e-03,\n",
      "            9.1626e-04, -1.5811e-03],\n",
      "          ...,\n",
      "          [ 2.1000e-03,  2.1379e-03, -4.2126e-04,  ...,  1.8860e-03,\n",
      "            2.8278e-03, -2.7317e-03],\n",
      "          [-1.8014e-03,  2.3983e-03,  4.4666e-04,  ...,  2.4009e-03,\n",
      "            1.2525e-03,  1.1788e-03],\n",
      "          [ 2.8163e-03,  8.2688e-04,  1.1674e-03,  ...,  2.5834e-03,\n",
      "            2.2769e-04, -3.0302e-04]],\n",
      "\n",
      "         [[-2.3219e-03,  1.2509e-03, -3.6276e-04,  ..., -2.6716e-03,\n",
      "            1.4948e-03, -2.6400e-03],\n",
      "          [-6.3660e-04, -1.8160e-03, -1.1251e-03,  ..., -1.9809e-03,\n",
      "            1.6614e-03,  3.0338e-04],\n",
      "          [ 1.5170e-04, -2.8440e-04,  5.5707e-04,  ...,  3.0908e-04,\n",
      "            2.6007e-03, -2.5338e-03],\n",
      "          ...,\n",
      "          [ 5.7184e-04,  4.1395e-04,  7.0188e-04,  ..., -2.7665e-03,\n",
      "           -2.6105e-03, -3.6377e-04],\n",
      "          [ 5.5018e-04,  1.7014e-03,  3.5603e-04,  ..., -1.7215e-03,\n",
      "            3.2838e-04, -2.3556e-04],\n",
      "          [ 1.3498e-03,  9.0891e-04,  2.4586e-03,  ...,  2.6449e-03,\n",
      "           -1.3374e-04,  1.3582e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2063e-03,  1.8908e-03,  1.7722e-03,  ...,  8.1351e-04,\n",
      "            1.3177e-03, -1.7506e-03],\n",
      "          [ 1.2144e-03,  4.6809e-04,  3.1516e-04,  ..., -7.3178e-04,\n",
      "           -9.8343e-04,  5.7316e-04],\n",
      "          [-2.2167e-03, -2.7091e-03, -2.2610e-03,  ...,  2.0837e-03,\n",
      "           -8.1803e-04, -2.5093e-03],\n",
      "          ...,\n",
      "          [-1.0151e-04,  7.9048e-04, -1.1632e-03,  ...,  9.8486e-05,\n",
      "            6.7565e-04,  1.4427e-03],\n",
      "          [-1.5524e-03, -1.5593e-03,  1.5556e-03,  ...,  1.8552e-03,\n",
      "            2.8033e-03, -2.7526e-03],\n",
      "          [-1.2411e-03, -5.5433e-04,  2.3493e-03,  ...,  1.4359e-03,\n",
      "           -2.6974e-03, -1.8272e-03]],\n",
      "\n",
      "         [[-2.1825e-03,  1.0278e-03, -2.3773e-03,  ..., -7.8278e-05,\n",
      "           -3.4941e-04,  5.1786e-04],\n",
      "          [ 8.9110e-04, -9.2807e-04, -1.9594e-03,  ...,  8.0184e-04,\n",
      "            1.4437e-03, -2.6625e-03],\n",
      "          [-7.8664e-05,  4.5465e-04, -1.9246e-03,  ..., -2.3323e-03,\n",
      "            2.8391e-03, -1.0119e-03],\n",
      "          ...,\n",
      "          [-2.4850e-03, -1.7874e-03, -2.6928e-03,  ..., -2.7158e-03,\n",
      "           -2.7609e-03,  2.1177e-03],\n",
      "          [ 2.6583e-03, -2.8149e-04, -1.2360e-03,  ...,  3.9357e-04,\n",
      "            2.8899e-03, -2.3956e-03],\n",
      "          [-1.0328e-03,  2.1675e-03,  4.7504e-04,  ..., -1.7305e-03,\n",
      "            5.0911e-04, -1.0237e-03]],\n",
      "\n",
      "         [[ 7.6053e-04,  2.3907e-03,  9.7863e-04,  ...,  3.5461e-04,\n",
      "           -1.8679e-03, -1.5721e-03],\n",
      "          [-8.4288e-04, -2.1164e-04,  1.7051e-03,  ...,  1.9415e-03,\n",
      "           -2.8085e-03,  2.1185e-03],\n",
      "          [ 2.0726e-04,  2.6636e-03, -7.2749e-05,  ..., -2.8102e-04,\n",
      "            1.9273e-03,  9.7540e-04],\n",
      "          ...,\n",
      "          [ 2.7244e-03,  1.0745e-03,  2.5583e-03,  ...,  1.4256e-03,\n",
      "           -1.6089e-04, -1.6386e-03],\n",
      "          [ 1.4590e-03,  3.5354e-04,  9.4288e-04,  ...,  9.2140e-04,\n",
      "           -2.5712e-03,  2.8005e-03],\n",
      "          [ 1.1565e-03, -1.4871e-03,  2.5625e-03,  ...,  6.1833e-04,\n",
      "            2.7241e-03, -1.8105e-03]]]])\n",
      "\n",
      "Layer: KANClassifier.2.bias, Biases: tensor([[ 0.1172, -0.0860,  0.2700,  0.2198, -0.5460, -0.7233,  0.0012,  0.1588,\n",
      "         -0.2863,  0.0824,  0.0211, -0.0283, -0.0369,  0.4879,  0.1513, -0.5101,\n",
      "         -0.1711,  0.3719,  0.2771, -0.2989, -0.0080,  0.0536, -0.3841,  0.3246,\n",
      "         -0.1394, -0.0064]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in fsl_model.named_parameters():\n",
    "  if \"fouriercoeffs\" in name:\n",
    "    print(f\"Layer: {name}, Fourier Coefficients:\\n{param.data}\\n\")\n",
    "  # if \"weight\" in name:\n",
    "  #   print(f\"Layer: {name}:, Weights: {param.data}\")\n",
    "  elif \"bias\" in name:\n",
    "    print(f\"Layer: {name}, Biases: {param.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128])\n",
      "tensor([[ 0.2318,  0.0547,  0.6426, -0.0577, -0.4347, -0.6254,  0.0017, -0.1285,\n",
      "         -0.3610,  0.3880,  0.3038, -0.2632, -0.0245,  0.8562,  0.0191, -0.2727,\n",
      "         -0.2240,  0.4684, -0.0673, -0.1873,  0.2119, -0.1177, -0.5432,  0.2457,\n",
      "          0.1707,  0.0375]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn(size=(3, 128, 128))  # Single image (C, H, W)\n",
    "print(tensor1.shape)\n",
    "\n",
    "output = fsl_model(tensor1.unsqueeze(dim=0))  # Add batch dimension\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: KANClassifier.1.fouriercoeffs, Fourier Coefficients:\n",
      "tensor([[[[-3.4559e-03, -2.1262e-02, -9.9435e-03,  ...,  1.6428e-02,\n",
      "            4.7876e-03,  9.2377e-03],\n",
      "          [-1.2113e-02, -1.9879e-02,  6.0552e-03,  ..., -1.0994e-02,\n",
      "           -9.2924e-03, -9.6265e-03],\n",
      "          [-3.7348e-03, -8.3083e-03,  9.9061e-04,  ..., -8.4609e-04,\n",
      "           -1.7406e-02, -1.5949e-03],\n",
      "          ...,\n",
      "          [ 1.8264e-03,  1.5777e-03, -1.9887e-03,  ...,  1.3169e-03,\n",
      "            1.2873e-02,  6.6389e-03],\n",
      "          [-8.6265e-04,  6.7001e-04, -1.5453e-02,  ...,  1.8155e-02,\n",
      "            2.4868e-03, -6.7170e-03],\n",
      "          [ 1.8978e-03,  4.0801e-03, -5.2635e-03,  ...,  1.5672e-02,\n",
      "           -1.3830e-02,  6.7259e-03]],\n",
      "\n",
      "         [[-4.9398e-03,  1.3222e-03, -1.7405e-02,  ..., -4.0871e-03,\n",
      "           -1.2426e-02,  4.2763e-03],\n",
      "          [-5.5842e-03, -1.0265e-02,  1.2770e-02,  ...,  4.6642e-03,\n",
      "           -1.5909e-02,  1.4144e-02],\n",
      "          [ 6.1552e-03,  1.3524e-02,  4.9043e-03,  ..., -3.9339e-03,\n",
      "            5.6631e-03,  5.3981e-03],\n",
      "          ...,\n",
      "          [-6.4075e-05, -1.0007e-02, -1.1058e-02,  ..., -4.4879e-05,\n",
      "            1.2401e-04, -2.7541e-04],\n",
      "          [-3.5638e-03, -2.3098e-03, -4.9780e-03,  ...,  1.2032e-02,\n",
      "           -5.3673e-03,  6.6105e-03],\n",
      "          [-2.2035e-03,  1.1991e-02,  3.2959e-03,  ...,  8.4816e-03,\n",
      "           -5.3778e-03,  1.1158e-02]],\n",
      "\n",
      "         [[ 6.8574e-03, -4.1592e-04, -5.9675e-03,  ..., -2.8007e-03,\n",
      "            8.4004e-03,  4.8035e-04],\n",
      "          [ 6.7042e-03, -2.9214e-03, -1.2185e-03,  ...,  1.0292e-02,\n",
      "           -6.9407e-03,  7.0481e-03],\n",
      "          [ 1.3808e-02, -4.5668e-03, -6.4027e-03,  ..., -9.1375e-03,\n",
      "            6.1706e-03,  1.0119e-03],\n",
      "          ...,\n",
      "          [ 1.0239e-02, -1.2638e-03,  7.8803e-03,  ..., -1.1274e-02,\n",
      "            2.8397e-03,  9.7702e-03],\n",
      "          [-1.4313e-02,  6.1605e-03, -1.1281e-02,  ..., -1.2874e-02,\n",
      "           -3.4314e-03,  1.1552e-02],\n",
      "          [-2.5306e-03, -1.8621e-02, -8.0862e-03,  ...,  8.1381e-03,\n",
      "           -1.7130e-02, -7.4548e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2354e-03,  1.2515e-02, -2.1365e-03,  ...,  1.0490e-02,\n",
      "           -9.0490e-03,  5.0679e-04],\n",
      "          [-1.3124e-03,  7.3235e-04, -4.8175e-03,  ..., -4.9147e-03,\n",
      "           -3.8997e-03,  3.8939e-03],\n",
      "          [ 4.7957e-03,  7.9565e-04, -1.1142e-03,  ..., -3.7343e-03,\n",
      "            2.8734e-03,  5.1538e-03],\n",
      "          ...,\n",
      "          [ 4.4995e-03, -1.7972e-02,  2.5948e-04,  ...,  4.8895e-03,\n",
      "            1.1110e-02, -4.6725e-03],\n",
      "          [-2.9709e-03, -6.2571e-03, -5.0165e-03,  ..., -1.4996e-02,\n",
      "            9.1907e-03,  1.0840e-02],\n",
      "          [ 1.3294e-02, -1.2523e-03,  2.0483e-02,  ...,  4.8480e-03,\n",
      "           -4.1369e-03,  6.8393e-03]],\n",
      "\n",
      "         [[-4.9021e-03, -7.5130e-05, -8.8701e-03,  ...,  2.4439e-05,\n",
      "           -1.7307e-02,  1.1973e-03],\n",
      "          [ 3.6054e-03, -1.6000e-03,  6.8142e-03,  ...,  1.4199e-02,\n",
      "            1.3308e-03, -6.7078e-03],\n",
      "          [ 2.9111e-03, -2.6860e-03, -6.8879e-03,  ..., -2.2888e-03,\n",
      "            9.0747e-03, -7.0413e-03],\n",
      "          ...,\n",
      "          [-1.3847e-02,  6.9073e-03,  5.4964e-03,  ..., -5.9138e-03,\n",
      "            1.0287e-02,  7.0473e-03],\n",
      "          [-1.3096e-02,  5.1547e-03, -7.5601e-03,  ...,  5.9155e-03,\n",
      "           -9.1066e-03, -5.1103e-03],\n",
      "          [-3.0986e-03, -5.1980e-03,  1.4103e-03,  ..., -6.9784e-03,\n",
      "           -1.0605e-02,  3.4004e-03]],\n",
      "\n",
      "         [[ 1.9899e-02,  2.8159e-03, -3.4806e-03,  ...,  1.4270e-02,\n",
      "            1.0300e-03, -3.2993e-03],\n",
      "          [-7.5311e-03, -2.9285e-04,  1.3277e-02,  ...,  3.9661e-03,\n",
      "           -5.2030e-03,  1.5613e-02],\n",
      "          [ 2.3303e-03, -6.9154e-03, -5.0971e-03,  ..., -1.1615e-02,\n",
      "            2.4428e-03, -2.7955e-02],\n",
      "          ...,\n",
      "          [ 2.1616e-03,  7.3945e-03,  6.5579e-03,  ..., -4.0344e-03,\n",
      "            2.3806e-03, -6.9216e-03],\n",
      "          [-1.3080e-02, -1.3293e-03, -7.5586e-03,  ...,  1.5669e-02,\n",
      "            3.1958e-03,  2.6311e-03],\n",
      "          [-1.1554e-02, -6.0180e-03,  2.1308e-03,  ...,  8.7971e-04,\n",
      "           -9.9546e-03,  9.9350e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.5483e-03,  1.5948e-03,  1.1176e-02,  ..., -1.5845e-02,\n",
      "           -6.4302e-03, -3.6591e-03],\n",
      "          [-8.9015e-03, -6.0439e-04,  2.2444e-03,  ..., -2.6261e-03,\n",
      "            1.8270e-02, -6.9725e-03],\n",
      "          [-1.3000e-02, -1.1331e-02, -2.7410e-03,  ..., -1.9213e-03,\n",
      "            9.2563e-03, -8.9777e-03],\n",
      "          ...,\n",
      "          [-7.8165e-04, -2.3071e-03, -1.7473e-03,  ...,  5.5570e-03,\n",
      "            1.0612e-02, -7.6399e-03],\n",
      "          [-1.3242e-02,  1.4047e-03,  5.8024e-03,  ...,  8.9210e-03,\n",
      "           -1.7881e-03, -4.1427e-03],\n",
      "          [-1.4559e-02, -1.3955e-02, -1.3229e-02,  ...,  2.6972e-04,\n",
      "            6.6826e-04,  3.2882e-03]],\n",
      "\n",
      "         [[ 1.3014e-03, -1.3622e-02,  4.4526e-03,  ..., -8.7074e-03,\n",
      "           -2.1218e-03,  2.8582e-03],\n",
      "          [-3.5446e-03, -6.5032e-03,  4.9511e-03,  ...,  2.8888e-03,\n",
      "           -2.3081e-03, -3.9614e-03],\n",
      "          [ 9.3318e-03, -1.7034e-03, -4.5691e-03,  ...,  5.7108e-04,\n",
      "            5.5897e-03, -6.6973e-03],\n",
      "          ...,\n",
      "          [-6.0663e-03,  1.1520e-03, -1.4211e-02,  ...,  7.6816e-03,\n",
      "            3.1575e-03,  7.9096e-03],\n",
      "          [-9.4085e-04, -6.4660e-03,  1.4107e-03,  ...,  3.1091e-03,\n",
      "           -1.7726e-04, -6.0790e-03],\n",
      "          [-9.7315e-04, -2.6200e-03, -2.1128e-03,  ...,  1.4681e-02,\n",
      "            1.2644e-02, -2.0937e-04]],\n",
      "\n",
      "         [[ 4.3900e-03,  3.0687e-03,  1.2345e-02,  ...,  6.6344e-03,\n",
      "            5.4280e-03, -1.0380e-03],\n",
      "          [ 1.2187e-02, -1.5564e-02,  4.4410e-03,  ..., -1.2956e-02,\n",
      "           -2.3123e-03, -5.6334e-03],\n",
      "          [-6.5445e-03, -1.9010e-03,  3.9584e-03,  ..., -9.5205e-03,\n",
      "            5.5388e-03,  1.1082e-03],\n",
      "          ...,\n",
      "          [-8.4704e-03, -1.6158e-02, -6.6268e-03,  ..., -8.4732e-03,\n",
      "           -1.6787e-02,  8.2767e-03],\n",
      "          [ 1.8517e-02,  1.4461e-03,  5.5267e-04,  ..., -1.0385e-02,\n",
      "            1.0620e-02,  4.0168e-03],\n",
      "          [ 3.5775e-03, -2.3751e-05,  5.3498e-03,  ..., -5.1628e-03,\n",
      "            1.1498e-02, -5.8255e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1986e-02, -3.0030e-04, -1.0624e-04,  ..., -7.9473e-03,\n",
      "            3.3394e-03, -8.5754e-03],\n",
      "          [-8.9353e-03, -1.1390e-02,  3.3132e-03,  ...,  2.7824e-03,\n",
      "            2.4071e-04, -1.2787e-03],\n",
      "          [-5.3496e-03, -1.8558e-03,  1.5716e-03,  ..., -6.3154e-03,\n",
      "            6.5239e-03, -8.0610e-03],\n",
      "          ...,\n",
      "          [ 5.6375e-03, -6.7807e-03, -5.8026e-03,  ...,  7.2608e-03,\n",
      "            2.2054e-03,  8.1068e-03],\n",
      "          [-5.2574e-05, -1.0386e-02, -1.4026e-02,  ..., -1.3200e-02,\n",
      "            2.2542e-03,  8.2773e-03],\n",
      "          [-4.6919e-03,  4.6941e-03, -8.8554e-03,  ..., -1.0580e-02,\n",
      "           -1.3046e-02,  1.3974e-02]],\n",
      "\n",
      "         [[-9.4542e-04,  8.7763e-03,  3.5604e-03,  ...,  1.5365e-03,\n",
      "           -1.8879e-02,  4.2269e-04],\n",
      "          [ 2.3393e-03,  4.5852e-03, -1.6319e-03,  ...,  8.9635e-04,\n",
      "            8.0186e-03,  1.4044e-02],\n",
      "          [-3.4263e-03, -5.2017e-03, -1.7249e-02,  ...,  1.0965e-03,\n",
      "            1.2966e-03, -1.0709e-02],\n",
      "          ...,\n",
      "          [ 1.5906e-02, -1.4100e-03,  4.1148e-03,  ..., -2.2146e-03,\n",
      "            1.1593e-02, -1.0885e-02],\n",
      "          [ 8.7116e-03,  7.0489e-03, -1.3765e-02,  ...,  2.0007e-03,\n",
      "           -3.5180e-03, -7.6533e-03],\n",
      "          [ 1.7482e-02, -3.3188e-03,  6.9728e-03,  ..., -1.0035e-02,\n",
      "           -4.5228e-03, -5.4916e-03]],\n",
      "\n",
      "         [[ 1.0221e-02,  1.1848e-02, -2.5785e-03,  ..., -1.3762e-02,\n",
      "           -1.7211e-02,  1.2421e-03],\n",
      "          [ 8.7580e-03,  6.2626e-03,  4.3459e-03,  ...,  4.0264e-03,\n",
      "            5.0162e-03, -1.2530e-02],\n",
      "          [-1.9657e-03, -4.9956e-03, -1.7915e-03,  ..., -2.1669e-02,\n",
      "           -1.0741e-02, -1.0137e-02],\n",
      "          ...,\n",
      "          [-3.7132e-03, -2.6904e-03, -4.5322e-03,  ...,  1.6782e-03,\n",
      "            1.7198e-02, -1.0178e-02],\n",
      "          [-3.9337e-03,  2.6735e-03, -7.3814e-03,  ...,  1.6836e-03,\n",
      "            1.9435e-03,  2.4666e-03],\n",
      "          [-3.5597e-03, -1.0542e-03, -5.4628e-03,  ..., -1.1372e-02,\n",
      "           -2.5777e-03, -8.8814e-03]]]])\n",
      "\n",
      "Layer: KANClassifier.2.fouriercoeffs, Fourier Coefficients:\n",
      "tensor([[[[-1.2056e-02,  9.2639e-03,  3.2626e-02,  ..., -6.3407e-02,\n",
      "           -1.9300e-03,  3.4612e-03],\n",
      "          [-5.0652e-02,  1.2677e-02, -1.4997e-02,  ...,  2.9430e-02,\n",
      "            5.9365e-03,  4.8268e-02],\n",
      "          [ 6.4207e-02,  3.7493e-02,  3.1270e-02,  ...,  4.7124e-03,\n",
      "            1.7578e-03,  7.7001e-02],\n",
      "          ...,\n",
      "          [-2.6847e-03,  2.7426e-02,  3.9331e-02,  ..., -2.7356e-03,\n",
      "           -3.1827e-02,  7.1475e-03],\n",
      "          [-3.4994e-02, -7.1565e-02, -2.6075e-02,  ..., -6.1595e-02,\n",
      "            2.2613e-02,  7.9938e-02],\n",
      "          [ 1.2207e-02,  3.2698e-02,  2.1068e-02,  ..., -5.0006e-02,\n",
      "           -5.9997e-02,  5.2722e-02]],\n",
      "\n",
      "         [[ 1.8029e-02,  3.1918e-04,  5.3149e-02,  ..., -7.8389e-03,\n",
      "            2.8270e-02, -4.4853e-03],\n",
      "          [-4.8432e-02,  1.7718e-02, -3.8828e-02,  ..., -9.7312e-03,\n",
      "           -2.5264e-02,  1.5578e-02],\n",
      "          [ 5.1564e-02,  3.0337e-03,  3.9809e-02,  ...,  4.8178e-02,\n",
      "            1.6707e-02, -1.4809e-02],\n",
      "          ...,\n",
      "          [-5.3199e-02, -1.9230e-02, -4.4660e-02,  ...,  3.2378e-02,\n",
      "            5.2605e-02, -4.3812e-02],\n",
      "          [-2.4902e-02,  1.4773e-02,  3.1109e-02,  ..., -6.3807e-03,\n",
      "            7.5379e-02,  3.2995e-02],\n",
      "          [ 2.7052e-02,  3.5942e-02,  2.7292e-02,  ...,  2.8243e-03,\n",
      "            2.2196e-03,  2.0045e-02]],\n",
      "\n",
      "         [[ 1.0527e-02, -2.3914e-02, -3.8730e-02,  ..., -1.8452e-02,\n",
      "           -4.9145e-02, -2.8651e-02],\n",
      "          [ 2.0892e-03,  6.1931e-03,  1.1840e-02,  ..., -3.3644e-02,\n",
      "           -4.6019e-03,  5.2295e-02],\n",
      "          [ 5.6309e-02,  2.7393e-02,  1.3113e-02,  ..., -1.8872e-02,\n",
      "           -1.8353e-02, -1.2460e-03],\n",
      "          ...,\n",
      "          [-2.4668e-02, -9.8465e-03,  3.7362e-02,  ..., -2.4582e-02,\n",
      "           -5.4891e-02, -2.4627e-02],\n",
      "          [ 2.4603e-02,  6.5480e-02,  6.1601e-02,  ...,  1.8988e-03,\n",
      "            1.7808e-02, -1.3795e-02],\n",
      "          [ 3.1537e-02, -5.1083e-02, -2.5044e-02,  ...,  2.2054e-02,\n",
      "           -2.0482e-02,  1.8677e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.2990e-03,  2.0072e-03, -2.7293e-02,  ...,  3.3025e-02,\n",
      "           -8.4268e-02,  8.3223e-03],\n",
      "          [-1.0000e-01, -1.2428e-02,  3.1594e-02,  ...,  1.0910e-02,\n",
      "            2.3062e-02,  3.2375e-02],\n",
      "          [-9.0648e-02, -5.4210e-02,  2.8236e-02,  ..., -4.4818e-03,\n",
      "            1.1972e-02, -3.3351e-03],\n",
      "          ...,\n",
      "          [-1.6391e-03,  9.8934e-02,  6.1161e-02,  ...,  3.3817e-02,\n",
      "            2.5457e-02,  1.4590e-02],\n",
      "          [-4.3002e-02,  3.7533e-02, -4.6302e-02,  ...,  4.6257e-02,\n",
      "           -2.4604e-02,  6.7698e-02],\n",
      "          [ 1.3000e-02, -4.4146e-02, -2.5630e-02,  ..., -1.7989e-02,\n",
      "           -2.3598e-02,  3.0439e-02]],\n",
      "\n",
      "         [[-1.9147e-02, -2.4227e-02, -5.3208e-02,  ..., -1.9946e-02,\n",
      "           -2.7236e-02, -9.3744e-03],\n",
      "          [ 2.1999e-02, -5.0927e-02, -1.3882e-03,  ...,  1.3263e-02,\n",
      "            7.0355e-02, -7.5421e-03],\n",
      "          [ 5.0945e-02, -7.1943e-03,  1.5903e-02,  ...,  7.6836e-02,\n",
      "            2.1056e-03,  1.5856e-02],\n",
      "          ...,\n",
      "          [ 1.1861e-02,  2.5462e-02, -3.9274e-02,  ...,  1.5005e-02,\n",
      "           -9.4501e-03, -2.6977e-02],\n",
      "          [-5.9979e-02,  4.2142e-03,  6.3953e-03,  ...,  1.5250e-02,\n",
      "            3.4333e-02, -5.9247e-02],\n",
      "          [-6.4686e-02, -5.1764e-02, -2.9919e-02,  ...,  3.8691e-02,\n",
      "           -1.0098e-03,  3.7067e-02]],\n",
      "\n",
      "         [[ 5.5239e-03, -5.3160e-02,  7.6344e-02,  ...,  5.0315e-03,\n",
      "           -2.6280e-02,  4.5282e-02],\n",
      "          [ 6.6655e-03,  2.4496e-02,  3.1192e-02,  ..., -6.1605e-02,\n",
      "           -2.7846e-02, -1.8489e-02],\n",
      "          [-3.6085e-02, -4.2126e-02,  6.3536e-02,  ..., -6.6241e-02,\n",
      "           -1.6518e-02,  8.0490e-03],\n",
      "          ...,\n",
      "          [ 7.1810e-04,  3.9232e-02,  3.6676e-02,  ..., -6.8149e-02,\n",
      "            1.3674e-02, -4.8715e-02],\n",
      "          [ 1.1514e-02, -3.4991e-02,  6.1187e-02,  ...,  1.5774e-02,\n",
      "           -4.5660e-02,  9.7319e-02],\n",
      "          [-7.1030e-02, -4.2985e-02, -8.8469e-03,  ...,  2.0850e-02,\n",
      "            5.5973e-02, -2.7379e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1769e-02, -1.1167e-02, -5.7405e-03,  ...,  2.6804e-02,\n",
      "           -2.2666e-02,  2.8687e-02],\n",
      "          [-8.2063e-03, -1.1010e-02,  5.4911e-02,  ..., -4.7678e-02,\n",
      "           -3.0951e-02,  1.3525e-02],\n",
      "          [ 9.6019e-03, -4.7307e-02, -1.9872e-02,  ..., -6.7785e-03,\n",
      "            5.4610e-02, -7.3737e-03],\n",
      "          ...,\n",
      "          [ 6.7983e-02,  3.6385e-02,  1.2750e-02,  ..., -6.8576e-02,\n",
      "            8.6908e-03,  4.3000e-03],\n",
      "          [-2.3499e-02, -6.5757e-03,  4.4805e-02,  ..., -1.1031e-02,\n",
      "           -2.0166e-04, -1.3128e-02],\n",
      "          [-1.5179e-02, -2.3093e-02, -8.7345e-02,  ...,  1.5553e-02,\n",
      "           -3.6461e-02,  3.2988e-03]],\n",
      "\n",
      "         [[ 6.2366e-02,  3.8077e-02,  2.2149e-03,  ..., -2.8893e-02,\n",
      "            5.2460e-02,  5.7662e-02],\n",
      "          [ 2.3475e-02,  3.6824e-03, -1.4210e-02,  ..., -2.7986e-02,\n",
      "            1.4340e-02, -3.1243e-02],\n",
      "          [ 2.2341e-02,  6.3564e-02, -1.2652e-02,  ..., -2.4019e-02,\n",
      "           -2.8304e-02,  3.6539e-03],\n",
      "          ...,\n",
      "          [-8.9406e-02, -6.1032e-02, -7.4228e-02,  ...,  5.2102e-03,\n",
      "           -4.1041e-02,  7.1477e-02],\n",
      "          [-2.8645e-02, -3.3591e-02,  1.0655e-02,  ..., -1.7488e-02,\n",
      "            1.1484e-02, -5.3676e-02],\n",
      "          [ 3.6097e-02,  5.7380e-02,  2.0090e-02,  ..., -6.6283e-02,\n",
      "            3.3221e-02, -3.9489e-02]],\n",
      "\n",
      "         [[ 1.3697e-02,  6.6438e-02, -3.1237e-03,  ...,  5.3518e-02,\n",
      "           -2.1891e-02, -5.1378e-02],\n",
      "          [ 2.9912e-02, -7.0687e-03,  7.6258e-03,  ...,  3.5082e-02,\n",
      "            1.0892e-02,  9.6124e-02],\n",
      "          [ 8.5044e-02, -8.4718e-02, -4.5024e-02,  ..., -1.9205e-03,\n",
      "           -4.6874e-02, -6.1394e-03],\n",
      "          ...,\n",
      "          [-1.6803e-02,  1.3835e-02,  6.5313e-03,  ..., -9.7145e-03,\n",
      "           -4.6123e-02,  2.0538e-02],\n",
      "          [-2.3733e-02,  2.7593e-02, -3.7383e-02,  ...,  1.0382e-01,\n",
      "           -1.4616e-02, -1.7701e-02],\n",
      "          [ 1.7318e-02, -3.1576e-02, -5.4794e-02,  ..., -3.3659e-02,\n",
      "            5.5381e-02, -1.0176e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.5353e-02, -4.1057e-02, -1.9858e-02,  ...,  1.8801e-02,\n",
      "           -4.0979e-02,  5.5450e-02],\n",
      "          [-3.8898e-02,  4.3164e-02,  1.5128e-02,  ...,  1.5402e-02,\n",
      "            5.2116e-02,  1.5618e-02],\n",
      "          [ 2.5878e-02, -6.8985e-03,  5.2563e-02,  ..., -7.8139e-03,\n",
      "           -1.0654e-02,  1.0923e-02],\n",
      "          ...,\n",
      "          [-3.2688e-03,  7.4370e-02, -6.5463e-03,  ..., -2.5189e-02,\n",
      "           -7.0845e-02,  5.1532e-03],\n",
      "          [-8.9555e-02,  4.5531e-02,  6.1225e-02,  ..., -2.6013e-02,\n",
      "            2.7032e-02,  1.9001e-02],\n",
      "          [ 1.1743e-02,  7.3933e-03,  2.7833e-02,  ...,  1.5285e-02,\n",
      "           -3.8337e-02,  3.7614e-02]],\n",
      "\n",
      "         [[ 3.2988e-02, -4.2103e-02,  7.0850e-03,  ..., -9.1061e-03,\n",
      "            1.9136e-02,  3.9209e-02],\n",
      "          [ 3.4168e-02, -1.8250e-02, -9.1856e-02,  ..., -5.5769e-02,\n",
      "           -2.0296e-02,  5.3098e-05],\n",
      "          [ 3.0047e-02,  5.8669e-03, -1.6842e-02,  ..., -5.5131e-02,\n",
      "            1.4183e-02,  1.9838e-02],\n",
      "          ...,\n",
      "          [ 2.4300e-02,  6.0326e-02, -8.3824e-02,  ..., -1.5326e-02,\n",
      "            5.9549e-02,  4.5015e-02],\n",
      "          [-1.6333e-02,  3.9904e-02, -8.6297e-02,  ...,  2.9992e-02,\n",
      "            3.2486e-02, -4.1994e-02],\n",
      "          [-1.3217e-01,  3.3973e-02,  8.5180e-02,  ..., -2.7556e-02,\n",
      "            3.0979e-02, -2.9414e-02]],\n",
      "\n",
      "         [[-2.0600e-02,  1.0142e-02,  1.7277e-03,  ...,  6.0612e-02,\n",
      "           -3.5069e-02, -2.0641e-02],\n",
      "          [-4.6761e-02,  8.2994e-03, -1.4733e-02,  ...,  6.4064e-02,\n",
      "           -2.7990e-02, -2.3611e-02],\n",
      "          [-4.1921e-03, -4.8430e-02,  4.3116e-02,  ...,  2.2415e-02,\n",
      "            3.9048e-02,  4.6265e-02],\n",
      "          ...,\n",
      "          [-4.2809e-02,  4.2199e-02, -4.3055e-02,  ...,  9.9935e-03,\n",
      "            2.0759e-02, -3.5766e-02],\n",
      "          [-2.1139e-04,  6.4916e-02, -1.4831e-02,  ...,  1.4398e-02,\n",
      "           -2.6083e-02,  1.1574e-02],\n",
      "          [-2.2677e-02,  2.7970e-02, -4.8711e-02,  ...,  9.3334e-02,\n",
      "           -2.3845e-02,  2.2786e-02]]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in fsl_model.named_parameters():\n",
    "    if param.requires_grad:  # Ensures we only inspect trainable parameters\n",
    "        if \"fouriercoeffs\" in name:\n",
    "            print(f\"Layer: {name}, Fourier Coefficients:\\n{param.data}\\n\")\n",
    "        # elif \"bias\" in name:\n",
    "        #     print(f\"Layer: {name}, Biases:\\n{param.data}\\n\")\n",
    "        # elif \"weight\" in name:\n",
    "        #     print(f\"Layer: {name}, Weights:\\n{param.data}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet_KAN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (KANClassifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): NaiveFourierKANLayer()\n",
      "    (4): Linear(in_features=128, out_features=26, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 11058299\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NaiveFourierKANLayer(nn.Module):\n",
    "    def __init__(self, inputdim, outdim, initial_gridsize, addbias=True):\n",
    "        super(NaiveFourierKANLayer, self).__init__()\n",
    "        self.addbias = addbias\n",
    "        self.inputdim = inputdim\n",
    "        self.outdim = outdim\n",
    "        self.gridsize_param = nn.Parameter(torch.tensor(initial_gridsize, dtype=torch.float32)) #adjusted during training\n",
    "        self.fouriercoeffs = nn.Parameter(torch.empty(2, outdim, inputdim, initial_gridsize))\n",
    "        nn.init.xavier_uniform_(self.fouriercoeffs)\n",
    "        if self.addbias:\n",
    "            self.bias = nn.Parameter(torch.zeros(1, outdim))\n",
    "            nn.init.kaiming_normal_(self.bias)\n",
    "\n",
    "    def forward(self, x): #Combines cosine/sine terms with learnable coefficents for Fourier expansion and sums them + bias\n",
    "        gridsize = torch.clamp(self.gridsize_param, min=1).round().int()\n",
    "        outshape = x.shape[:-1] + (self.outdim,)\n",
    "        x = torch.reshape(x, (-1, self.inputdim))\n",
    "        k = torch.reshape(torch.arange(1, gridsize + 1, device=x.device), (1, 1, 1, gridsize))\n",
    "        xrshp = torch.reshape(x, (x.shape[0], 1, x.shape[1], 1))\n",
    "        c = torch.cos(k * xrshp)\n",
    "        s = torch.sin(k * xrshp)\n",
    "        y = torch.sum(c * self.fouriercoeffs[0:1, :, :, :gridsize], (-2, -1))\n",
    "        y += torch.sum(s * self.fouriercoeffs[1:2, :, :, :gridsize], (-2, -1))\n",
    "        if self.addbias:\n",
    "            y += self.bias\n",
    "        y = torch.reshape(y, outshape)\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    expansion: int = 2\n",
    "    def __init__(self, input, output, identity_downsample=None, stride=(1,1)):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input, output, kernel_size=(3,3), stride=(1,1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(output)\n",
    "        self.conv2 = nn.Conv2d(output, output*self.expansion, \n",
    "                               kernel_size=(3,3), stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(output*self.expansion,)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet_KAN(nn.Module):\n",
    "    def __init__(self, block: Block, layers, input_shape, output_shape):\n",
    "        super(ResNet_KAN, self).__init__()\n",
    "        self.input = 32\n",
    "        self.conv1 = nn.Conv2d(input_shape, 32, kernel_size=(5,5), stride=(2,2), padding=(2,2))\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(3,3), stride=(2,2), padding=(1,1))\n",
    "\n",
    "        ###ResNet Layers\n",
    "        self.layer1 = self.make_layer(block,layers[0], output=32, stride=(1,1))\n",
    "        self.layer2 = self.make_layer(block,layers[1], output=64, stride=(2,2))\n",
    "        self.layer3 = self.make_layer(block,layers[2], output=128, stride=(2,2))\n",
    "        self.layer4 = self.make_layer(block,layers[3], output=256, stride=(2,2))\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        self.KANClassifier = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(256*block.expansion, 256),\n",
    "            nn.ReLU(),\n",
    "            NaiveFourierKANLayer(256, 128, initial_gridsize=30),\n",
    "            nn.Linear(128, output_shape),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = self.KANClassifier(x)\n",
    "        return x\n",
    "    \n",
    "    def make_layer(self, block, num_residual_MyBlocks, output, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.input != output*block.expansion:\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(\n",
    "                self.input, output * block.expansion, kernel_size=(1,1), stride=stride),\n",
    "                nn.BatchNorm2d(output*block.expansion)\n",
    "            )\n",
    "        layers.append(block(self.input, output, identity_downsample, stride))\n",
    "        self.input = output*block.expansion\n",
    "\n",
    "        for i in range(num_residual_MyBlocks-1):\n",
    "            layers.append(block(self.input, output))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "torch.manual_seed(23)\n",
    "fsl_model = ResNet_KAN(Block, [3,4,3,3], input_shape=3, \n",
    "                       output_shape=26)\n",
    "print(f\"{fsl_model}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in fsl_model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet_KAN(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (KANClassifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=64, out_features=50, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): NaiveFourierKANLayer()\n",
      "    (4): NaiveFourierKANLayer()\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 147856\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NaiveFourierKANLayer(nn.Module):\n",
    "    def __init__(self, inputdim, outdim, initial_gridsize, addbias=True):\n",
    "        super(NaiveFourierKANLayer, self).__init__()\n",
    "        self.addbias = addbias\n",
    "        self.inputdim = inputdim\n",
    "        self.outdim = outdim\n",
    "        self.gridsize_param = nn.Parameter(torch.tensor(initial_gridsize, dtype=torch.float32)) #adjusted during training\n",
    "        self.fouriercoeffs = nn.Parameter(torch.empty(2, outdim, inputdim, initial_gridsize))\n",
    "        nn.init.xavier_uniform_(self.fouriercoeffs)\n",
    "        if self.addbias:\n",
    "            self.bias = nn.Parameter(torch.zeros(1, outdim))\n",
    "            nn.init.kaiming_normal_(self.bias)\n",
    "\n",
    "    def forward(self, x): #Combines cosine/sine terms with learnable coefficents for Fourier expansion and sums them + bias\n",
    "        gridsize = torch.clamp(self.gridsize_param, min=1).round().int()\n",
    "        outshape = x.shape[:-1] + (self.outdim,)\n",
    "        x = torch.reshape(x, (-1, self.inputdim))\n",
    "        k = torch.reshape(torch.arange(1, gridsize + 1, device=x.device), (1, 1, 1, gridsize))\n",
    "        xrshp = torch.reshape(x, (x.shape[0], 1, x.shape[1], 1))\n",
    "        c = torch.cos(k * xrshp)\n",
    "        s = torch.sin(k * xrshp)\n",
    "        y = torch.sum(c * self.fouriercoeffs[0:1, :, :, :gridsize], (-2, -1))\n",
    "        y += torch.sum(s * self.fouriercoeffs[1:2, :, :, :gridsize], (-2, -1))\n",
    "        if self.addbias:\n",
    "            y += self.bias\n",
    "        y = torch.reshape(y, outshape)\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    expansion: int = 2\n",
    "    def __init__(self, input, output, identity_downsample=None, stride=(1,1)):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input, output, kernel_size=(3,3), stride=(1,1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(output)\n",
    "        self.conv2 = nn.Conv2d(output, output*self.expansion, \n",
    "                               kernel_size=(3,3), stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(output*self.expansion,)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet_KAN(nn.Module):\n",
    "    def __init__(self, block: Block, layers, input_shape, output_shape):\n",
    "        super(ResNet_KAN, self).__init__()\n",
    "        self.input = 16\n",
    "        self.conv1 = nn.Conv2d(input_shape, 16, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "\n",
    "        ###ResNet Layers\n",
    "        self.layer1 = self.make_layer(block,layers[0], output=16, stride=(2,2))\n",
    "        self.layer2 = self.make_layer(block,layers[1], output=32, stride=(2,2))\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        self.KANClassifier = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(32*block.expansion, 50),  # MLP first\n",
    "            nn.ReLU(),\n",
    "            NaiveFourierKANLayer(50, 50, initial_gridsize=10),\n",
    "            NaiveFourierKANLayer(50, output_shape, initial_gridsize=10)\n",
    "\n",
    "        )\n",
    "    \n",
    "    def forward(self,x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = self.KANClassifier(x)\n",
    "        return x\n",
    "    \n",
    "    def make_layer(self, block, num_residual_MyBlocks, output, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.input != output*block.expansion:\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(\n",
    "                self.input, output * block.expansion, kernel_size=(1,1), stride=stride),\n",
    "                nn.BatchNorm2d(output*block.expansion)\n",
    "            )\n",
    "        layers.append(block(self.input, output, identity_downsample, stride))\n",
    "        self.input = output*block.expansion\n",
    "\n",
    "        for _ in range(num_residual_MyBlocks-1):\n",
    "            layers.append(block(self.input, output))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(23)\n",
    "fsl_model = ResNet_KAN(Block, [2,2,2], input_shape=1, \n",
    "                       output_shape=10)\n",
    "print(f\"{fsl_model}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in fsl_model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "#Adan: Adaptive Nesterov Momentum Algorithm (2024)\n",
    "\n",
    "import math\n",
    "from typing import List\n",
    "from torch import Tensor\n",
    "\n",
    "class MultiTensorApply(object):\n",
    "    available = False\n",
    "    warned = False\n",
    "\n",
    "    def __init__(self, chunk_size):\n",
    "        try:\n",
    "            MultiTensorApply.available = True\n",
    "            self.chunk_size = chunk_size\n",
    "        except ImportError as err:\n",
    "            MultiTensorApply.available = False\n",
    "            MultiTensorApply.import_err = err\n",
    "\n",
    "    def __call__(self, op, noop_flag_buffer, tensor_lists, *args):\n",
    "        return op(self.chunk_size, noop_flag_buffer, tensor_lists, *args)\n",
    "\n",
    "\n",
    "class Adan(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Implements a pytorch variant of Adan\n",
    "    Adan was proposed in\n",
    "    Adan: Adaptive Nesterov Momentum Algorithm for\n",
    "        Faster Optimizing Deep Models[J].arXiv preprint arXiv:2208.06677, 2022.\n",
    "    https://arxiv.org/abs/2208.06677\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or\n",
    "            dicts defining parameter groups.\n",
    "        lr (float, optional): learning rate. (default: 1e-3)\n",
    "        betas (Tuple[float, float, flot], optional): coefficients used for\n",
    "            first- and second-order moments. (default: (0.98, 0.92, 0.99))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability. (default: 1e-8)\n",
    "        weight_decay (float, optional): decoupled weight decay\n",
    "            (L2 penalty) (default: 0)\n",
    "        max_grad_norm (float, optional): value used to clip\n",
    "            global grad norm (default: 0.0 no clip)\n",
    "        no_prox (bool): how to perform the decoupled weight decay\n",
    "            (default: False)\n",
    "        foreach (bool): if True would use torch._foreach implementation.\n",
    "            It's faster but uses slightly more memory. (default: True)\n",
    "        fused (bool, optional): whether fused implementation is used.\n",
    "            (default: False)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-3,\n",
    "                 betas=(0.98, 0.92, 0.99),\n",
    "                 eps=1e-8,\n",
    "                 weight_decay=0.0,\n",
    "                 max_grad_norm=0.0,\n",
    "                 no_prox=False,\n",
    "                 foreach: bool = True,\n",
    "                 fused: bool = False):\n",
    "        if not 0.0 <= max_grad_norm:\n",
    "            raise ValueError('Invalid Max grad norm: {}'.format(max_grad_norm))\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError('Invalid learning rate: {}'.format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError('Invalid epsilon value: {}'.format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 0: {}'.format(\n",
    "                betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 1: {}'.format(\n",
    "                betas[1]))\n",
    "        if not 0.0 <= betas[2] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 2: {}'.format(\n",
    "                betas[2]))\n",
    "        if fused:\n",
    "            _check_fused_available()\n",
    "\n",
    "        defaults = dict(lr=lr,\n",
    "                        betas=betas,\n",
    "                        eps=eps,\n",
    "                        weight_decay=weight_decay,\n",
    "                        max_grad_norm=max_grad_norm,\n",
    "                        no_prox=no_prox,\n",
    "                        foreach=foreach,\n",
    "                        fused=fused)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adan, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('no_prox', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restart_opt(self):\n",
    "        for group in self.param_groups:\n",
    "            group['step'] = 0\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    state = self.state[p]\n",
    "                    # State initialization\n",
    "\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of gradient difference\n",
    "                    state['exp_avg_diff'] = torch.zeros_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\"\"\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        if self.defaults['max_grad_norm'] > 0:\n",
    "            device = self.param_groups[0]['params'][0].device\n",
    "            global_grad_norm = torch.zeros(1, device=device)\n",
    "\n",
    "            max_grad_norm = torch.tensor(self.defaults['max_grad_norm'],\n",
    "                                         device=device)\n",
    "            for group in self.param_groups:\n",
    "\n",
    "                for p in group['params']:\n",
    "                    if p.grad is not None:\n",
    "                        grad = p.grad\n",
    "                        global_grad_norm.add_(grad.pow(2).sum())\n",
    "\n",
    "            global_grad_norm = torch.sqrt(global_grad_norm)\n",
    "\n",
    "            clip_global_grad_norm = torch.clamp(\n",
    "                max_grad_norm / (global_grad_norm + group['eps']),\n",
    "                max=1.0).item()\n",
    "        else:\n",
    "            clip_global_grad_norm = 1.0\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            exp_avg_diffs = []\n",
    "            neg_pre_grads = []\n",
    "\n",
    "            beta1, beta2, beta3 = group['betas']\n",
    "            # assume same# Since we added LogSoftmax, use NLLLoss instead step across group now to simplify things\n",
    "            # per parameter step can be easily support\n",
    "            # by making it tensor, or pass list into kernel\n",
    "            if 'step' in group:\n",
    "                group['step'] += 1\n",
    "            else:\n",
    "                group['step'] = 1\n",
    "\n",
    "            bias_correction1 = 1.0 - beta1**group['step']\n",
    "            bias_correction2 = 1.0 - beta2**group['step']\n",
    "            bias_correction3 = 1.0 - beta3**group['step']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                params_with_grad.append(p)\n",
    "                grads.append(p.grad)\n",
    "\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "                    state['exp_avg_diff'] = torch.zeros_like(p)\n",
    "\n",
    "                if 'neg_pre_grad' not in state or group['step'] == 1:\n",
    "                    state['neg_pre_grad'] = p.grad.clone().mul_(\n",
    "                        -clip_global_grad_norm)\n",
    "\n",
    "                exp_avgs.append(state['exp_avg'])\n",
    "                exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "                exp_avg_diffs.append(state['exp_avg_diff'])\n",
    "                neg_pre_grads.append(state['neg_pre_grad'])\n",
    "\n",
    "            if not params_with_grad:\n",
    "                continue\n",
    "\n",
    "            kwargs = dict(\n",
    "                params=params_with_grad,\n",
    "                grads=grads,\n",
    "                exp_avgs=exp_avgs,\n",
    "                exp_avg_sqs=exp_avg_sqs,\n",
    "                exp_avg_diffs=exp_avg_diffs,\n",
    "                neg_pre_grads=neg_pre_grads,\n",
    "                beta1=beta1,\n",
    "                beta2=beta2,\n",
    "                beta3=beta3,\n",
    "                bias_correction1=bias_correction1,\n",
    "                bias_correction2=bias_correction2,\n",
    "                bias_correction3_sqrt=math.sqrt(bias_correction3),\n",
    "                lr=group['lr'],\n",
    "                weight_decay=group['weight_decay'],\n",
    "                eps=group['eps'],\n",
    "                no_prox=group['no_prox'],\n",
    "                clip_global_grad_norm=clip_global_grad_norm,\n",
    "            )\n",
    "\n",
    "            if group['foreach']:\n",
    "                if group['fused']:\n",
    "                    if torch.cuda.is_available():\n",
    "                        _fused_adan_multi_tensor(**kwargs)\n",
    "                    else:\n",
    "                        raise ValueError('Fused Adan does not support CPU')\n",
    "                else:\n",
    "                    _multi_tensor_adan(**kwargs)\n",
    "            elif group['fused']:\n",
    "                if torch.cuda.is_available():\n",
    "                    _fused_adan_single_tensor(**kwargs)\n",
    "                else:\n",
    "                    raise ValueError('Fused Adan does not support CPU')\n",
    "            else:\n",
    "                _single_tensor_adan(**kwargs)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def _single_tensor_adan(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    for i, param in enumerate(params):\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        exp_avg_diff = exp_avg_diffs[i]\n",
    "        neg_grad_or_diff = neg_pre_grads[i]\n",
    "\n",
    "        grad.mul_(clip_global_grad_norm)\n",
    "\n",
    "        # for memory saving, we use `neg_grad_or_diff`\n",
    "        # to get some temp variable in a inplace way\n",
    "        neg_grad_or_diff.add_(grad)\n",
    "\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  # m_t\n",
    "        exp_avg_diff.mul_(beta2).add_(neg_grad_or_diff,\n",
    "                                      alpha=1 - beta2)  # diff_t\n",
    "\n",
    "        neg_grad_or_diff.mul_(beta2).add_(grad)\n",
    "        exp_avg_sq.mul_(beta3).addcmul_(neg_grad_or_diff,\n",
    "                                        neg_grad_or_diff,\n",
    "                                        value=1 - beta3)  # n_t\n",
    "\n",
    "        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)\n",
    "        step_size_diff = lr * beta2 / bias_correction2\n",
    "        step_size = lr / bias_correction1\n",
    "\n",
    "        if no_prox:\n",
    "            param.mul_(1 - lr * weight_decay)\n",
    "            param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)\n",
    "        else:\n",
    "            param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)\n",
    "            param.div_(1 + lr * weight_decay)\n",
    "\n",
    "        neg_grad_or_diff.zero_().add_(grad, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _multi_tensor_adan(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    if len(params) == 0:\n",
    "        return\n",
    "\n",
    "    torch._foreach_mul_(grads, clip_global_grad_norm)\n",
    "\n",
    "    # for memory saving, we use `neg_pre_grads`\n",
    "    # to get some temp variable in a inplace way\n",
    "    torch._foreach_add_(neg_pre_grads, grads)\n",
    "\n",
    "    torch._foreach_mul_(exp_avgs, beta1)\n",
    "    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  # m_t\n",
    "\n",
    "    torch._foreach_mul_(exp_avg_diffs, beta2)\n",
    "    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,\n",
    "                        alpha=1 - beta2)  # diff_t\n",
    "\n",
    "    torch._foreach_mul_(neg_pre_grads, beta2)\n",
    "    torch._foreach_add_(neg_pre_grads, grads)\n",
    "    torch._foreach_mul_(exp_avg_sqs, beta3)\n",
    "    torch._foreach_addcmul_(exp_avg_sqs,\n",
    "                            neg_pre_grads,\n",
    "                            neg_pre_grads,\n",
    "                            value=1 - beta3)  # n_t\n",
    "\n",
    "    denom = torch._foreach_sqrt(exp_avg_sqs)\n",
    "    torch._foreach_div_(denom, bias_correction3_sqrt)\n",
    "    torch._foreach_add_(denom, eps)\n",
    "\n",
    "    step_size_diff = lr * beta2 / bias_correction2\n",
    "    step_size = lr / bias_correction1\n",
    "\n",
    "    if no_prox:\n",
    "        torch._foreach_mul_(params, 1 - lr * weight_decay)\n",
    "        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)\n",
    "        torch._foreach_addcdiv_(params,\n",
    "                                exp_avg_diffs,\n",
    "                                denom,\n",
    "                                value=-step_size_diff)\n",
    "    else:\n",
    "        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)\n",
    "        torch._foreach_addcdiv_(params,\n",
    "                                exp_avg_diffs,\n",
    "                                denom,\n",
    "                                value=-step_size_diff)\n",
    "        torch._foreach_div_(params, 1 + lr * weight_decay)\n",
    "    torch._foreach_zero_(neg_pre_grads)\n",
    "    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _fused_adan_multi_tensor(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    import fused_adan\n",
    "    multi_tensor_applier = MultiTensorApply(2048 * 32)\n",
    "    _dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
    "    multi_tensor_applier(\n",
    "        fused_adan.adan_multi_tensor, _dummy_overflow_buf,\n",
    "        [params, grads, exp_avgs, exp_avg_sqs, exp_avg_diffs, neg_pre_grads],\n",
    "        beta1, beta2, beta3, bias_correction1, bias_correction2,\n",
    "        bias_correction3_sqrt, lr, weight_decay, eps, no_prox,\n",
    "        clip_global_grad_norm)\n",
    "    torch._foreach_zero_(neg_pre_grads)\n",
    "    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _fused_adan_single_tensor(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    for i, param in enumerate(params):\n",
    "        p_data_fp32 = param.data.float()\n",
    "        out_p = param.data\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        exp_avg_diff = exp_avg_diffs[i]\n",
    "        neg_grad = neg_pre_grads[i]\n",
    "        with torch.cuda.device(param.device):\n",
    "            import fused_adan\n",
    "            fused_adan.adan_single_tensor(\n",
    "                p_data_fp32,\n",
    "                out_p,\n",
    "                grad,\n",
    "                exp_avg,\n",
    "                exp_avg_sq,\n",
    "                exp_avg_diff,\n",
    "                neg_grad,\n",
    "                beta1,\n",
    "                beta2,\n",
    "                beta3,\n",
    "                bias_correction1,\n",
    "                bias_correction2,\n",
    "                bias_correction3_sqrt,\n",
    "                lr,\n",
    "                weight_decay,\n",
    "                eps,\n",
    "                no_prox,\n",
    "                clip_global_grad_norm,\n",
    "            )\n",
    "        neg_grad.zero_().add_(grad, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _check_fused_available():\n",
    "    try:\n",
    "        import fused_adan\n",
    "    except ImportError as exc:\n",
    "        if torch.cuda.is_available():\n",
    "            # The module should be available but isn't. Try to\n",
    "            # help the user in this case.\n",
    "            raise ImportError((\n",
    "                str(exc)\n",
    "                + (\n",
    "                    '\\nThis could be caused by not having compiled '\n",
    "                    'the CUDA extension during package installation. '\n",
    "                    'Please try to re-install the package with '\n",
    "                    'the environment flag `FORCE_CUDA=1` set.'\n",
    "                )\n",
    "            ))\n",
    "        else:\n",
    "            raise ImportError(\n",
    "                str(exc) + '\\nFused Adan does not support CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fb94b8ab2a4c82a56f7a4e07d484f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2064\n",
      "Epoch [2/5], Loss: 0.0725\n",
      "Epoch [3/5], Loss: 0.0598\n",
      "Epoch [4/5], Loss: 0.0564\n",
      "Epoch [5/5], Loss: 0.0501\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 📌 1️⃣ Load MNIST Dataset (Grayscale 1-Channel Images)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  # Resize to match model input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 📌 2️⃣ Modify Model for MNIST (1-channel input, 10 output classes)\n",
    "model = ResNet_KAN(Block, layers=[2, 2, 2], input_shape=1, output_shape=10)\n",
    "\n",
    "# 📌 3️⃣ Define Loss & Optimizer\n",
    "# criterion = nn.NLLLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = Adan(model.parameters(), lr=0.001, betas=(0.98,0.92,0.99), weight_decay=1e-3)\n",
    "\n",
    "# 📌 4️⃣ Training Loop\n",
    "num_epochs = 5\n",
    "train_losses = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        # images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 📌 5️⃣ Plot Loss Curve\n",
    "plt.plot(train_losses, marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Loss Reduction Check\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SqueezeNet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fire(torch.nn.Module):\n",
    "    def __init__(self, inplanes: int , squeeze_planes: int, expand_planes: int):\n",
    "        super(fire, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.conv1 = torch.nn.Conv2d(inplanes, squeeze_planes, kernel_size=1, stride=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(squeeze_planes)\n",
    "        self.act1 = torch.nn.ReLU(inplace=True)\n",
    "        self.conv2 = torch.nn.Conv2d(squeeze_planes, expand_planes, kernel_size=1, stride=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(expand_planes)\n",
    "        self.conv3 = torch.nn.Conv2d(squeeze_planes, expand_planes, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(expand_planes)\n",
    "        self.act2 = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "        #using the initialization from Guo et al. (2024)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.normal_(m.weight, mean=-1.0, std=0.1)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.normal_(m.bias, mean=-1.0, std=0.1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        output1 = self.conv2(x)\n",
    "        output1 = self.bn2(output1)\n",
    "        output2 = self.conv3(x)\n",
    "        output2 = self.bn3(output2)\n",
    "        final_output = torch.cat((output1, output2), dim=1)\n",
    "        final_output = self.act2(final_output)\n",
    "        return final_output\n",
    "    \n",
    "class SqueezeNet(torch.nn.Module):\n",
    "    def __init__(self, number_of_classes=10):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 96, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(96)\n",
    "        self.relu1 = torch.nn.ReLU(inplace=True)\n",
    "        self.maxpool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fire2 = fire(96, 16, 64)\n",
    "        self.fire3 = fire(128, 16, 64)\n",
    "        self.fire4 = fire(128, 32, 128)\n",
    "        self.maxpool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fire5 = fire(256, 32, 128)\n",
    "        self.fire6 = fire(256, 48, 192)\n",
    "        self.fire7 = fire(384, 48, 192)\n",
    "        self.fire8 = fire(384, 64, 256)\n",
    "        self.maxpool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fire9 = fire(512, 64, 256)\n",
    "        self.conv2 = torch.nn.Conv2d(512, number_of_classes, kernel_size=1, stride=1)\n",
    "        self.average_pool = torch.nn.AvgPool2d(kernel_size=4, stride=4)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "        #using the initialization from Guo et al. (2024)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.normal_(m.weight, mean=-1.0, std=0.1)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.normal_(m.bias, mean=-1.0, std=0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.fire2(x)\n",
    "        x = self.fire3(x)\n",
    "        x = self.fire4(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.fire5(x)\n",
    "        x = self.fire6(x)\n",
    "        x = self.fire7(x)\n",
    "        x = self.fire8(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.fire9(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.average_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def fire_layer(inp, squeeze, expand):\n",
    "        return fire(inp, squeeze, expand)\n",
    "    \n",
    "\n",
    "model = SqueezeNet(number_of_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv2d: 1, BatchNorm2d: 1, ReLU: 1, MaxPool2d: 1, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, MaxPool2d: 1, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, MaxPool2d: 1, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Aries/lib/python3.11/site-packages/torchinfo/torchinfo.py:295\u001b[39m, in \u001b[36mforward_pass\u001b[39m\u001b[34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Aries/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Aries/lib/python3.11/site-packages/torch/nn/modules/module.py:1844\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1843\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1844\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1846\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1847\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1848\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Aries/lib/python3.11/site-packages/torch/nn/modules/module.py:1790\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1788\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mSqueezeNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     76\u001b[39m x = \u001b[38;5;28mself\u001b[39m.conv2(x)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maverage_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m x = torch.flatten(x, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Aries/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Aries/lib/python3.11/site-packages/torch/nn/modules/module.py:1844\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1843\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1844\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1846\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1847\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1848\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Aries/lib/python3.11/site-packages/torch/nn/modules/module.py:1790\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1788\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Aries/lib/python3.11/site-packages/torch/nn/modules/pooling.py:756\u001b[39m, in \u001b[36mAvgPool2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m756\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mavg_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcount_include_pad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdivisor_override\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Given input size: (10x2x2). Calculated output size: (10x0x0). Output size is too small",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# dummy_tensor = torch.randn(1, 3, 32, 32)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# model(dummy_tensor)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      5\u001b[39m total_params = \u001b[38;5;28msum\u001b[39m(p.numel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model.parameters())\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal number of parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Aries/lib/python3.11/site-packages/torchinfo/torchinfo.py:223\u001b[39m, in \u001b[36msummary\u001b[39m\u001b[34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m validate_user_params(\n\u001b[32m    217\u001b[39m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[32m    218\u001b[39m )\n\u001b[32m    220\u001b[39m x, correct_input_size = process_input(\n\u001b[32m    221\u001b[39m     input_data, input_size, batch_dim, device, dtypes\n\u001b[32m    222\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m summary_list = \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m formatting = FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[32m    227\u001b[39m results = ModelStatistics(\n\u001b[32m    228\u001b[39m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[32m    229\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Aries/lib/python3.11/site-packages/torchinfo/torchinfo.py:304\u001b[39m, in \u001b[36mforward_pass\u001b[39m\u001b[34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    303\u001b[39m     executed_layers = [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer.executed]\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    305\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    306\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv2d: 1, BatchNorm2d: 1, ReLU: 1, MaxPool2d: 1, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, MaxPool2d: 1, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, MaxPool2d: 1, fire: 1, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 2, BatchNorm2d: 2, Conv2d: 2, BatchNorm2d: 2, ReLU: 2, Conv2d: 1]"
     ]
    }
   ],
   "source": [
    "# dummy_tensor = torch.randn(1, 3, 32, 32)\n",
    "# model(dummy_tensor)\n",
    "\n",
    "print(summary(model=model, input_size=(1, 3, 32, 32), device='cpu'))\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "SqueezeNet                               [1, 10, 1, 1]             --\n",
      "├─Conv2d: 1-1                            [1, 96, 32, 32]           2,688\n",
      "├─BatchNorm2d: 1-2                       [1, 96, 32, 32]           192\n",
      "├─ReLU: 1-3                              [1, 96, 32, 32]           --\n",
      "├─MaxPool2d: 1-4                         [1, 96, 16, 16]           --\n",
      "├─fire: 1-5                              [1, 128, 16, 16]          --\n",
      "│    └─Conv2d: 2-1                       [1, 16, 16, 16]           1,552\n",
      "│    └─BatchNorm2d: 2-2                  [1, 16, 16, 16]           32\n",
      "│    └─ReLU: 2-3                         [1, 16, 16, 16]           --\n",
      "│    └─Conv2d: 2-4                       [1, 64, 16, 16]           1,088\n",
      "│    └─BatchNorm2d: 2-5                  [1, 64, 16, 16]           128\n",
      "│    └─Conv2d: 2-6                       [1, 64, 16, 16]           9,280\n",
      "│    └─BatchNorm2d: 2-7                  [1, 64, 16, 16]           128\n",
      "│    └─ReLU: 2-8                         [1, 128, 16, 16]          --\n",
      "├─fire: 1-6                              [1, 128, 16, 16]          --\n",
      "│    └─Conv2d: 2-9                       [1, 16, 16, 16]           2,064\n",
      "│    └─BatchNorm2d: 2-10                 [1, 16, 16, 16]           32\n",
      "│    └─ReLU: 2-11                        [1, 16, 16, 16]           --\n",
      "│    └─Conv2d: 2-12                      [1, 64, 16, 16]           1,088\n",
      "│    └─BatchNorm2d: 2-13                 [1, 64, 16, 16]           128\n",
      "│    └─Conv2d: 2-14                      [1, 64, 16, 16]           9,280\n",
      "│    └─BatchNorm2d: 2-15                 [1, 64, 16, 16]           128\n",
      "│    └─ReLU: 2-16                        [1, 128, 16, 16]          --\n",
      "├─fire: 1-7                              [1, 256, 16, 16]          --\n",
      "│    └─Conv2d: 2-17                      [1, 32, 16, 16]           4,128\n",
      "│    └─BatchNorm2d: 2-18                 [1, 32, 16, 16]           64\n",
      "│    └─ReLU: 2-19                        [1, 32, 16, 16]           --\n",
      "│    └─Conv2d: 2-20                      [1, 128, 16, 16]          4,224\n",
      "│    └─BatchNorm2d: 2-21                 [1, 128, 16, 16]          256\n",
      "│    └─Conv2d: 2-22                      [1, 128, 16, 16]          36,992\n",
      "│    └─BatchNorm2d: 2-23                 [1, 128, 16, 16]          256\n",
      "│    └─ReLU: 2-24                        [1, 256, 16, 16]          --\n",
      "├─MaxPool2d: 1-8                         [1, 256, 8, 8]            --\n",
      "├─fire: 1-9                              [1, 256, 8, 8]            --\n",
      "│    └─Conv2d: 2-25                      [1, 32, 8, 8]             8,224\n",
      "│    └─BatchNorm2d: 2-26                 [1, 32, 8, 8]             64\n",
      "│    └─ReLU: 2-27                        [1, 32, 8, 8]             --\n",
      "│    └─Conv2d: 2-28                      [1, 128, 8, 8]            4,224\n",
      "│    └─BatchNorm2d: 2-29                 [1, 128, 8, 8]            256\n",
      "│    └─Conv2d: 2-30                      [1, 128, 8, 8]            36,992\n",
      "│    └─BatchNorm2d: 2-31                 [1, 128, 8, 8]            256\n",
      "│    └─ReLU: 2-32                        [1, 256, 8, 8]            --\n",
      "├─fire: 1-10                             [1, 384, 8, 8]            --\n",
      "│    └─Conv2d: 2-33                      [1, 48, 8, 8]             12,336\n",
      "│    └─BatchNorm2d: 2-34                 [1, 48, 8, 8]             96\n",
      "│    └─ReLU: 2-35                        [1, 48, 8, 8]             --\n",
      "│    └─Conv2d: 2-36                      [1, 192, 8, 8]            9,408\n",
      "│    └─BatchNorm2d: 2-37                 [1, 192, 8, 8]            384\n",
      "│    └─Conv2d: 2-38                      [1, 192, 8, 8]            83,136\n",
      "│    └─BatchNorm2d: 2-39                 [1, 192, 8, 8]            384\n",
      "│    └─ReLU: 2-40                        [1, 384, 8, 8]            --\n",
      "├─fire: 1-11                             [1, 384, 8, 8]            --\n",
      "│    └─Conv2d: 2-41                      [1, 48, 8, 8]             18,480\n",
      "│    └─BatchNorm2d: 2-42                 [1, 48, 8, 8]             96\n",
      "│    └─ReLU: 2-43                        [1, 48, 8, 8]             --\n",
      "│    └─Conv2d: 2-44                      [1, 192, 8, 8]            9,408\n",
      "│    └─BatchNorm2d: 2-45                 [1, 192, 8, 8]            384\n",
      "│    └─Conv2d: 2-46                      [1, 192, 8, 8]            83,136\n",
      "│    └─BatchNorm2d: 2-47                 [1, 192, 8, 8]            384\n",
      "│    └─ReLU: 2-48                        [1, 384, 8, 8]            --\n",
      "├─fire: 1-12                             [1, 512, 8, 8]            --\n",
      "│    └─Conv2d: 2-49                      [1, 64, 8, 8]             24,640\n",
      "│    └─BatchNorm2d: 2-50                 [1, 64, 8, 8]             128\n",
      "│    └─ReLU: 2-51                        [1, 64, 8, 8]             --\n",
      "│    └─Conv2d: 2-52                      [1, 256, 8, 8]            16,640\n",
      "│    └─BatchNorm2d: 2-53                 [1, 256, 8, 8]            512\n",
      "│    └─Conv2d: 2-54                      [1, 256, 8, 8]            147,712\n",
      "│    └─BatchNorm2d: 2-55                 [1, 256, 8, 8]            512\n",
      "│    └─ReLU: 2-56                        [1, 512, 8, 8]            --\n",
      "├─MaxPool2d: 1-13                        [1, 512, 4, 4]            --\n",
      "├─fire: 1-14                             [1, 512, 4, 4]            --\n",
      "│    └─Conv2d: 2-57                      [1, 64, 4, 4]             32,832\n",
      "│    └─BatchNorm2d: 2-58                 [1, 64, 4, 4]             128\n",
      "│    └─ReLU: 2-59                        [1, 64, 4, 4]             --\n",
      "│    └─Conv2d: 2-60                      [1, 256, 4, 4]            16,640\n",
      "│    └─BatchNorm2d: 2-61                 [1, 256, 4, 4]            512\n",
      "│    └─Conv2d: 2-62                      [1, 256, 4, 4]            147,712\n",
      "│    └─BatchNorm2d: 2-63                 [1, 256, 4, 4]            512\n",
      "│    └─ReLU: 2-64                        [1, 512, 4, 4]            --\n",
      "├─Conv2d: 1-15                           [1, 10, 4, 4]             5,130\n",
      "├─AvgPool2d: 1-16                        [1, 10, 1, 1]             --\n",
      "├─LogSoftmax: 1-17                       [1, 10, 1, 1]             --\n",
      "==========================================================================================\n",
      "Total params: 734,986\n",
      "Trainable params: 734,986\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 52.92\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.85\n",
      "Params size (MB): 2.94\n",
      "Estimated Total Size (MB): 8.80\n",
      "==========================================================================================\n",
      "Total number of parameters: 734986\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "class fire(nn.Module):\n",
    "    def __init__(self, inplanes, squeeze_planes, expand_planes):\n",
    "        super(fire, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(squeeze_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=1, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(expand_planes)\n",
    "        self.conv3 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(expand_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # # using MSR initilization\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d):\n",
    "        #         n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
    "        #         m.weight.data.normal_(0, math.sqrt(2./n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        out1 = self.conv2(x)\n",
    "        out1 = self.bn2(out1)\n",
    "        out2 = self.conv3(x)\n",
    "        out2 = self.bn3(out2)\n",
    "        out = torch.cat([out1, out2], 1)\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SqueezeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=3, stride=1, padding=1) # 32\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2) # 16\n",
    "        self.fire2 = fire(96, 16, 64)\n",
    "        self.fire3 = fire(128, 16, 64)\n",
    "        self.fire4 = fire(128, 32, 128)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2) # 8\n",
    "        self.fire5 = fire(256, 32, 128)\n",
    "        self.fire6 = fire(256, 48, 192)\n",
    "        self.fire7 = fire(384, 48, 192)\n",
    "        self.fire8 = fire(384, 64, 256)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2) # 4\n",
    "        self.fire9 = fire(512, 64, 256)\n",
    "        self.conv2 = nn.Conv2d(512, 10, kernel_size=1, stride=1)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=4, stride=4)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d):\n",
    "        #         n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
    "        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        #     elif isinstance(m, nn.BatchNorm2d):\n",
    "        #         m.weight.data.fill_(1)\n",
    "        #         m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.fire2(x)\n",
    "        x = self.fire3(x)\n",
    "        x = self.fire4(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.fire5(x)\n",
    "        x = self.fire6(x)\n",
    "        x = self.fire7(x)\n",
    "        x = self.fire8(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.fire9(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "def fire_layer(inp, s, e):\n",
    "    f = fire(inp, s, e)\n",
    "    return f\n",
    "\n",
    "model = SqueezeNet()\n",
    "\n",
    "print(summary(model=model, input_size=(1, 3, 32, 32), device='cpu'))\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fire(torch.nn.Module):\n",
    "    def __init__(self, inplanes: int , squeeze_planes: int, expand1x1: int, expand3x3: int) -> None:\n",
    "        super(fire, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.squeeze = torch.nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
    "        self.act1 = torch.nn.ReLU(inplace=True)\n",
    "        self.expand1x1\n",
    "\n",
    "        #using the initialization from Guo et al. (2024)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.normal_(m.weight, mean=-1.0, std=0.1)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.normal_(m.bias, mean=-1.0, std=0.1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        output1 = self.conv2(x)\n",
    "        output1 = self.bn2(output1)\n",
    "        output2 = self.conv3(x)\n",
    "        output2 = self.bn3(output2)\n",
    "        final_output = torch.cat((output1, output2), dim=1)\n",
    "        final_output = self.act2(final_output)\n",
    "        return final_output\n",
    "    \n",
    "class SqueezeNet(torch.nn.Module):\n",
    "    def __init__(self, number_of_classes=10):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 96, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(96)\n",
    "        self.relu1 = torch.nn.ReLU(inplace=True)\n",
    "        self.maxpool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fire2 = fire(96, 16, 64)\n",
    "        self.fire3 = fire(128, 16, 64)\n",
    "        self.fire4 = fire(128, 32, 128)\n",
    "        self.maxpool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fire5 = fire(256, 32, 128)\n",
    "        self.fire6 = fire(256, 48, 192)\n",
    "        self.fire7 = fire(384, 48, 192)\n",
    "        self.fire8 = fire(384, 64, 256)\n",
    "        self.maxpool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fire9 = fire(512, 64, 256)\n",
    "        self.conv2 = torch.nn.Conv2d(512, number_of_classes, kernel_size=1, stride=1)\n",
    "        self.average_pool = torch.nn.AvgPool2d(kernel_size=4, stride=4)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "        #using the initialization from Guo et al. (2024)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.normal_(m.weight, mean=-1.0, std=0.1)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.normal_(m.bias, mean=-1.0, std=0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.fire2(x)\n",
    "        x = self.fire3(x)\n",
    "        x = self.fire4(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.fire5(x)\n",
    "        x = self.fire6(x)\n",
    "        x = self.fire7(x)\n",
    "        x = self.fire8(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.fire9(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.average_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def fire_layer(inp, squeeze, expand):\n",
    "        return fire(inp, squeeze, expand)\n",
    "    \n",
    "\n",
    "model = SqueezeNet(number_of_classes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
