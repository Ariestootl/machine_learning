{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 Classification Model for ISL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sign_Lang_VGG(\n",
      "  (convolutional_block_1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (convolutional_block_2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (convolutional_block_3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (convolutional_block_4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (convolutional_block_5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=8192, out_features=1000, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=1000, out_features=26, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 23934714\n"
     ]
    }
   ],
   "source": [
    "#Building Model: ISL Classification Model\n",
    "class Sign_Lang_VGG(torch.nn.Module):\n",
    "  '''\n",
    "  Built a VGG16 architecture variant for  Indian Sign Language Classification\n",
    "  The dataset: https://www.kaggle.com/datasets/prathumarikeri/indian-sign-language-isl\n",
    "  '''\n",
    "  def __init__(self,\n",
    "               input_shape: int,\n",
    "               output_shape: int):\n",
    "    super().__init__()\n",
    "    self.convolutional_block_1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=input_shape,\n",
    "                  out_channels=64,\n",
    "                  kernel_size=(3,3),\n",
    "                  stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=64,\n",
    "                  out_channels=64,\n",
    "                  kernel_size=(3,3),\n",
    "                  stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "    )\n",
    "    self.convolutional_block_2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=64,\n",
    "                  out_channels=128,\n",
    "                  kernel_size=(3,3),\n",
    "                  stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=128,\n",
    "                out_channels=128,\n",
    "                kernel_size=(3,3),\n",
    "                stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2))\n",
    "    )\n",
    "    self.convolutional_block_3 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=128,\n",
    "                  out_channels=256,\n",
    "                  kernel_size=(3,3),\n",
    "                  stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=256,\n",
    "                out_channels=256,\n",
    "                kernel_size=(3,3),\n",
    "                stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=256,\n",
    "        out_channels=256,\n",
    "        kernel_size=(3,3),\n",
    "        stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2))\n",
    "    )\n",
    "    self.convolutional_block_4 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=256,\n",
    "                  out_channels=512,\n",
    "                  kernel_size=(3,3),\n",
    "                  stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512,\n",
    "                out_channels=512,\n",
    "                kernel_size=(3,3),\n",
    "                stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512,\n",
    "        out_channels=512,\n",
    "        kernel_size=(3,3),\n",
    "        stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2))\n",
    "    )\n",
    "    self.convolutional_block_5 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=512,\n",
    "                  out_channels=512,\n",
    "                  kernel_size=(3,3),\n",
    "                  stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512,\n",
    "                out_channels=512,\n",
    "                kernel_size=(3,3),\n",
    "                stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512,\n",
    "        out_channels=512,\n",
    "        kernel_size=(3,3),\n",
    "        stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2))\n",
    "    )\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(512*4*4,\n",
    "                  1000),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1000,\n",
    "                  1000),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1000,\n",
    "                  output_shape)\n",
    "    )\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    x = self.convolutional_block_1(x)\n",
    "    x = self.convolutional_block_2(x)\n",
    "    x = self.convolutional_block_3(x)\n",
    "    x = self.convolutional_block_4(x)\n",
    "    x = self.convolutional_block_5(x)\n",
    "    x = self.classifier(x)\n",
    "    return x\n",
    "\n",
    "seed = 25\n",
    "torch.manual_seed(seed)\n",
    "model= Sign_Lang_VGG(input_shape=3,\n",
    "                  output_shape=26)\n",
    "print(f\"{model}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16 = [64, 64, \"MaxPool\", 128, 128, \"MaxPool\", 256, 256, 256, \"MaxPool\", 512, 512, 512, \"MaxPool\", 512, 512, 512, \"MaxPool\"]\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, input=3, output_shape=1000):\n",
    "        super(VGG, self).__init__()\n",
    "        self.input = input\n",
    "        self.convolutional_blocks = self.create_convolutional_blocks(VGG16)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(512*7*7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, out):\n",
    "        out = self.convolutional_blocks(out)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "    \n",
    "    def create_convolutional_blocks(self, architecture):\n",
    "        layers = []\n",
    "        input = self.input\n",
    "\n",
    "        for k in architecture:\n",
    "            if type(k) == int:\n",
    "                output = k\n",
    "\n",
    "                layers += [nn.Conv2d(input, output, \n",
    "                                     kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "                                     nn.ReLU()]\n",
    "                input = k\n",
    "            elif k == \"MaxPool\":\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n",
    "        return nn.Sequential(*layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FSL VGG13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (convolutional_blocks): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU()\n",
      "    (17): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU()\n",
      "    (20): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU()\n",
      "    (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU()\n",
      "    (27): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU()\n",
      "    (30): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU()\n",
      "    (33): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=1024, out_features=2096, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=2096, out_features=2096, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.2, inplace=False)\n",
      "    (7): Linear(in_features=2096, out_features=26, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 11903866\n"
     ]
    }
   ],
   "source": [
    "chosen_model_params = [32, 32, \"MaxPool\", 64, 64, \"MaxPool\", 128, 128, 128,\"MaxPool\", 256, 256, 256, \"MaxPool\"]\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, input=3, output_shape=1000):\n",
    "        super(VGG, self).__init__()\n",
    "        self.input = input\n",
    "        self.convolutional_blocks = self.create_convolutional_blocks(chosen_model_params)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(256*2*2, 2096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2096, 2096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2096, output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, out):\n",
    "        out = self.convolutional_blocks(out)\n",
    "        # print(f\"Shape after convolutional blocks: {out.shape}\") ### --> print this to get proper shape for the fully connected layer\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "    \n",
    "    def create_convolutional_blocks(self, architecture):\n",
    "        layers = []\n",
    "        input = self.input\n",
    "\n",
    "        for k in architecture:\n",
    "            if type(k) == int:\n",
    "                output = k\n",
    "\n",
    "                layers += [nn.Conv2d(input, output, \n",
    "                                     kernel_size=(5,5), stride=(1,1), padding=(1,1)),\n",
    "                                     nn.BatchNorm2d(k),\n",
    "                                     nn.ReLU()]\n",
    "                input = k\n",
    "            elif k == \"MaxPool\":\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "seed = 25\n",
    "torch.manual_seed(25)\n",
    "# fsl_model = VGG(input=3, output_shape=len(train_data.classes)).to(device)\n",
    "fsl_model = VGG(input=3, output_shape=26)\n",
    "print(f\"{fsl_model}\")\n",
    "\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in fsl_model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=2048, out_features=17, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 23563153\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    expansion: int = 4\n",
    "    def __init__(self, input, output, identity_downsample=None, stride=(1,1)):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input, output, kernel_size=(1,1), stride=(1,1), padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(output)\n",
    "        self.conv2 = nn.Conv2d(output, output, kernel_size=(3,3), stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(output)\n",
    "        self.conv3 = nn.Conv2d(output, output*self.expansion, \n",
    "                               kernel_size=(1,1), stride=(1,1), padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(output*self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block: Block, layers, input_shape, output_shape):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.input = 64\n",
    "        self.conv1 = nn.Conv2d(input_shape, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3))\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(3,3), stride=(2,2), padding=(1,1))\n",
    "\n",
    "        ###ResNet Layers\n",
    "        self.layer1 = self.make_layer(block,layers[0], output=64, stride=(1,1))\n",
    "        self.layer2 = self.make_layer(block,layers[1], output=128, stride=(1,1))\n",
    "        self.layer3 = self.make_layer(block,layers[2], output=256, stride=(1,1))\n",
    "        self.layer4 = self.make_layer(block,layers[3], output=512, stride=(1,1))\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Sequential( \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512*block.expansion, output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def make_layer(self, block, num_residual_MyBlocks, output, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.input != output*block.expansion:\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(\n",
    "                self.input, output * block.expansion, kernel_size=(1,1), stride=stride),\n",
    "                nn.BatchNorm2d(output*4)\n",
    "            )\n",
    "        layers.append(block(self.input, output, identity_downsample, stride))\n",
    "        self.input = output*block.expansion\n",
    "\n",
    "        for i in range(num_residual_MyBlocks-1):\n",
    "            layers.append(block(self.input, output))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "seed = 25\n",
    "torch.manual_seed(25)\n",
    "model2 = ResNet(Block, [3,4,6,3], input_shape=1, output_shape=17)\n",
    "print(f\"{model2}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in model2.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=256, out_features=17, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 1424721\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    expansion: int = 2\n",
    "\n",
    "    def __init__(self, input, output, identity_downsample=None, stride=(1,1)):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input, output, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.bn1 = nn.BatchNorm2d(output)\n",
    "        self.conv2 = nn.Conv2d(output,output*self.expansion, kernel_size=(3,3), stride=stride, padding=(1,1))\n",
    "        self.bn2 = nn.BatchNorm2d(output*self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        # print(f\"{self.expansion}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block: Block, layers, input_shape, output_shape):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.input = 16\n",
    "        self.conv1 = nn.Conv2d(input_shape, 16, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2,2), stride=1, padding=0)\n",
    "\n",
    "        self.layer1 = self.create_layers(block, layers[0], output=16, stride=1)\n",
    "        self.layer2 = self.create_layers(block, layers[1], output=32, stride=2)\n",
    "        self.layer3 = self.create_layers(block, layers[2], output=64, stride=2)\n",
    "        self.layer4 = self.create_layers(block, layers[3], output=128, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential( \n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(128 * block.expansion, output_shape)\n",
    "        )\n",
    "        # print(f\"{block.expansion}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def create_layers(self, block, num_residual_blocks, output, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.input != output * block.expansion:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.input, output * block.expansion, kernel_size=(1,1), stride=stride),\n",
    "                nn.BatchNorm2d(output * block.expansion)\n",
    "            )\n",
    "\n",
    "        layers.append(block(self.input, output, identity_downsample, stride))\n",
    "        self.input = output * block.expansion\n",
    "\n",
    "        for _ in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.input, output))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "  \n",
    "\n",
    "seed = 25\n",
    "torch.manual_seed(25)\n",
    "model2 = ResNet(Block, [2,2,2,2], input_shape=1, output_shape=17)\n",
    "print(f\"{model2}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in model2.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    expansion: int = 2\n",
    "\n",
    "    def __init__(self, input, output, identity_downsample=None, stride=(1,1)):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input, output, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.bn1 = nn.BatchNorm2d(output)\n",
    "        self.conv2 = nn.Conv2d(output,output*self.expansion, kernel_size=(3,3), stride=stride, padding=(1,1))\n",
    "        self.bn2 = nn.BatchNorm2d(output*self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class Baybayin_ResNet(nn.Module):\n",
    "    def __init__(self, block: Block, layers, input_shape, output_shape):\n",
    "        super(Baybayin_ResNet, self).__init__()\n",
    "        self.input = 16\n",
    "        self.conv1 = nn.Conv2d(input_shape, 16, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2,2), stride=1, padding=0)\n",
    "\n",
    "        self.layer1 = self.create_layers(block, layers[0], output=16, stride=1)\n",
    "        self.layer2 = self.create_layers(block, layers[1], output=32, stride=2)\n",
    "        self.layer3 = self.create_layers(block, layers[2], output=64, stride=2)\n",
    "        self.layer4 = self.create_layers(block, layers[3], output=128, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential( \n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(128 * block.expansion, output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def create_layers(self, block, num_residual_blocks, output, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.input != output * block.expansion:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.input, output * block.expansion, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(output * block.expansion)\n",
    "            )\n",
    "\n",
    "        layers.append(block(self.input, output, identity_downsample, stride))\n",
    "        self.input = output * block.expansion\n",
    "\n",
    "        for _ in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.input, output))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "  \n",
    "\n",
    "def ResNet18(img_channels=1, output_shape=10):\n",
    "    return ResNet(Block, [2, 2, 2, 2], img_channels, output_shape)\n",
    "\n",
    "# Example usage:\n",
    "model = ResNet18()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FSL ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=512, out_features=26, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 10743674\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    expansion: int = 2\n",
    "    def __init__(self, input, output, identity_downsample=None, stride=(1,1)):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input, output, kernel_size=(3,3), stride=(1,1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(output)\n",
    "        self.conv2 = nn.Conv2d(output, output*self.expansion, \n",
    "                               kernel_size=(3,3), stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(output*self.expansion,)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block: Block, layers, input_shape, output_shape):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.input = 32\n",
    "        self.conv1 = nn.Conv2d(input_shape, 32, kernel_size=(5,5), stride=(2,2), padding=(2,2))\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(3,3), stride=(2,2), padding=(1,1))\n",
    "\n",
    "        ###ResNet Layers\n",
    "        self.layer1 = self.make_layer(block,layers[0], output=32, stride=(1,1))\n",
    "        self.layer2 = self.make_layer(block,layers[1], output=64, stride=(2,2))\n",
    "        self.layer3 = self.make_layer(block,layers[2], output=128, stride=(2,2))\n",
    "        self.layer4 = self.make_layer(block,layers[3], output=256, stride=(2,2))\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Sequential( \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*block.expansion, output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def make_layer(self, block, num_residual_MyBlocks, output, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.input != output*block.expansion:\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(\n",
    "                self.input, output * block.expansion, kernel_size=(1,1), stride=stride),\n",
    "                nn.BatchNorm2d(output*block.expansion)\n",
    "            )\n",
    "        layers.append(block(self.input, output, identity_downsample, stride))\n",
    "        self.input = output*block.expansion\n",
    "\n",
    "        for i in range(num_residual_MyBlocks-1):\n",
    "            layers.append(block(self.input, output))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "seed = 25\n",
    "torch.manual_seed(25)\n",
    "# fsl_model = ResNet(Block, [3,4,6,3], input_shape=3, output_shape=26).to(device)\n",
    "fsl_model = ResNet(Block, [3,4,6,3], input_shape=3, output_shape=26)\n",
    "print(f\"{fsl_model}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in fsl_model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper codes F1, Recall, Precision, and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import ConfusionMatrix\n",
    "from torchmetrics.functional import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "def infer(data_loader:torch.utils.data.DataLoader,\n",
    "           model:torch.nn.Module,\n",
    "           test_data):\n",
    "    y_preds = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in tqdm(data_loader):\n",
    "            # Send the data to the proper target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Forward pass\n",
    "            y_logit = model(X)\n",
    "            # Turn predictions into class labels\n",
    "            y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1)\n",
    "            # Put predictions on CPU for evaluation\n",
    "            y_preds.append(y_pred.cpu())\n",
    "\n",
    "    # Concatenate list of predictions into a tensor\n",
    "    y_preds = torch.cat(y_preds)  # This line ensures y_preds is a tensor\n",
    "\n",
    "    # Ensure test_data.targets is a tensor\n",
    "    test_targets_tensor = torch.tensor(test_data.targets) if not isinstance(test_data.targets, torch.Tensor) else test_data.targets\n",
    "    return test_targets_tensor, y_preds\n",
    "\n",
    "test_targets1, y_preds1 = infer(test_dataloader, model_LION, test_data)\n",
    "\n",
    "def create_confusion_matrix(y_true, y_pred, class_names, num_classes=None, figsize=(19, 10)):\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = len(class_names)\n",
    "    \n",
    "    # Create confusion matrix instance\n",
    "    confmat = ConfusionMatrix(num_classes=num_classes, task=\"multiclass\")\n",
    "    confmat_tensor = confmat(preds=y_pred, target=y_true)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plot_confusion_matrix(\n",
    "        conf_mat=confmat_tensor.numpy(),\n",
    "        class_names=class_names,\n",
    "        figsize=figsize\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    return confmat_tensor\n",
    "\n",
    "create_confusion_matrix(test_targets1, y_preds1, class_names=train_data.classes)\n",
    "f1_score1, precision_score1, recall_score1 = calculate_metric(test_targets1.cpu(), y_preds1)\n",
    "print(f\"Print F1 score: {f1_score1}\")\n",
    "print(f\"Precision Score: {precision_score1}\")\n",
    "print(f\"Recall Score: {recall_score1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "def calculate_metric(y_true, y_pred, average=\"weighted\"): ##--> Choose 'micro', 'macro', or 'weighted'\n",
    "    # Convert logits to predicted class labels if necessary\n",
    "    if y_pred.dim() > 1:\n",
    "        y_pred = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "    # Move to CPU and convert to numpy arrays if necessary\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_true, y_pred, average=average)  \n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    \n",
    "    return f1, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN KAN\n",
    "\n",
    "Convolutional Neural Network Kolgomorov Arnold Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG_KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG_KAN(\n",
      "  (convolutional_blocks): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SELU()\n",
      "    (3): MaxPool2d(kernel_size=(4, 4), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): SELU()\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): SELU()\n",
      "    (10): MaxPool2d(kernel_size=(4, 4), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (KANClassifier): Sequential(\n",
      "    (0): NaiveFourierKANLayer()\n",
      "    (1): NaiveFourierKANLayer()\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 15302556\n"
     ]
    }
   ],
   "source": [
    "# chosen_model_params = [32, 32, \"MaxPool\", 64, 64, \"MaxPool\", 128, 128, 128,\"MaxPool\", 256, 256, 256, \"MaxPool\"]\n",
    "chosen_model_params = [64, \"MaxPool\",128, 128, \"MaxPool\"]\n",
    "# chosen_model_params = [16,16,\"MaxPool\",32,\"Maxpool\"]\n",
    "\n",
    "class NaiveFourierKANLayer(nn.Module):\n",
    "    def __init__(self, inputdim, outdim, initial_gridsize, addbias=True):\n",
    "        super(NaiveFourierKANLayer, self).__init__()\n",
    "        self.addbias = addbias\n",
    "        self.inputdim = inputdim\n",
    "        self.outdim = outdim\n",
    "        self.gridsize_param = nn.Parameter(torch.tensor(initial_gridsize, dtype=torch.float32)) #adjusted during training\n",
    "        self.fouriercoeffs = nn.Parameter(torch.empty(2, outdim, inputdim, initial_gridsize)) #adjusted during training\n",
    "        nn.init.xavier_uniform_(self.fouriercoeffs)\n",
    "        if self.addbias:\n",
    "            self.bias = nn.Parameter(torch.zeros(1, outdim))\n",
    "\n",
    "    def forward(self, x): #Combines cosine/sine terms with learnable coefficents for Fourier expansion and sums them + bias\n",
    "        gridsize = torch.clamp(self.gridsize_param, min=1).round().int()\n",
    "        outshape = x.shape[:-1] + (self.outdim,)\n",
    "        x = torch.reshape(x, (-1, self.inputdim))\n",
    "        k = torch.reshape(torch.arange(1, gridsize + 1, device=x.device), (1, 1, 1, gridsize))\n",
    "        xrshp = torch.reshape(x, (x.shape[0], 1, x.shape[1], 1))\n",
    "        c = torch.cos(k * xrshp)\n",
    "        s = torch.sin(k * xrshp)\n",
    "        y = torch.sum(c * self.fouriercoeffs[0:1, :, :, :gridsize], (-2, -1))\n",
    "        y += torch.sum(s * self.fouriercoeffs[1:2, :, :, :gridsize], (-2, -1))\n",
    "        if self.addbias:\n",
    "            y += self.bias\n",
    "        y = torch.reshape(y, outshape)\n",
    "        return y\n",
    "\n",
    "class VGG_KAN(nn.Module):\n",
    "    def __init__(self, input=3, output_shape=26):\n",
    "        super(VGG_KAN, self).__init__()\n",
    "        self.input = input\n",
    "        self.convolutional_blocks = self.create_convolutional_blocks(chosen_model_params)\n",
    "\n",
    "        self.KANClassifier = nn.Sequential(\n",
    "            NaiveFourierKANLayer(128*3*3, 256, initial_gridsize=30),\n",
    "            NaiveFourierKANLayer(256, output_shape, initial_gridsize=25)\n",
    "\n",
    "        )\n",
    "    \n",
    "    def forward(self, out):\n",
    "        out = self.convolutional_blocks(out)\n",
    "        # print(f\"Shape after convolutional blocks: {out.shape}\") ### --> print this to get proper shape for the fully connected layer\n",
    "\n",
    "        # out = self.classifier(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.KANClassifier(out)\n",
    "        return out\n",
    "    \n",
    "    def create_convolutional_blocks(self, architecture):\n",
    "        layers = []\n",
    "        input = self.input\n",
    "\n",
    "        for k in architecture:\n",
    "            if type(k) == int:\n",
    "                output = k\n",
    "\n",
    "                layers += [nn.Conv2d(input, output, \n",
    "                                     kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
    "                                     nn.BatchNorm2d(k),\n",
    "                                     nn.SELU()]\n",
    "                input = k\n",
    "            elif k == \"MaxPool\":\n",
    "                layers += [nn.MaxPool2d(kernel_size=(4,4), stride=(2,2))]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "\n",
    "torch.manual_seed(42)\n",
    "fsl_model = VGG_KAN(input=3, output_shape=26)\n",
    "print(f\"{fsl_model}\")\n",
    "\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in fsl_model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128])\n",
      "Shape after convolutional blocks: torch.Size([1, 256, 7, 7])\n",
      "tensor([[-0.0244,  0.1089, -0.3710, -0.1900, -0.3575, -0.3632,  0.3811, -0.0240,\n",
      "         -0.0849, -0.0614,  0.0439, -0.0904, -0.1795, -0.0823,  0.0301,  0.2366,\n",
      "          0.3434,  0.1383,  0.1374,  0.2822,  0.1175,  0.0741,  0.1067, -0.1370,\n",
      "          0.1333, -0.2222]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn(size=(3, 128, 128))  # Single image (C, H, W)\n",
    "print(tensor1.shape) \n",
    "\n",
    "output = fsl_model(tensor1.unsqueeze(dim=0))  # Add batch dimension\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet_KAN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (KANClassifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): NaiveFourierKANLayer()\n",
      "    (2): NaiveFourierKANLayer()\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 33217756\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NaiveFourierKANLayer(nn.Module):\n",
    "    def __init__(self, inputdim, outdim, initial_gridsize, addbias=True):\n",
    "        super(NaiveFourierKANLayer, self).__init__()\n",
    "        self.addbias = addbias\n",
    "        self.inputdim = inputdim\n",
    "        self.outdim = outdim\n",
    "        self.gridsize_param = nn.Parameter(torch.tensor(initial_gridsize, dtype=torch.float32)) #adjusted during training\n",
    "        self.fouriercoeffs = nn.Parameter(torch.empty(2, outdim, inputdim, initial_gridsize))\n",
    "        nn.init.xavier_uniform_(self.fouriercoeffs)\n",
    "        if self.addbias:\n",
    "            self.bias = nn.Parameter(torch.zeros(1, outdim))\n",
    "            nn.init.kaiming_normal_(self.bias)\n",
    "\n",
    "    def forward(self, x): #Combines cosine/sine terms with learnable coefficents for Fourier expansion and sums them + bias\n",
    "        gridsize = torch.clamp(self.gridsize_param, min=1).round().int()\n",
    "        outshape = x.shape[:-1] + (self.outdim,)\n",
    "        x = torch.reshape(x, (-1, self.inputdim))\n",
    "        k = torch.reshape(torch.arange(1, gridsize + 1, device=x.device), (1, 1, 1, gridsize))\n",
    "        xrshp = torch.reshape(x, (x.shape[0], 1, x.shape[1], 1))\n",
    "        c = torch.cos(k * xrshp)\n",
    "        s = torch.sin(k * xrshp)\n",
    "        y = torch.sum(c * self.fouriercoeffs[0:1, :, :, :gridsize], (-2, -1))\n",
    "        y += torch.sum(s * self.fouriercoeffs[1:2, :, :, :gridsize], (-2, -1))\n",
    "        if self.addbias:\n",
    "            y += self.bias\n",
    "        y = torch.reshape(y, outshape)\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    expansion: int = 2\n",
    "    def __init__(self, input, output, identity_downsample=None, stride=(1,1)):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input, output, kernel_size=(3,3), stride=(1,1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(output)\n",
    "        self.conv2 = nn.Conv2d(output, output*self.expansion, \n",
    "                               kernel_size=(3,3), stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(output*self.expansion,)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet_KAN(nn.Module):\n",
    "    def __init__(self, block: Block, layers, input_shape, output_shape):\n",
    "        super(ResNet_KAN, self).__init__()\n",
    "        self.input = 32\n",
    "        self.conv1 = nn.Conv2d(input_shape, 32, kernel_size=(5,5), stride=(2,2), padding=(2,2))\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(3,3), stride=(2,2), padding=(1,1))\n",
    "\n",
    "        ###ResNet Layers\n",
    "        self.layer1 = self.make_layer(block,layers[0], output=32, stride=(1,1))\n",
    "        self.layer2 = self.make_layer(block,layers[1], output=64, stride=(2,2))\n",
    "        self.layer3 = self.make_layer(block,layers[2], output=128, stride=(2,2))\n",
    "        self.layer4 = self.make_layer(block,layers[3], output=256, stride=(2,2))\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        self.KANClassifier = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            NaiveFourierKANLayer(256*block.expansion, 512, initial_gridsize=50),\n",
    "            NaiveFourierKANLayer(512, output_shape, initial_gridsize=50)\n",
    "\n",
    "        )\n",
    "    \n",
    "    def forward(self,x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = self.KANClassifier(x)\n",
    "        return x\n",
    "    \n",
    "    def make_layer(self, block, num_residual_MyBlocks, output, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.input != output*block.expansion:\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(\n",
    "                self.input, output * block.expansion, kernel_size=(1,1), stride=stride),\n",
    "                nn.BatchNorm2d(output*block.expansion)\n",
    "            )\n",
    "        layers.append(block(self.input, output, identity_downsample, stride))\n",
    "        self.input = output*block.expansion\n",
    "\n",
    "        for i in range(num_residual_MyBlocks-1):\n",
    "            layers.append(block(self.input, output))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(23)\n",
    "fsl_model = ResNet_KAN(Block, [2,2,2,2], input_shape=3, \n",
    "                       output_shape=26)\n",
    "print(f\"{fsl_model}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in fsl_model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View initialization for ResNet KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv1.bias, Biases: tensor([-0.0781, -0.1030,  0.0394,  0.0026,  0.1062, -0.0296, -0.0620, -0.0299,\n",
      "        -0.0952,  0.0856, -0.0479,  0.0041, -0.0407, -0.0992,  0.0846,  0.1045,\n",
      "         0.0254, -0.0170,  0.1145,  0.0511,  0.0584,  0.0230,  0.1045, -0.0673,\n",
      "         0.0402,  0.0710, -0.0604, -0.0720, -0.0469, -0.0301,  0.0789,  0.0958])\n",
      "Layer: bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer1.0.conv1.bias, Biases: tensor([-0.0253, -0.0417,  0.0310, -0.0109,  0.0068, -0.0416, -0.0042,  0.0353,\n",
      "        -0.0095,  0.0414,  0.0152, -0.0585,  0.0407, -0.0427,  0.0546, -0.0252,\n",
      "        -0.0504, -0.0239,  0.0060,  0.0002, -0.0388,  0.0135, -0.0153,  0.0252,\n",
      "        -0.0422,  0.0571,  0.0250, -0.0188,  0.0241,  0.0499,  0.0254,  0.0565])\n",
      "Layer: layer1.0.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer1.0.conv2.bias, Biases: tensor([-0.0579,  0.0131, -0.0047, -0.0491,  0.0358,  0.0515,  0.0093, -0.0347,\n",
      "         0.0532, -0.0210,  0.0334,  0.0391, -0.0239, -0.0342,  0.0304,  0.0173,\n",
      "         0.0354, -0.0463,  0.0262,  0.0068, -0.0108, -0.0028, -0.0486, -0.0160,\n",
      "        -0.0064,  0.0017,  0.0349, -0.0285,  0.0188,  0.0391,  0.0585, -0.0470,\n",
      "         0.0466, -0.0488, -0.0308, -0.0361, -0.0374, -0.0398, -0.0280,  0.0141,\n",
      "        -0.0381, -0.0566,  0.0062, -0.0500,  0.0528, -0.0118,  0.0388, -0.0364,\n",
      "         0.0509, -0.0461,  0.0070, -0.0418, -0.0471, -0.0582,  0.0225,  0.0513,\n",
      "         0.0138, -0.0566,  0.0025, -0.0369,  0.0420, -0.0038,  0.0410, -0.0106])\n",
      "Layer: layer1.0.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer1.0.identity_downsample.0.bias, Biases: tensor([-0.0742,  0.0336,  0.1620,  0.1529, -0.1516,  0.1710,  0.1039,  0.0396,\n",
      "         0.0338,  0.0468,  0.1593,  0.1536,  0.1189, -0.0048,  0.0571, -0.1432,\n",
      "         0.0149, -0.1099,  0.1181, -0.0836,  0.0252,  0.0821,  0.0654, -0.1745,\n",
      "        -0.0269,  0.1432, -0.1620, -0.0229, -0.0750,  0.0087, -0.1514,  0.0647,\n",
      "         0.1117, -0.1264,  0.0184,  0.0709,  0.1040,  0.0351,  0.1153, -0.0414,\n",
      "        -0.0411, -0.1503, -0.1630,  0.0205, -0.1548,  0.1504,  0.1192, -0.1256,\n",
      "         0.1342,  0.0394,  0.0935,  0.0851, -0.1093, -0.0744,  0.1376,  0.0165,\n",
      "        -0.0892, -0.0338,  0.1607,  0.0132, -0.0976,  0.0407, -0.1030,  0.1203])\n",
      "Layer: layer1.0.identity_downsample.1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer1.1.conv1.bias, Biases: tensor([-0.0266,  0.0210,  0.0134, -0.0048,  0.0016,  0.0128, -0.0417,  0.0320,\n",
      "         0.0017,  0.0075, -0.0193, -0.0128,  0.0138,  0.0081, -0.0154, -0.0319,\n",
      "        -0.0121,  0.0070,  0.0316, -0.0076,  0.0173,  0.0046, -0.0385, -0.0344,\n",
      "         0.0317,  0.0059,  0.0411,  0.0386,  0.0108, -0.0034,  0.0261,  0.0311])\n",
      "Layer: layer1.1.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer1.1.conv2.bias, Biases: tensor([-0.0230, -0.0269,  0.0573, -0.0188,  0.0526, -0.0209, -0.0344,  0.0115,\n",
      "         0.0080,  0.0103,  0.0523, -0.0207,  0.0360,  0.0097, -0.0547, -0.0061,\n",
      "         0.0406, -0.0423, -0.0193,  0.0417, -0.0309, -0.0193, -0.0039, -0.0168,\n",
      "        -0.0444,  0.0078, -0.0013,  0.0458,  0.0217, -0.0179,  0.0349,  0.0578,\n",
      "        -0.0228, -0.0388, -0.0059,  0.0542, -0.0270, -0.0003, -0.0353,  0.0249,\n",
      "         0.0478, -0.0053, -0.0519, -0.0241, -0.0173, -0.0248, -0.0207,  0.0095,\n",
      "         0.0462, -0.0252, -0.0262,  0.0110, -0.0176,  0.0467,  0.0110,  0.0302,\n",
      "         0.0520,  0.0561,  0.0044, -0.0354, -0.0334,  0.0280, -0.0580,  0.0201])\n",
      "Layer: layer1.1.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer1.2.conv1.bias, Biases: tensor([ 0.0063, -0.0175, -0.0415,  0.0110, -0.0025,  0.0387,  0.0397,  0.0034,\n",
      "        -0.0190,  0.0402, -0.0150, -0.0051,  0.0158, -0.0075, -0.0311,  0.0028,\n",
      "        -0.0095, -0.0290,  0.0416, -0.0009,  0.0088,  0.0308,  0.0016, -0.0386,\n",
      "        -0.0079, -0.0107, -0.0353,  0.0219,  0.0114, -0.0262,  0.0165, -0.0350])\n",
      "Layer: layer1.2.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer1.2.conv2.bias, Biases: tensor([-0.0533,  0.0588, -0.0512,  0.0094, -0.0006, -0.0227,  0.0327,  0.0348,\n",
      "        -0.0120,  0.0153, -0.0319,  0.0031, -0.0033, -0.0323,  0.0272,  0.0124,\n",
      "        -0.0323, -0.0382, -0.0471, -0.0120,  0.0174, -0.0436, -0.0152,  0.0582,\n",
      "        -0.0080, -0.0030, -0.0130,  0.0114,  0.0114,  0.0584, -0.0549, -0.0393,\n",
      "         0.0047,  0.0242,  0.0366,  0.0157,  0.0273,  0.0210,  0.0126,  0.0106,\n",
      "        -0.0115,  0.0139, -0.0249,  0.0420,  0.0024, -0.0473, -0.0307,  0.0173,\n",
      "        -0.0500, -0.0207,  0.0149,  0.0063,  0.0295,  0.0477, -0.0100,  0.0423,\n",
      "         0.0345,  0.0487, -0.0576, -0.0027, -0.0319, -0.0583,  0.0315,  0.0326])\n",
      "Layer: layer1.2.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.0.conv1.bias, Biases: tensor([ 0.0155,  0.0221, -0.0409, -0.0063,  0.0402,  0.0404, -0.0035, -0.0413,\n",
      "        -0.0388,  0.0187,  0.0285, -0.0232, -0.0143, -0.0165, -0.0351, -0.0191,\n",
      "         0.0335,  0.0026, -0.0008,  0.0171, -0.0284, -0.0035,  0.0174,  0.0334,\n",
      "         0.0161, -0.0041,  0.0234,  0.0318, -0.0285, -0.0368, -0.0240, -0.0276,\n",
      "        -0.0381,  0.0315, -0.0232,  0.0382,  0.0294, -0.0064,  0.0016,  0.0162,\n",
      "         0.0312,  0.0160, -0.0176, -0.0083, -0.0106,  0.0061, -0.0196, -0.0145,\n",
      "         0.0412, -0.0081,  0.0211,  0.0058, -0.0387, -0.0033, -0.0375, -0.0312,\n",
      "        -0.0287,  0.0003,  0.0024, -0.0021,  0.0295, -0.0386, -0.0035, -0.0332])\n",
      "Layer: layer2.0.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.0.conv2.bias, Biases: tensor([ 0.0214,  0.0402, -0.0169,  0.0243, -0.0218, -0.0274,  0.0192, -0.0011,\n",
      "         0.0204, -0.0385, -0.0161,  0.0140,  0.0341,  0.0140,  0.0076,  0.0136,\n",
      "        -0.0053,  0.0206, -0.0346, -0.0038, -0.0290,  0.0085, -0.0132, -0.0289,\n",
      "        -0.0147,  0.0320,  0.0195,  0.0280,  0.0252,  0.0376, -0.0168,  0.0174,\n",
      "        -0.0193,  0.0078,  0.0131,  0.0273, -0.0184, -0.0339, -0.0413,  0.0221,\n",
      "        -0.0109, -0.0335,  0.0062, -0.0400, -0.0059, -0.0057,  0.0253, -0.0078,\n",
      "         0.0203, -0.0303,  0.0192, -0.0231, -0.0205, -0.0151, -0.0320,  0.0135,\n",
      "        -0.0288,  0.0292,  0.0307, -0.0018, -0.0129, -0.0137, -0.0208,  0.0053,\n",
      "         0.0039, -0.0136, -0.0251,  0.0053, -0.0089,  0.0367,  0.0300,  0.0127,\n",
      "         0.0260,  0.0267,  0.0197, -0.0200, -0.0173,  0.0152, -0.0303,  0.0102,\n",
      "         0.0118,  0.0064,  0.0068,  0.0285, -0.0196,  0.0219, -0.0300, -0.0237,\n",
      "         0.0082,  0.0239, -0.0298, -0.0319, -0.0072, -0.0026, -0.0235, -0.0069,\n",
      "        -0.0359,  0.0319,  0.0065,  0.0183,  0.0301, -0.0368,  0.0286, -0.0047,\n",
      "         0.0099,  0.0370, -0.0205, -0.0384,  0.0208,  0.0216,  0.0239,  0.0108,\n",
      "        -0.0165,  0.0110,  0.0148, -0.0015,  0.0403, -0.0312, -0.0115, -0.0223,\n",
      "        -0.0172, -0.0280, -0.0019, -0.0352,  0.0235,  0.0088, -0.0203,  0.0042])\n",
      "Layer: layer2.0.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.0.identity_downsample.0.bias, Biases: tensor([-0.1131, -0.0382,  0.0616,  0.0380,  0.1079, -0.1070, -0.1026, -0.0917,\n",
      "         0.0716,  0.0611, -0.0389,  0.0396,  0.0697,  0.0747,  0.0907, -0.1225,\n",
      "        -0.1056, -0.1094,  0.0083,  0.0785,  0.1084,  0.0099,  0.0972, -0.1044,\n",
      "         0.0224, -0.0084,  0.0702, -0.0702,  0.0487, -0.0670,  0.0411, -0.1032,\n",
      "        -0.0008,  0.0687, -0.0725, -0.0318, -0.0395,  0.0526, -0.0486,  0.0089,\n",
      "         0.0066,  0.1137, -0.0384, -0.1069, -0.0856, -0.0801,  0.1209, -0.0460,\n",
      "         0.0811, -0.0552,  0.1013,  0.1055, -0.0485,  0.1060,  0.0453,  0.0425,\n",
      "         0.0633, -0.0498, -0.0889, -0.0426, -0.0506, -0.1059, -0.1247,  0.0692,\n",
      "         0.1098,  0.0906, -0.0581, -0.1130,  0.0886, -0.0435,  0.0433,  0.0239,\n",
      "        -0.0223,  0.0187, -0.0364,  0.0461, -0.1240, -0.0160, -0.0341, -0.1035,\n",
      "         0.0727, -0.0469, -0.1125, -0.0611, -0.1242, -0.0850, -0.0330,  0.1193,\n",
      "         0.0050, -0.0027, -0.0422,  0.0449,  0.0248,  0.0921,  0.1107, -0.0745,\n",
      "         0.0424,  0.0431,  0.0879,  0.0484, -0.0470,  0.0603,  0.0614, -0.0410,\n",
      "        -0.0232, -0.0256, -0.0533,  0.0508,  0.0967,  0.0895,  0.0001, -0.0229,\n",
      "         0.0670,  0.0557,  0.0089, -0.0882,  0.0875,  0.0449,  0.0031, -0.0845,\n",
      "        -0.1183, -0.0552,  0.0294,  0.0720, -0.0420, -0.0247, -0.0089,  0.1233])\n",
      "Layer: layer2.0.identity_downsample.1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.1.conv1.bias, Biases: tensor([ 0.0083,  0.0270,  0.0128, -0.0107,  0.0079,  0.0293, -0.0113,  0.0054,\n",
      "        -0.0259,  0.0079,  0.0236,  0.0109, -0.0131,  0.0030,  0.0128,  0.0209,\n",
      "         0.0088, -0.0095,  0.0198,  0.0067,  0.0130,  0.0073,  0.0176, -0.0006,\n",
      "        -0.0262,  0.0192, -0.0222,  0.0125, -0.0041, -0.0162,  0.0259,  0.0199,\n",
      "        -0.0219,  0.0018,  0.0288,  0.0248,  0.0204, -0.0004, -0.0158,  0.0130,\n",
      "        -0.0159,  0.0160,  0.0123,  0.0232,  0.0072,  0.0056, -0.0084,  0.0032,\n",
      "        -0.0105,  0.0083,  0.0089, -0.0081, -0.0241, -0.0141,  0.0120,  0.0057,\n",
      "        -0.0230,  0.0029, -0.0021,  0.0026, -0.0254, -0.0153, -0.0097, -0.0178])\n",
      "Layer: layer2.1.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.1.conv2.bias, Biases: tensor([-2.7402e-02, -2.8701e-02, -7.1458e-03,  1.7150e-02, -1.7895e-02,\n",
      "         2.1894e-02,  1.3199e-02,  1.0164e-02,  3.2119e-02,  1.4204e-02,\n",
      "        -3.8552e-02,  3.3037e-02,  1.4362e-03, -8.6687e-04,  6.4318e-03,\n",
      "        -8.8756e-03, -3.0279e-02,  4.1473e-02, -3.2564e-02, -2.8058e-02,\n",
      "        -1.5399e-03,  4.4659e-03,  2.3707e-02, -9.6472e-03, -3.9053e-02,\n",
      "         1.2131e-02, -6.3007e-03,  2.5474e-02,  8.4298e-03,  1.7180e-02,\n",
      "         3.9763e-02,  6.2213e-03,  2.5117e-02,  2.2619e-02,  2.3351e-02,\n",
      "        -6.0141e-03, -2.2042e-02, -2.7533e-02, -3.1604e-02, -3.3967e-02,\n",
      "        -2.5664e-02, -3.0472e-02,  2.1978e-02,  7.6379e-03,  1.2030e-02,\n",
      "        -2.5449e-02,  4.0974e-02,  3.3303e-02, -8.5857e-03,  2.7892e-02,\n",
      "        -3.9309e-02,  5.5265e-03, -1.0358e-02,  1.7033e-02,  1.4574e-02,\n",
      "        -2.7127e-02, -1.5209e-02,  8.2604e-03, -2.8666e-02, -3.6104e-02,\n",
      "        -6.0780e-03, -2.2477e-02, -4.1329e-03, -6.1940e-03,  1.0558e-02,\n",
      "         3.0424e-02, -3.6584e-02,  3.9446e-02, -2.4939e-03,  3.7158e-02,\n",
      "         3.1748e-02,  6.1116e-03,  3.6340e-02,  3.2801e-02, -3.9910e-02,\n",
      "        -1.5549e-02,  1.1180e-02, -7.0300e-03, -3.2492e-02,  1.5288e-02,\n",
      "         1.6081e-02,  2.3418e-02,  1.3691e-02, -9.5657e-03, -3.9085e-02,\n",
      "         2.0433e-03, -3.6073e-02,  4.1011e-02, -9.3769e-03, -2.5564e-02,\n",
      "        -2.9350e-02, -2.0500e-02, -1.4935e-02,  4.0364e-02,  2.5077e-02,\n",
      "         1.4864e-02,  2.0576e-02,  2.7961e-02,  1.5780e-02,  2.3471e-02,\n",
      "         2.9070e-03, -2.3799e-02,  7.9345e-04, -9.8144e-05, -2.5033e-02,\n",
      "         1.5488e-02,  6.3496e-03,  2.5464e-02, -1.7638e-03, -2.0237e-02,\n",
      "        -1.6639e-02,  1.9822e-02, -2.8968e-02,  4.0800e-02, -8.7613e-03,\n",
      "         2.3207e-02,  3.4511e-02,  1.7708e-02,  3.9059e-02,  2.7187e-02,\n",
      "         3.9759e-02,  3.3287e-02, -3.4237e-03, -3.8550e-02,  9.1774e-03,\n",
      "         8.5172e-03,  1.7245e-03, -4.2171e-03])\n",
      "Layer: layer2.1.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.2.conv1.bias, Biases: tensor([-0.0226,  0.0202, -0.0143,  0.0027,  0.0020,  0.0090,  0.0223,  0.0134,\n",
      "        -0.0094, -0.0148,  0.0200,  0.0152,  0.0268,  0.0285, -0.0030,  0.0134,\n",
      "        -0.0101, -0.0029,  0.0277,  0.0284,  0.0018,  0.0043,  0.0105, -0.0266,\n",
      "        -0.0210,  0.0066, -0.0109,  0.0036, -0.0101, -0.0274, -0.0116, -0.0223,\n",
      "        -0.0113, -0.0011, -0.0260, -0.0289,  0.0026, -0.0238,  0.0227, -0.0227,\n",
      "        -0.0202, -0.0202, -0.0149, -0.0101,  0.0289,  0.0176, -0.0013,  0.0019,\n",
      "         0.0070,  0.0187,  0.0209, -0.0276, -0.0215, -0.0125,  0.0086,  0.0194,\n",
      "        -0.0011, -0.0212, -0.0179, -0.0007, -0.0050,  0.0161, -0.0094, -0.0086])\n",
      "Layer: layer2.2.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.2.conv2.bias, Biases: tensor([ 0.0401, -0.0278,  0.0152, -0.0321, -0.0023, -0.0275,  0.0378,  0.0152,\n",
      "         0.0340,  0.0324, -0.0076, -0.0145,  0.0256,  0.0063, -0.0194, -0.0283,\n",
      "         0.0107,  0.0084, -0.0317, -0.0093, -0.0181, -0.0032,  0.0233, -0.0236,\n",
      "         0.0094, -0.0269,  0.0362,  0.0126, -0.0285, -0.0164, -0.0227,  0.0056,\n",
      "        -0.0174, -0.0138, -0.0170, -0.0296,  0.0280,  0.0185,  0.0200,  0.0026,\n",
      "        -0.0353, -0.0333, -0.0370, -0.0278,  0.0319,  0.0257, -0.0215, -0.0241,\n",
      "         0.0065, -0.0399,  0.0015, -0.0172,  0.0144, -0.0337,  0.0240, -0.0135,\n",
      "        -0.0312,  0.0416, -0.0363, -0.0374,  0.0356, -0.0085,  0.0139,  0.0001,\n",
      "         0.0220,  0.0142,  0.0345, -0.0082,  0.0058,  0.0289, -0.0295,  0.0024,\n",
      "         0.0044,  0.0037, -0.0128,  0.0233, -0.0126, -0.0294,  0.0369,  0.0319,\n",
      "        -0.0035, -0.0366,  0.0032,  0.0313, -0.0106, -0.0100,  0.0284, -0.0199,\n",
      "        -0.0242,  0.0242,  0.0017, -0.0332, -0.0088,  0.0218,  0.0275,  0.0362,\n",
      "         0.0309, -0.0355,  0.0228,  0.0276, -0.0266,  0.0295,  0.0402,  0.0283,\n",
      "        -0.0184, -0.0268,  0.0240,  0.0065,  0.0117, -0.0324,  0.0244, -0.0117,\n",
      "        -0.0387, -0.0402,  0.0018, -0.0238, -0.0277, -0.0063,  0.0265,  0.0024,\n",
      "         0.0057,  0.0166, -0.0129, -0.0357,  0.0111,  0.0208,  0.0346, -0.0403])\n",
      "Layer: layer2.2.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.3.conv1.bias, Biases: tensor([ 0.0139,  0.0205, -0.0269,  0.0283,  0.0144, -0.0135,  0.0073,  0.0096,\n",
      "        -0.0184, -0.0111,  0.0273, -0.0128,  0.0095,  0.0096, -0.0013,  0.0100,\n",
      "        -0.0163,  0.0116,  0.0253,  0.0008,  0.0163,  0.0176,  0.0285,  0.0041,\n",
      "        -0.0054,  0.0174,  0.0202, -0.0151,  0.0266,  0.0077, -0.0052,  0.0160,\n",
      "         0.0117, -0.0219,  0.0061, -0.0147,  0.0256, -0.0265, -0.0266,  0.0188,\n",
      "        -0.0250,  0.0152, -0.0285, -0.0006, -0.0178, -0.0230, -0.0061,  0.0047,\n",
      "         0.0210,  0.0057,  0.0004, -0.0277,  0.0078, -0.0169,  0.0081, -0.0270,\n",
      "         0.0289,  0.0034,  0.0247,  0.0148,  0.0078, -0.0292, -0.0269, -0.0162])\n",
      "Layer: layer2.3.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer2.3.conv2.bias, Biases: tensor([-0.0037,  0.0072,  0.0227, -0.0335, -0.0111,  0.0076,  0.0071, -0.0338,\n",
      "         0.0320, -0.0207, -0.0350,  0.0188, -0.0039, -0.0156, -0.0372,  0.0098,\n",
      "        -0.0191,  0.0367, -0.0258,  0.0008, -0.0181,  0.0251, -0.0385,  0.0218,\n",
      "         0.0126, -0.0143, -0.0091, -0.0263, -0.0222, -0.0210,  0.0021, -0.0143,\n",
      "        -0.0049, -0.0175, -0.0356, -0.0045, -0.0041, -0.0223, -0.0364, -0.0141,\n",
      "         0.0101,  0.0243,  0.0385,  0.0122,  0.0138,  0.0248, -0.0287, -0.0156,\n",
      "        -0.0352,  0.0221,  0.0323, -0.0271,  0.0047, -0.0274,  0.0095, -0.0368,\n",
      "         0.0074,  0.0273, -0.0371,  0.0042,  0.0319,  0.0228, -0.0329, -0.0192,\n",
      "        -0.0268,  0.0303, -0.0373, -0.0384, -0.0058,  0.0106,  0.0243,  0.0124,\n",
      "        -0.0171,  0.0218,  0.0387, -0.0032,  0.0162, -0.0309,  0.0093,  0.0084,\n",
      "         0.0054,  0.0209, -0.0379, -0.0006, -0.0171,  0.0367,  0.0371, -0.0195,\n",
      "        -0.0157,  0.0246, -0.0018,  0.0078, -0.0239,  0.0006, -0.0126,  0.0226,\n",
      "        -0.0038,  0.0395, -0.0072, -0.0212,  0.0384,  0.0278,  0.0095,  0.0074,\n",
      "        -0.0080, -0.0122, -0.0006, -0.0312,  0.0334, -0.0330,  0.0326, -0.0254,\n",
      "        -0.0108, -0.0283, -0.0104, -0.0184,  0.0111, -0.0236,  0.0165, -0.0129,\n",
      "         0.0406,  0.0317,  0.0091, -0.0083, -0.0137,  0.0374, -0.0408,  0.0157])\n",
      "Layer: layer2.3.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.0.conv1.bias, Biases: tensor([ 0.0244, -0.0034, -0.0243, -0.0115,  0.0184,  0.0098, -0.0075,  0.0006,\n",
      "        -0.0091, -0.0109,  0.0170,  0.0017,  0.0037,  0.0115,  0.0144, -0.0162,\n",
      "        -0.0258, -0.0167, -0.0083, -0.0259,  0.0140,  0.0071,  0.0197, -0.0085,\n",
      "         0.0084, -0.0262,  0.0068, -0.0174, -0.0003, -0.0191, -0.0184, -0.0287,\n",
      "        -0.0268,  0.0054, -0.0214,  0.0081, -0.0234,  0.0160,  0.0187, -0.0269,\n",
      "         0.0264,  0.0035,  0.0210,  0.0005,  0.0051,  0.0014,  0.0250, -0.0110,\n",
      "         0.0037,  0.0157,  0.0287,  0.0273,  0.0163,  0.0275,  0.0041,  0.0063,\n",
      "        -0.0225, -0.0166, -0.0172, -0.0151, -0.0195, -0.0129, -0.0189,  0.0285,\n",
      "        -0.0213,  0.0262, -0.0083,  0.0268, -0.0287, -0.0046,  0.0246,  0.0274,\n",
      "        -0.0287,  0.0072, -0.0121,  0.0245, -0.0270, -0.0261, -0.0198,  0.0234,\n",
      "         0.0164, -0.0275,  0.0172, -0.0115, -0.0076,  0.0156,  0.0044, -0.0239,\n",
      "        -0.0278, -0.0098,  0.0207,  0.0242, -0.0145,  0.0037, -0.0220, -0.0189,\n",
      "        -0.0078,  0.0091,  0.0272, -0.0032, -0.0183,  0.0146, -0.0253,  0.0148,\n",
      "         0.0234,  0.0263, -0.0228, -0.0070,  0.0233,  0.0096, -0.0167,  0.0044,\n",
      "         0.0041, -0.0099,  0.0133,  0.0011,  0.0234, -0.0019,  0.0241, -0.0133,\n",
      "        -0.0224,  0.0036, -0.0269,  0.0090, -0.0229, -0.0234,  0.0020, -0.0054])\n",
      "Layer: layer3.0.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.0.conv2.bias, Biases: tensor([ 1.9217e-02, -1.2952e-02,  6.4348e-04,  8.0655e-03, -1.3457e-02,\n",
      "         2.1581e-02, -1.3174e-02, -2.6985e-03, -6.6829e-03, -1.4424e-03,\n",
      "        -2.5283e-02, -8.1989e-03, -1.4486e-02, -2.1362e-02, -1.4849e-02,\n",
      "        -6.8669e-03, -1.9572e-02,  7.8376e-03,  1.6778e-02, -7.8095e-03,\n",
      "        -1.8065e-02, -2.1958e-02,  2.6917e-02,  8.7550e-04,  2.6927e-02,\n",
      "         1.2681e-02, -9.2622e-03, -2.1977e-03, -1.9089e-02, -9.0176e-03,\n",
      "        -1.6459e-02,  2.7840e-02,  1.3590e-02, -1.4744e-02,  2.7633e-02,\n",
      "        -1.6788e-02,  2.2660e-02,  6.1549e-03, -2.4703e-02,  2.5910e-02,\n",
      "        -2.6504e-02,  2.6362e-03, -1.6271e-02, -2.2910e-02, -1.4423e-02,\n",
      "        -1.9651e-02, -2.3816e-02,  1.5660e-02,  2.8761e-02,  1.9884e-02,\n",
      "        -3.8235e-03, -2.0849e-02,  2.8569e-02, -4.9626e-03, -1.8407e-02,\n",
      "        -6.3223e-04, -1.3280e-02,  6.6046e-03, -1.6764e-02, -2.5523e-02,\n",
      "         2.3858e-02,  1.2951e-02, -1.9977e-03, -1.5614e-02, -1.6340e-02,\n",
      "         2.7536e-02, -1.0744e-02,  1.7502e-02,  3.6566e-03,  1.7103e-02,\n",
      "         1.0009e-02,  8.7332e-03,  3.0518e-03,  8.2746e-03,  2.4154e-02,\n",
      "        -2.4777e-02,  1.9506e-02,  1.4876e-02, -5.8033e-05,  2.0781e-02,\n",
      "        -3.8245e-03,  1.7788e-02, -7.3896e-03, -2.4749e-02, -2.0189e-02,\n",
      "         2.2940e-02, -6.7330e-03, -2.6732e-02,  1.2933e-02,  1.9955e-02,\n",
      "        -5.7996e-03, -1.2584e-02, -4.2228e-03, -1.7485e-02,  1.6158e-02,\n",
      "        -1.1305e-02, -2.6280e-02,  2.7063e-02,  2.8184e-02,  1.5073e-02,\n",
      "         1.0625e-03, -2.0408e-02,  1.4364e-02, -2.4512e-02, -2.3156e-02,\n",
      "         6.5199e-03,  7.4635e-03,  1.7964e-02,  1.3142e-02,  2.8740e-02,\n",
      "        -7.3732e-04,  1.2338e-02, -1.6213e-02, -2.6042e-02,  2.2052e-02,\n",
      "         3.1544e-03,  7.9855e-03, -1.3899e-03, -1.6060e-02,  2.9431e-02,\n",
      "         2.2176e-02,  1.4870e-02, -1.7797e-02,  1.2433e-03,  2.9071e-02,\n",
      "        -4.5046e-03, -2.3568e-02,  2.2444e-02, -2.2293e-02, -1.7914e-03,\n",
      "        -6.5618e-03,  1.1628e-03, -1.7988e-02,  2.1342e-02,  1.4648e-02,\n",
      "         1.3320e-03,  7.2057e-03,  9.0131e-05, -1.0864e-02, -2.4661e-02,\n",
      "        -2.9266e-02,  1.9619e-02,  1.0112e-02,  9.2686e-03, -4.1141e-03,\n",
      "        -7.6994e-03, -2.1292e-02, -2.6317e-02,  2.0611e-03,  1.6970e-02,\n",
      "         1.0590e-02, -1.0262e-02, -2.0218e-02,  2.4084e-02,  6.2313e-03,\n",
      "        -4.7803e-03,  1.6677e-02, -2.8904e-02, -1.8564e-02,  1.2840e-02,\n",
      "        -1.2806e-02,  1.9955e-02,  6.2593e-03, -1.4482e-03, -4.3429e-03,\n",
      "         2.0089e-02,  1.3173e-02, -1.8683e-02,  1.4023e-02, -1.2482e-02,\n",
      "         2.0886e-02,  2.6313e-02,  2.6190e-02,  1.9663e-02, -1.5168e-02,\n",
      "         1.6448e-02,  1.6473e-02,  8.2779e-03,  1.9856e-02,  2.1652e-02,\n",
      "        -2.7343e-02,  2.6186e-02,  7.2435e-03,  7.6838e-04,  2.7045e-02,\n",
      "        -1.4026e-02,  2.0281e-02,  2.7137e-02, -1.8776e-02, -1.7910e-03,\n",
      "         2.5496e-03,  5.9284e-03,  1.4682e-02,  1.7080e-02,  3.5507e-03,\n",
      "         1.9755e-02, -2.2026e-02,  2.0101e-03, -2.9030e-02,  2.3887e-02,\n",
      "         2.5307e-02, -1.8222e-02, -1.7897e-04,  2.1504e-02,  2.9235e-02,\n",
      "        -2.1406e-02,  1.0870e-02,  2.8254e-02,  2.3991e-02,  2.0570e-02,\n",
      "         2.0347e-02, -1.8731e-02, -2.0207e-02, -1.9834e-02, -1.9532e-02,\n",
      "        -5.4675e-05,  2.2173e-02,  5.4499e-03,  2.6013e-02,  1.7608e-03,\n",
      "         2.7652e-02,  1.9449e-02,  1.9916e-02, -1.6369e-02, -2.6686e-02,\n",
      "         3.6879e-03,  6.1294e-03, -7.1309e-03, -1.2414e-02,  7.0200e-03,\n",
      "        -1.3814e-02, -8.6097e-03,  1.1109e-02, -2.8441e-02, -9.9081e-03,\n",
      "         2.5732e-02, -2.8650e-02, -2.3117e-02,  4.9077e-04, -9.7225e-03,\n",
      "        -2.6266e-03,  2.3549e-02, -1.1260e-02, -1.3719e-02,  2.8835e-02,\n",
      "        -2.2360e-02,  4.3215e-03,  1.2406e-02, -2.8769e-02,  8.7059e-03,\n",
      "        -2.0980e-02, -2.2414e-02,  2.0009e-02,  2.1428e-02, -1.5902e-02,\n",
      "         2.1021e-02])\n",
      "Layer: layer3.0.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.0.identity_downsample.0.bias, Biases: tensor([ 0.0585,  0.0350, -0.0115, -0.0048, -0.0155,  0.0181, -0.0163,  0.0436,\n",
      "        -0.0198, -0.0639, -0.0647, -0.0132,  0.0047, -0.0514, -0.0211,  0.0091,\n",
      "        -0.0053,  0.0814, -0.0436,  0.0870, -0.0380, -0.0808,  0.0600, -0.0232,\n",
      "         0.0872, -0.0048,  0.0185,  0.0136, -0.0472, -0.0533, -0.0789, -0.0500,\n",
      "        -0.0114, -0.0755,  0.0845,  0.0845, -0.0491,  0.0155,  0.0418, -0.0605,\n",
      "        -0.0720, -0.0752,  0.0261,  0.0796,  0.0781, -0.0114,  0.0065,  0.0834,\n",
      "         0.0520,  0.0024,  0.0604,  0.0343,  0.0290, -0.0793,  0.0620, -0.0413,\n",
      "         0.0123, -0.0248, -0.0684,  0.0407, -0.0299,  0.0329, -0.0256,  0.0806,\n",
      "        -0.0028,  0.0875, -0.0347,  0.0837,  0.0088,  0.0833, -0.0421,  0.0517,\n",
      "         0.0297,  0.0845, -0.0040,  0.0491,  0.0014,  0.0444,  0.0571, -0.0687,\n",
      "        -0.0506, -0.0273,  0.0477, -0.0833,  0.0207,  0.0059,  0.0158,  0.0687,\n",
      "        -0.0357,  0.0398, -0.0102, -0.0447, -0.0574, -0.0102, -0.0136, -0.0244,\n",
      "        -0.0448,  0.0405, -0.0592, -0.0307, -0.0550, -0.0794,  0.0730,  0.0599,\n",
      "         0.0447,  0.0695, -0.0661, -0.0871,  0.0617, -0.0537,  0.0264,  0.0707,\n",
      "         0.0019,  0.0533,  0.0549,  0.0569,  0.0817,  0.0059,  0.0239,  0.0130,\n",
      "        -0.0725,  0.0219,  0.0841, -0.0205, -0.0583, -0.0035, -0.0249, -0.0030,\n",
      "         0.0136, -0.0593,  0.0056, -0.0872, -0.0515, -0.0028, -0.0114, -0.0136,\n",
      "         0.0725,  0.0589, -0.0403, -0.0389,  0.0423,  0.0649, -0.0106,  0.0257,\n",
      "        -0.0464, -0.0407, -0.0587, -0.0594, -0.0658,  0.0010, -0.0500,  0.0349,\n",
      "        -0.0474,  0.0053,  0.0097,  0.0421, -0.0803, -0.0194,  0.0501,  0.0092,\n",
      "        -0.0744, -0.0409,  0.0478,  0.0468,  0.0624, -0.0209, -0.0820, -0.0210,\n",
      "        -0.0574, -0.0463, -0.0772, -0.0814,  0.0340, -0.0803, -0.0682, -0.0588,\n",
      "        -0.0363, -0.0158,  0.0026,  0.0153,  0.0710, -0.0110, -0.0057, -0.0676,\n",
      "        -0.0526,  0.0499, -0.0403,  0.0840, -0.0244, -0.0611,  0.0318,  0.0870,\n",
      "         0.0427, -0.0756,  0.0659, -0.0827, -0.0823, -0.0214,  0.0007, -0.0194,\n",
      "         0.0101, -0.0323,  0.0547, -0.0070, -0.0725, -0.0175,  0.0063,  0.0155,\n",
      "        -0.0531,  0.0862,  0.0594, -0.0318,  0.0416,  0.0148,  0.0847,  0.0510,\n",
      "         0.0288,  0.0590,  0.0712,  0.0420,  0.0353, -0.0606,  0.0407, -0.0179,\n",
      "         0.0566,  0.0862,  0.0656,  0.0146,  0.0505,  0.0810,  0.0467, -0.0117,\n",
      "        -0.0733, -0.0587, -0.0313, -0.0848, -0.0723,  0.0851,  0.0646, -0.0872,\n",
      "        -0.0878, -0.0539, -0.0513, -0.0324,  0.0001, -0.0196,  0.0555, -0.0289,\n",
      "         0.0016,  0.0256, -0.0113,  0.0163, -0.0882,  0.0368, -0.0475,  0.0100])\n",
      "Layer: layer3.0.identity_downsample.1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.1.conv1.bias, Biases: tensor([-0.0012,  0.0020, -0.0003, -0.0153,  0.0149, -0.0157, -0.0191, -0.0101,\n",
      "        -0.0151,  0.0132,  0.0106,  0.0011, -0.0072, -0.0072,  0.0046, -0.0100,\n",
      "        -0.0179,  0.0122, -0.0076,  0.0036, -0.0064, -0.0150,  0.0188,  0.0095,\n",
      "         0.0029,  0.0005,  0.0111, -0.0015,  0.0052, -0.0034,  0.0182,  0.0010,\n",
      "         0.0097, -0.0178, -0.0110,  0.0079,  0.0116, -0.0113,  0.0041, -0.0173,\n",
      "         0.0058, -0.0074, -0.0081,  0.0074, -0.0008, -0.0032,  0.0197, -0.0186,\n",
      "        -0.0064,  0.0050,  0.0007, -0.0205,  0.0081,  0.0067,  0.0019,  0.0015,\n",
      "         0.0031, -0.0042, -0.0073, -0.0082,  0.0119, -0.0202, -0.0039,  0.0067,\n",
      "        -0.0192,  0.0178,  0.0109,  0.0197, -0.0088, -0.0036,  0.0087,  0.0157,\n",
      "         0.0094, -0.0155,  0.0171,  0.0127,  0.0138, -0.0063,  0.0049, -0.0050,\n",
      "         0.0067,  0.0114,  0.0030,  0.0183,  0.0045, -0.0148,  0.0114, -0.0154,\n",
      "         0.0029, -0.0112, -0.0154, -0.0139,  0.0127,  0.0130, -0.0095,  0.0036,\n",
      "         0.0086, -0.0167, -0.0020,  0.0056,  0.0140,  0.0088, -0.0088, -0.0119,\n",
      "        -0.0204, -0.0189, -0.0141,  0.0088,  0.0103, -0.0110,  0.0075, -0.0166,\n",
      "         0.0190,  0.0124,  0.0110,  0.0157, -0.0096,  0.0031,  0.0177, -0.0136,\n",
      "        -0.0194, -0.0043,  0.0126,  0.0117,  0.0112,  0.0084,  0.0098,  0.0011])\n",
      "Layer: layer3.1.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.1.conv2.bias, Biases: tensor([-0.0033, -0.0018,  0.0060,  0.0017,  0.0134, -0.0220, -0.0190, -0.0144,\n",
      "        -0.0113, -0.0263,  0.0209,  0.0219,  0.0055,  0.0221,  0.0171,  0.0273,\n",
      "        -0.0009, -0.0121, -0.0098,  0.0257,  0.0014, -0.0151, -0.0067, -0.0174,\n",
      "         0.0256, -0.0159,  0.0217, -0.0108, -0.0021,  0.0117, -0.0239,  0.0089,\n",
      "         0.0265, -0.0101,  0.0005, -0.0075, -0.0007,  0.0085, -0.0058,  0.0144,\n",
      "        -0.0240,  0.0288,  0.0025, -0.0140,  0.0152,  0.0157, -0.0293,  0.0166,\n",
      "         0.0262, -0.0179,  0.0219, -0.0124,  0.0103,  0.0207, -0.0218, -0.0258,\n",
      "        -0.0121, -0.0054, -0.0214, -0.0017,  0.0242, -0.0124,  0.0249, -0.0282,\n",
      "        -0.0265, -0.0246, -0.0148,  0.0271, -0.0105, -0.0118,  0.0085, -0.0036,\n",
      "         0.0027,  0.0218, -0.0041,  0.0018, -0.0230,  0.0258,  0.0109, -0.0044,\n",
      "         0.0143, -0.0122, -0.0078,  0.0092, -0.0177, -0.0217, -0.0170, -0.0119,\n",
      "         0.0287, -0.0226, -0.0002,  0.0280,  0.0020,  0.0052,  0.0073, -0.0072,\n",
      "        -0.0102,  0.0224,  0.0203,  0.0004, -0.0209, -0.0295, -0.0055, -0.0050,\n",
      "         0.0117,  0.0284,  0.0062, -0.0165, -0.0101, -0.0120, -0.0280,  0.0040,\n",
      "        -0.0165,  0.0279,  0.0265, -0.0075,  0.0229,  0.0240,  0.0207,  0.0033,\n",
      "         0.0250, -0.0289,  0.0165,  0.0258, -0.0006, -0.0100, -0.0027,  0.0146,\n",
      "        -0.0153,  0.0168,  0.0052, -0.0187, -0.0110, -0.0165, -0.0101,  0.0229,\n",
      "         0.0274,  0.0162, -0.0030, -0.0259, -0.0108,  0.0077,  0.0253,  0.0220,\n",
      "        -0.0041,  0.0163, -0.0122, -0.0224,  0.0283,  0.0015, -0.0220,  0.0202,\n",
      "        -0.0004, -0.0286,  0.0085, -0.0100,  0.0188, -0.0039,  0.0002, -0.0151,\n",
      "         0.0163, -0.0266, -0.0268,  0.0022,  0.0056, -0.0115, -0.0114, -0.0287,\n",
      "         0.0288, -0.0270,  0.0051,  0.0115, -0.0058,  0.0085,  0.0265, -0.0193,\n",
      "        -0.0131, -0.0068,  0.0102, -0.0004,  0.0049, -0.0193, -0.0192, -0.0105,\n",
      "         0.0123,  0.0142,  0.0276,  0.0274,  0.0199, -0.0178, -0.0278,  0.0101,\n",
      "        -0.0281,  0.0256,  0.0081, -0.0183,  0.0218,  0.0204, -0.0255, -0.0085,\n",
      "        -0.0161, -0.0186, -0.0271, -0.0277,  0.0097, -0.0224, -0.0279, -0.0052,\n",
      "         0.0276,  0.0288,  0.0004,  0.0262,  0.0045, -0.0227, -0.0050, -0.0246,\n",
      "         0.0223, -0.0209,  0.0242,  0.0075, -0.0144,  0.0278, -0.0199, -0.0062,\n",
      "         0.0151, -0.0061, -0.0105,  0.0270, -0.0026,  0.0026, -0.0033, -0.0167,\n",
      "        -0.0234, -0.0139, -0.0139, -0.0291,  0.0004, -0.0203, -0.0235,  0.0050,\n",
      "         0.0060, -0.0175,  0.0097,  0.0239,  0.0283,  0.0080, -0.0045, -0.0078,\n",
      "         0.0121,  0.0102, -0.0181,  0.0294,  0.0051, -0.0014, -0.0104, -0.0020])\n",
      "Layer: layer3.1.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.2.conv1.bias, Biases: tensor([ 0.0063, -0.0142, -0.0088,  0.0059,  0.0146, -0.0172,  0.0208,  0.0090,\n",
      "         0.0058, -0.0123,  0.0098,  0.0185,  0.0169, -0.0175,  0.0054, -0.0171,\n",
      "        -0.0061, -0.0116,  0.0207,  0.0160, -0.0008, -0.0097,  0.0179, -0.0193,\n",
      "        -0.0161, -0.0010,  0.0139, -0.0203, -0.0129,  0.0038,  0.0185, -0.0090,\n",
      "         0.0149,  0.0091, -0.0066, -0.0006,  0.0008,  0.0007, -0.0145, -0.0172,\n",
      "        -0.0204, -0.0080,  0.0134, -0.0171,  0.0124, -0.0189, -0.0058,  0.0025,\n",
      "         0.0158,  0.0053, -0.0043,  0.0065, -0.0087, -0.0180, -0.0107,  0.0200,\n",
      "         0.0110,  0.0139,  0.0043,  0.0206,  0.0075,  0.0101, -0.0180,  0.0029,\n",
      "         0.0121, -0.0166, -0.0020, -0.0060,  0.0156,  0.0038, -0.0156, -0.0188,\n",
      "         0.0136,  0.0204, -0.0176, -0.0115, -0.0172, -0.0179,  0.0040,  0.0136,\n",
      "        -0.0100, -0.0007,  0.0083,  0.0187, -0.0127, -0.0013, -0.0028, -0.0202,\n",
      "         0.0152, -0.0085,  0.0107, -0.0033, -0.0033, -0.0022,  0.0122, -0.0075,\n",
      "         0.0128,  0.0155,  0.0058,  0.0164,  0.0032,  0.0141, -0.0001, -0.0154,\n",
      "        -0.0008, -0.0075,  0.0001, -0.0185, -0.0009,  0.0153,  0.0196, -0.0135,\n",
      "        -0.0106, -0.0151, -0.0076,  0.0070,  0.0119, -0.0058,  0.0185, -0.0195,\n",
      "        -0.0019, -0.0044, -0.0168,  0.0139,  0.0074,  0.0175,  0.0101,  0.0008])\n",
      "Layer: layer3.2.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.2.conv2.bias, Biases: tensor([-1.8489e-03,  8.3289e-03,  1.3770e-02, -2.5661e-02,  2.8029e-02,\n",
      "         1.3477e-02,  1.4729e-02, -2.8907e-02,  2.6529e-02,  6.1265e-03,\n",
      "         8.2403e-04, -2.9858e-04, -6.1158e-03,  1.7627e-02,  9.9658e-03,\n",
      "        -1.6321e-02,  2.4232e-02, -2.2133e-02, -1.1298e-02, -2.7753e-02,\n",
      "         2.5365e-02,  4.0102e-03, -7.1178e-03,  1.9279e-02,  7.8858e-03,\n",
      "         2.0888e-02,  2.0664e-02, -6.4864e-03, -1.3769e-02, -9.3945e-04,\n",
      "        -5.7553e-03,  2.0156e-02,  2.6558e-02,  2.8230e-02,  1.0059e-02,\n",
      "        -1.0448e-02,  2.2242e-02, -7.9468e-03, -3.5168e-03,  1.3787e-02,\n",
      "        -2.2882e-05,  8.2362e-03, -1.9494e-02, -1.4736e-02, -2.5144e-02,\n",
      "        -2.6387e-02,  1.5399e-02,  2.1363e-02,  1.5117e-02,  6.0031e-03,\n",
      "        -1.5084e-02,  7.7442e-03, -1.4743e-02,  1.0004e-02, -5.7136e-03,\n",
      "        -1.2219e-03, -9.8095e-03,  1.7645e-02,  5.4840e-03,  2.7961e-02,\n",
      "        -1.8513e-03, -1.7405e-02, -2.5968e-02, -1.8084e-02,  8.2368e-03,\n",
      "         2.5872e-02,  2.1292e-02,  1.4811e-02,  1.4979e-02,  1.9004e-02,\n",
      "        -7.0486e-03,  2.5989e-02,  7.6762e-03, -4.8754e-03,  4.1097e-03,\n",
      "        -8.0489e-03,  2.2102e-02, -6.4605e-03, -2.5794e-02,  8.4313e-03,\n",
      "        -1.5375e-02, -2.9238e-02, -1.5671e-02,  2.2440e-02, -4.0199e-03,\n",
      "         2.7478e-02, -4.0874e-03,  2.3216e-03,  2.0723e-03,  2.9018e-02,\n",
      "        -2.0919e-03, -2.0802e-02, -4.4550e-03, -2.6224e-02, -7.0260e-03,\n",
      "        -3.7325e-03, -1.7374e-02, -6.2183e-03, -1.9805e-02, -2.1355e-02,\n",
      "         1.7103e-03, -2.1406e-02,  2.4094e-03,  1.8228e-02, -3.3027e-03,\n",
      "        -1.8558e-02, -2.8752e-02,  1.5215e-02,  1.7580e-02, -1.1705e-02,\n",
      "        -1.1683e-02, -2.1773e-02, -2.7174e-03, -9.8019e-03, -2.2848e-02,\n",
      "         2.8043e-02,  1.4435e-02,  2.3943e-02, -4.7200e-03,  2.1813e-03,\n",
      "         8.1392e-03,  7.2298e-04,  1.9096e-02,  2.2857e-02, -9.5660e-03,\n",
      "         3.1541e-03,  2.7485e-02,  1.0515e-02,  2.0035e-02, -2.2413e-02,\n",
      "         1.8967e-02,  1.3619e-02, -1.7120e-02, -2.8670e-02,  1.6296e-02,\n",
      "        -3.1174e-03, -1.1046e-02,  8.2844e-03, -2.1382e-02,  2.8822e-02,\n",
      "         2.5264e-02, -1.8261e-02,  1.8926e-02,  2.2553e-02,  5.4060e-03,\n",
      "         2.4865e-02,  1.0691e-02, -1.0704e-02,  7.5696e-03,  1.1880e-02,\n",
      "        -3.8151e-03, -1.8807e-02,  2.8204e-02, -1.6149e-02,  1.5395e-02,\n",
      "        -1.1949e-02,  1.5437e-02,  1.8619e-02,  1.9398e-02,  1.1125e-02,\n",
      "        -2.6271e-02, -2.4747e-04, -1.6115e-02,  2.6085e-02, -1.2488e-04,\n",
      "         2.6610e-02,  5.8859e-03, -2.7755e-02, -6.1050e-03, -2.3927e-03,\n",
      "        -5.0576e-03,  2.1743e-02, -7.8943e-03, -2.3034e-02, -1.4646e-02,\n",
      "         1.0279e-02, -2.0387e-02, -6.5540e-03,  9.4973e-03,  1.2647e-02,\n",
      "         1.7252e-02,  1.9507e-05,  2.5382e-02,  2.3675e-02,  1.0666e-02,\n",
      "        -7.7628e-03, -1.1144e-03, -6.4877e-03, -2.5056e-02, -6.6541e-03,\n",
      "         1.5307e-03,  2.9031e-02,  2.8460e-02, -2.3570e-02,  2.7016e-02,\n",
      "        -1.2003e-02, -2.8246e-02,  2.5732e-02, -9.1369e-03, -2.8314e-02,\n",
      "        -1.9650e-02,  1.4143e-02, -1.6415e-03,  2.3710e-02,  1.3214e-02,\n",
      "         2.5191e-02, -2.1721e-02, -1.8879e-02,  1.0472e-02,  1.2356e-02,\n",
      "         1.9835e-02, -2.2473e-02,  9.4357e-03, -2.1480e-02, -7.0338e-03,\n",
      "         1.3991e-02,  2.4208e-02,  2.6652e-03,  5.9952e-03,  1.5626e-02,\n",
      "        -6.0489e-03,  2.6639e-02, -1.2808e-02,  2.1433e-04, -2.1462e-02,\n",
      "         1.1200e-02, -1.3678e-02, -1.3049e-02,  9.2765e-03, -5.6076e-03,\n",
      "         2.1714e-02, -1.6158e-02,  1.7349e-03, -4.1397e-03, -1.6410e-02,\n",
      "        -2.4828e-02,  1.1146e-03,  9.2454e-03,  4.8945e-03,  2.5547e-02,\n",
      "        -4.2373e-03, -1.8942e-02, -2.4033e-02, -2.7259e-03, -2.0040e-02,\n",
      "         1.8305e-02, -6.7265e-03, -8.8825e-03, -2.7135e-02,  2.4401e-02,\n",
      "         2.6840e-02,  2.1609e-03,  1.0158e-02, -1.9271e-02,  8.2183e-03,\n",
      "         4.3217e-03])\n",
      "Layer: layer3.2.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.3.conv1.bias, Biases: tensor([-0.0070,  0.0132, -0.0035, -0.0198,  0.0006,  0.0020,  0.0200, -0.0046,\n",
      "        -0.0178, -0.0140,  0.0031, -0.0124,  0.0198, -0.0050, -0.0078,  0.0199,\n",
      "         0.0171, -0.0176, -0.0096, -0.0148, -0.0154,  0.0114,  0.0157,  0.0160,\n",
      "         0.0067,  0.0191, -0.0039,  0.0116, -0.0020,  0.0085, -0.0191, -0.0136,\n",
      "         0.0202, -0.0099, -0.0095,  0.0188, -0.0143,  0.0046,  0.0052, -0.0022,\n",
      "        -0.0008, -0.0132, -0.0034,  0.0008,  0.0122,  0.0125, -0.0016,  0.0007,\n",
      "         0.0121,  0.0053, -0.0115,  0.0039, -0.0186,  0.0152,  0.0001,  0.0191,\n",
      "         0.0054, -0.0059, -0.0054, -0.0168, -0.0192,  0.0043,  0.0123, -0.0125,\n",
      "         0.0182,  0.0017, -0.0058, -0.0140, -0.0094, -0.0050, -0.0183,  0.0155,\n",
      "         0.0201,  0.0191,  0.0084, -0.0008, -0.0141,  0.0085, -0.0041,  0.0051,\n",
      "        -0.0081,  0.0002, -0.0021, -0.0050, -0.0043,  0.0007,  0.0055, -0.0136,\n",
      "        -0.0078,  0.0176,  0.0005,  0.0182,  0.0030,  0.0119,  0.0024,  0.0112,\n",
      "        -0.0032, -0.0145, -0.0161,  0.0131,  0.0112,  0.0166, -0.0101, -0.0198,\n",
      "         0.0192, -0.0093, -0.0147, -0.0153,  0.0114,  0.0182,  0.0137, -0.0035,\n",
      "         0.0051, -0.0196, -0.0188,  0.0148, -0.0051,  0.0031,  0.0159,  0.0192,\n",
      "        -0.0115, -0.0001,  0.0145,  0.0117, -0.0176, -0.0122,  0.0123,  0.0142])\n",
      "Layer: layer3.3.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.3.conv2.bias, Biases: tensor([-2.7354e-03, -4.2174e-03,  7.9183e-03,  2.5703e-02, -2.0967e-03,\n",
      "        -2.3633e-02,  2.0961e-02,  2.6765e-02, -7.4346e-03, -1.8476e-04,\n",
      "         2.2642e-02,  2.1319e-02, -1.0629e-02, -5.3697e-03, -4.1364e-03,\n",
      "         1.1091e-02, -2.5857e-02, -2.7536e-02,  2.3938e-02,  2.4116e-02,\n",
      "        -5.4529e-03, -1.2432e-02,  1.0193e-02, -1.4404e-02, -2.9058e-03,\n",
      "        -2.3085e-02, -5.1626e-03, -2.8224e-02,  2.3402e-02,  1.6822e-03,\n",
      "         2.9821e-03, -7.6848e-03,  2.4564e-02,  2.4308e-02,  2.3903e-02,\n",
      "         6.4862e-03, -9.0059e-04, -2.8426e-03,  2.2286e-02,  2.4632e-02,\n",
      "         2.1740e-02, -5.6710e-03,  1.6905e-02,  6.8242e-04,  2.3572e-03,\n",
      "         2.8267e-02,  2.0694e-02,  2.9353e-02, -5.8759e-03, -2.2264e-03,\n",
      "        -1.2173e-03, -2.0970e-02,  1.8516e-02, -1.1540e-02,  4.6296e-03,\n",
      "        -1.7172e-02,  1.7455e-02, -2.6123e-02, -5.4077e-03, -8.9614e-03,\n",
      "         1.7720e-02, -4.9175e-03,  2.9094e-02, -9.8950e-03, -4.6723e-03,\n",
      "         2.4302e-02,  8.8695e-04, -1.6526e-02,  1.2572e-02,  8.8181e-03,\n",
      "         1.2400e-02,  1.2726e-02,  1.1309e-02,  2.0482e-02,  1.3949e-03,\n",
      "         1.7433e-02,  2.8430e-02, -9.8461e-03,  2.0207e-02,  1.9377e-02,\n",
      "        -3.0806e-03,  1.4498e-02, -5.2316e-03,  1.2201e-02, -1.9954e-03,\n",
      "        -4.4635e-04,  1.5115e-02, -2.3641e-02,  4.5105e-03,  2.0148e-02,\n",
      "        -1.9003e-02, -2.7716e-02, -1.8619e-02, -2.7900e-03, -1.9616e-02,\n",
      "        -2.1890e-02,  1.3086e-02,  6.1766e-03, -1.5928e-02, -2.6975e-02,\n",
      "         9.9666e-03, -3.0202e-03,  9.9564e-03, -2.2384e-02, -1.0682e-02,\n",
      "         2.8875e-02, -8.4323e-03,  2.0530e-03,  4.9630e-03,  9.9219e-03,\n",
      "         5.9550e-03, -1.8177e-03, -1.6625e-02,  1.9632e-04, -2.7388e-02,\n",
      "         1.3976e-02,  1.4269e-02,  1.2343e-02,  2.1495e-02, -1.5608e-02,\n",
      "        -1.9856e-02, -1.6213e-02,  2.6120e-02,  1.2114e-02,  5.9336e-03,\n",
      "         9.1145e-03,  1.8139e-02,  2.3547e-03,  1.6971e-02, -2.2413e-02,\n",
      "         4.6644e-03, -2.6408e-03, -2.7449e-02,  2.3044e-02, -5.4446e-03,\n",
      "        -5.7732e-03,  5.5906e-03, -1.4898e-02,  9.5419e-03,  7.8637e-04,\n",
      "        -1.3895e-02, -1.6651e-02, -2.6464e-03,  2.1922e-02,  2.0776e-02,\n",
      "        -1.9699e-02, -1.9392e-02, -1.0143e-02, -2.2892e-02,  2.2308e-02,\n",
      "        -2.4630e-02,  5.5225e-03,  8.4559e-03, -6.8636e-03,  2.9242e-02,\n",
      "        -3.5635e-03,  2.7998e-02, -1.1872e-04,  2.1220e-02,  2.9110e-02,\n",
      "        -1.3965e-02, -8.7880e-05,  1.7132e-02, -2.2901e-02,  1.2582e-02,\n",
      "        -1.6309e-02, -1.5670e-02, -1.8794e-02, -7.0875e-03, -1.1599e-02,\n",
      "         2.3625e-02, -7.4163e-03, -2.2712e-03, -9.3598e-03,  2.4704e-02,\n",
      "        -1.9275e-02,  9.4633e-03, -3.0419e-03, -7.3000e-03,  3.4429e-04,\n",
      "        -2.3935e-02, -6.6705e-03,  2.0456e-03, -2.5124e-02,  4.1741e-04,\n",
      "         2.8386e-02, -2.0940e-02, -1.7405e-02, -2.2308e-02, -2.1355e-03,\n",
      "         5.5934e-03, -9.8659e-03,  6.3629e-03,  2.2984e-02, -1.8412e-02,\n",
      "        -1.0484e-02,  1.3510e-02,  8.9754e-03,  9.5765e-03, -1.6449e-02,\n",
      "         2.8784e-02, -1.6723e-02,  1.4953e-03, -4.7030e-03, -4.4704e-03,\n",
      "         1.4961e-02,  2.8467e-02, -1.8319e-03,  8.2325e-03, -2.7813e-02,\n",
      "         2.9444e-03, -2.9058e-02,  7.4087e-03,  1.4807e-02,  1.2581e-02,\n",
      "        -7.8193e-03, -2.0883e-02,  1.4021e-02, -9.6066e-03, -1.3131e-02,\n",
      "        -2.0382e-02,  2.3240e-02, -1.2124e-02,  2.1959e-02, -1.4407e-02,\n",
      "         2.2019e-02,  1.7974e-02, -1.3969e-02, -1.8820e-03,  7.0736e-03,\n",
      "        -7.5145e-03,  1.4475e-02, -1.9817e-02,  1.6552e-03, -1.1523e-03,\n",
      "         2.2392e-02,  8.1441e-03,  2.2189e-02, -6.7894e-03,  2.5230e-02,\n",
      "        -2.3812e-02, -2.3333e-02,  1.5639e-02,  2.4739e-02,  6.9350e-03,\n",
      "         1.6345e-02, -2.8391e-02,  2.5532e-02,  2.2870e-02,  1.3270e-02,\n",
      "        -1.8362e-02, -8.2886e-03, -9.3726e-03, -1.4750e-03,  2.3966e-02,\n",
      "         1.5015e-02])\n",
      "Layer: layer3.3.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.4.conv1.bias, Biases: tensor([ 0.0081,  0.0043, -0.0046, -0.0117,  0.0131, -0.0023, -0.0031, -0.0027,\n",
      "         0.0032,  0.0110, -0.0057,  0.0115, -0.0056,  0.0077,  0.0062,  0.0072,\n",
      "        -0.0191,  0.0122,  0.0023, -0.0097, -0.0167, -0.0143, -0.0095, -0.0161,\n",
      "        -0.0163,  0.0170,  0.0193, -0.0009, -0.0122,  0.0056,  0.0171, -0.0144,\n",
      "         0.0061,  0.0149, -0.0152,  0.0130,  0.0174,  0.0103,  0.0165,  0.0043,\n",
      "        -0.0088,  0.0096,  0.0176,  0.0041,  0.0012, -0.0113, -0.0141,  0.0111,\n",
      "        -0.0092, -0.0172, -0.0186, -0.0138, -0.0193,  0.0093,  0.0069,  0.0190,\n",
      "        -0.0195,  0.0134, -0.0054,  0.0028,  0.0025, -0.0172, -0.0167, -0.0149,\n",
      "         0.0129, -0.0129,  0.0011,  0.0016,  0.0186, -0.0109, -0.0147,  0.0115,\n",
      "        -0.0045, -0.0037,  0.0159, -0.0012,  0.0169, -0.0151, -0.0036, -0.0165,\n",
      "         0.0045, -0.0147,  0.0112,  0.0114,  0.0087,  0.0122, -0.0108,  0.0014,\n",
      "         0.0006, -0.0105,  0.0023, -0.0112,  0.0103, -0.0029, -0.0019,  0.0198,\n",
      "         0.0106,  0.0195,  0.0136,  0.0159,  0.0035, -0.0048,  0.0119, -0.0128,\n",
      "        -0.0028, -0.0110, -0.0083,  0.0137, -0.0126, -0.0187,  0.0035, -0.0049,\n",
      "         0.0013, -0.0155,  0.0042,  0.0046,  0.0186,  0.0121, -0.0019,  0.0065,\n",
      "         0.0152,  0.0193, -0.0097,  0.0146, -0.0164, -0.0141,  0.0171,  0.0195])\n",
      "Layer: layer3.4.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.4.conv2.bias, Biases: tensor([ 1.8430e-02, -1.2469e-02,  1.6399e-02,  2.1813e-02, -2.3068e-02,\n",
      "        -2.1222e-03,  2.9350e-02,  9.8871e-03, -3.1246e-03,  7.3674e-03,\n",
      "         1.2213e-02, -4.0401e-04,  1.8777e-03, -1.4541e-02,  1.8509e-02,\n",
      "        -1.8383e-02, -7.8497e-03, -1.3905e-03, -1.7624e-02,  2.1894e-02,\n",
      "        -2.2931e-02,  1.8447e-02,  2.7771e-02, -9.2368e-03, -1.3428e-02,\n",
      "        -1.5641e-02, -2.2998e-02, -1.6364e-02, -6.1851e-03, -1.1079e-02,\n",
      "        -2.6338e-02, -2.6333e-04,  2.6791e-02, -2.6737e-02, -2.4664e-03,\n",
      "        -3.2464e-04,  2.0378e-02,  3.5755e-06,  1.1109e-02,  2.8028e-02,\n",
      "        -9.4376e-03, -7.2041e-03,  2.8239e-02, -2.8126e-02,  5.3766e-03,\n",
      "        -2.6406e-02, -2.4632e-02, -1.7640e-02,  3.6428e-03,  5.6460e-03,\n",
      "        -2.0646e-02, -1.8777e-02,  1.5648e-03, -1.5833e-02, -1.0497e-02,\n",
      "        -2.5660e-02, -2.9726e-03, -4.5590e-03,  1.4621e-02, -2.9237e-02,\n",
      "        -1.3054e-02, -1.6160e-02, -4.0842e-03,  2.0224e-02,  3.5360e-03,\n",
      "        -3.9052e-03,  2.9405e-02,  2.4456e-02,  1.8885e-02,  2.4969e-02,\n",
      "        -2.5755e-02,  2.4370e-02,  2.2785e-02,  3.1068e-03,  1.7571e-02,\n",
      "        -2.1941e-02, -2.8416e-02,  2.7816e-03,  2.5079e-02,  8.7867e-03,\n",
      "         1.2236e-02,  4.2582e-03, -8.5954e-03, -1.8495e-02, -5.8579e-03,\n",
      "         2.8638e-02, -2.1928e-02, -2.3692e-02,  2.5211e-03,  2.3953e-02,\n",
      "        -2.3957e-02, -1.9839e-03, -8.3991e-04,  1.9624e-02,  2.5636e-02,\n",
      "         1.3267e-02, -2.8702e-02,  6.2092e-03, -1.4767e-03,  1.6739e-02,\n",
      "        -2.8597e-02, -2.6731e-02, -1.6137e-02,  9.9336e-03, -2.2425e-02,\n",
      "        -1.3996e-02,  2.3945e-02,  2.0453e-02, -2.3605e-02, -2.1914e-02,\n",
      "        -4.0890e-03, -9.2448e-03,  1.1038e-02,  1.7306e-02, -1.3045e-02,\n",
      "        -8.5569e-03,  2.6887e-04,  9.0889e-03,  1.0003e-02, -2.0334e-02,\n",
      "         1.9733e-02, -2.1366e-03, -2.4662e-03,  2.7191e-02, -2.3422e-02,\n",
      "         2.1886e-02, -5.5953e-03, -2.5846e-02, -2.6605e-02, -1.5093e-02,\n",
      "        -1.1160e-02,  1.6853e-02, -8.9109e-03, -9.1804e-03,  2.8191e-02,\n",
      "        -7.4474e-03, -1.5733e-02, -1.0338e-02,  3.8678e-03,  1.1914e-03,\n",
      "        -5.9911e-03,  9.6193e-03, -4.8825e-03,  1.8162e-02, -7.5954e-04,\n",
      "         2.6088e-02,  2.2709e-02,  2.5063e-02,  2.1163e-02,  3.5743e-03,\n",
      "         1.8160e-02, -2.2503e-02, -1.9750e-03, -2.4769e-02,  2.7051e-02,\n",
      "        -2.7333e-02,  1.1141e-03,  2.8888e-02,  1.7231e-02, -1.8198e-03,\n",
      "        -8.1553e-03, -2.8065e-02, -1.4948e-02,  2.1834e-03,  2.3764e-02,\n",
      "        -2.9449e-02,  9.1197e-03, -1.9015e-02,  1.5191e-02,  7.5210e-03,\n",
      "         2.5235e-02,  1.8064e-02, -2.4156e-02, -5.9394e-03,  4.5072e-03,\n",
      "        -8.4966e-03, -1.9683e-02, -6.9152e-03, -2.1514e-02, -9.0678e-03,\n",
      "        -6.4310e-04, -5.2589e-03,  1.4151e-02,  9.3762e-03, -1.1623e-02,\n",
      "        -2.5445e-02, -1.7140e-02, -7.7443e-04, -2.0637e-02, -2.3389e-02,\n",
      "         7.5152e-03, -2.4929e-02, -1.2597e-02,  1.5459e-02, -1.6712e-02,\n",
      "        -5.0284e-03, -6.3438e-03,  1.1665e-02, -2.7737e-02, -5.3423e-03,\n",
      "        -1.4440e-02, -1.5744e-02,  3.7921e-03, -1.3385e-02,  1.6190e-02,\n",
      "        -2.1804e-02, -2.0627e-02, -6.9023e-03,  1.2876e-02,  2.9009e-02,\n",
      "         2.2724e-02,  7.0328e-03, -2.7086e-02,  2.6843e-02, -2.6427e-02,\n",
      "         2.8565e-02,  1.7473e-02,  1.6223e-02, -1.2340e-02, -5.1101e-03,\n",
      "        -6.7025e-03, -2.3678e-02, -1.1149e-02,  2.7140e-02,  1.0997e-04,\n",
      "        -3.9306e-03,  2.2272e-02, -1.1792e-02,  6.0275e-03,  2.5344e-02,\n",
      "         6.2027e-03, -1.3758e-02, -2.4231e-02, -9.5310e-03, -5.6836e-03,\n",
      "         1.3306e-02, -2.8491e-02,  2.1278e-02,  1.7222e-02,  1.1346e-02,\n",
      "        -2.4081e-03,  1.1813e-02,  1.4579e-02,  9.4646e-03, -2.5648e-02,\n",
      "        -1.9296e-02, -3.4292e-03,  4.1486e-03, -2.2994e-02,  7.6908e-03,\n",
      "        -1.5128e-02,  2.6733e-02, -1.0946e-02,  1.8928e-02,  2.6724e-02,\n",
      "        -1.7327e-02])\n",
      "Layer: layer3.4.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.5.conv1.bias, Biases: tensor([ 0.0057,  0.0029,  0.0080, -0.0198, -0.0082, -0.0137,  0.0199, -0.0086,\n",
      "         0.0185,  0.0071, -0.0150, -0.0132, -0.0033,  0.0129, -0.0163,  0.0117,\n",
      "         0.0149,  0.0005, -0.0197, -0.0167,  0.0123, -0.0204,  0.0176, -0.0171,\n",
      "         0.0132, -0.0003, -0.0024, -0.0040,  0.0149, -0.0189,  0.0148,  0.0113,\n",
      "        -0.0088,  0.0019, -0.0040,  0.0176,  0.0067,  0.0094,  0.0117, -0.0057,\n",
      "        -0.0116, -0.0042,  0.0043,  0.0201, -0.0185, -0.0165,  0.0005,  0.0195,\n",
      "        -0.0177,  0.0019,  0.0096, -0.0189, -0.0058,  0.0189,  0.0024,  0.0026,\n",
      "        -0.0058,  0.0193,  0.0114, -0.0063, -0.0049,  0.0205,  0.0005,  0.0097,\n",
      "        -0.0147, -0.0155,  0.0019,  0.0150, -0.0057, -0.0083, -0.0071, -0.0058,\n",
      "        -0.0046, -0.0119, -0.0022,  0.0048,  0.0203, -0.0069,  0.0129,  0.0181,\n",
      "         0.0186, -0.0198,  0.0037, -0.0123,  0.0088,  0.0072,  0.0058,  0.0050,\n",
      "        -0.0113,  0.0196,  0.0006,  0.0051, -0.0171, -0.0019, -0.0060, -0.0022,\n",
      "        -0.0019, -0.0159, -0.0029,  0.0166, -0.0035, -0.0197, -0.0172, -0.0103,\n",
      "        -0.0181,  0.0112,  0.0169,  0.0081,  0.0049,  0.0147,  0.0125, -0.0098,\n",
      "        -0.0180, -0.0001,  0.0087,  0.0079,  0.0204, -0.0115,  0.0094, -0.0133,\n",
      "        -0.0046,  0.0163, -0.0098, -0.0191,  0.0009, -0.0036,  0.0161,  0.0202])\n",
      "Layer: layer3.5.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer3.5.conv2.bias, Biases: tensor([ 0.0171,  0.0058,  0.0121, -0.0219, -0.0244, -0.0100, -0.0202,  0.0177,\n",
      "        -0.0170,  0.0051, -0.0052, -0.0163, -0.0060, -0.0200,  0.0185,  0.0161,\n",
      "         0.0027, -0.0064,  0.0266, -0.0272, -0.0249, -0.0127,  0.0037, -0.0279,\n",
      "        -0.0040,  0.0057, -0.0099, -0.0064,  0.0277, -0.0051,  0.0004, -0.0037,\n",
      "         0.0086,  0.0184,  0.0088, -0.0291,  0.0294,  0.0268,  0.0150, -0.0063,\n",
      "        -0.0281,  0.0185, -0.0104,  0.0245, -0.0237, -0.0184,  0.0220,  0.0277,\n",
      "        -0.0040,  0.0283,  0.0147,  0.0139, -0.0229,  0.0192,  0.0039, -0.0215,\n",
      "         0.0102, -0.0173,  0.0023,  0.0256,  0.0116, -0.0228,  0.0028,  0.0215,\n",
      "        -0.0010,  0.0168,  0.0091, -0.0101,  0.0057, -0.0174,  0.0016, -0.0096,\n",
      "        -0.0063, -0.0294,  0.0101,  0.0185,  0.0023, -0.0071, -0.0211, -0.0229,\n",
      "         0.0198, -0.0036,  0.0095, -0.0021, -0.0140,  0.0141, -0.0242,  0.0123,\n",
      "         0.0086, -0.0218,  0.0183,  0.0144,  0.0009, -0.0111,  0.0133, -0.0066,\n",
      "        -0.0035,  0.0188, -0.0106,  0.0011,  0.0039,  0.0245, -0.0289,  0.0063,\n",
      "         0.0048, -0.0190,  0.0044,  0.0170,  0.0087, -0.0258, -0.0091,  0.0082,\n",
      "        -0.0142,  0.0242,  0.0059, -0.0026,  0.0024, -0.0202,  0.0138,  0.0200,\n",
      "        -0.0241,  0.0047,  0.0050, -0.0006,  0.0100,  0.0031, -0.0160,  0.0187,\n",
      "        -0.0234,  0.0208,  0.0165, -0.0083, -0.0025, -0.0179,  0.0240, -0.0167,\n",
      "        -0.0201,  0.0203,  0.0186,  0.0259,  0.0202,  0.0029, -0.0166, -0.0059,\n",
      "         0.0071, -0.0043, -0.0017,  0.0282, -0.0247, -0.0014, -0.0022, -0.0009,\n",
      "         0.0044, -0.0218, -0.0143,  0.0063, -0.0225,  0.0140,  0.0079, -0.0153,\n",
      "        -0.0177,  0.0084,  0.0097,  0.0169, -0.0121, -0.0163,  0.0049, -0.0204,\n",
      "         0.0096, -0.0040,  0.0110, -0.0199,  0.0128, -0.0223, -0.0013,  0.0108,\n",
      "        -0.0003, -0.0208,  0.0196,  0.0172,  0.0159, -0.0106, -0.0017,  0.0138,\n",
      "        -0.0262, -0.0148, -0.0104, -0.0084, -0.0177,  0.0057,  0.0262, -0.0147,\n",
      "         0.0107,  0.0173, -0.0247,  0.0177, -0.0078, -0.0241,  0.0135,  0.0090,\n",
      "        -0.0184,  0.0191,  0.0100, -0.0172, -0.0106,  0.0290, -0.0078,  0.0013,\n",
      "        -0.0047, -0.0039, -0.0175, -0.0282, -0.0008, -0.0158, -0.0293,  0.0119,\n",
      "         0.0233,  0.0071, -0.0144, -0.0251,  0.0017,  0.0002,  0.0146, -0.0039,\n",
      "        -0.0169,  0.0242, -0.0228,  0.0183,  0.0090,  0.0093,  0.0057,  0.0123,\n",
      "         0.0015,  0.0199, -0.0171, -0.0271, -0.0257,  0.0077, -0.0092, -0.0069,\n",
      "         0.0132,  0.0235, -0.0004, -0.0044,  0.0195, -0.0158,  0.0225,  0.0068,\n",
      "        -0.0275,  0.0189, -0.0060, -0.0128,  0.0171, -0.0044,  0.0050, -0.0048])\n",
      "Layer: layer3.5.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer4.0.conv1.bias, Biases: tensor([-0.0156,  0.0146,  0.0081,  0.0203, -0.0193,  0.0080, -0.0174,  0.0094,\n",
      "         0.0109,  0.0078, -0.0054, -0.0111,  0.0012,  0.0193,  0.0132, -0.0151,\n",
      "        -0.0036,  0.0182,  0.0087,  0.0063, -0.0054,  0.0073,  0.0126, -0.0029,\n",
      "        -0.0140, -0.0113, -0.0061, -0.0049,  0.0100, -0.0065,  0.0035, -0.0100,\n",
      "        -0.0102,  0.0155,  0.0019, -0.0094,  0.0155, -0.0090,  0.0206,  0.0144,\n",
      "         0.0077, -0.0056,  0.0054, -0.0013, -0.0184,  0.0019, -0.0199, -0.0027,\n",
      "        -0.0145, -0.0205, -0.0124, -0.0112,  0.0120,  0.0129, -0.0073, -0.0120,\n",
      "        -0.0076,  0.0169,  0.0152, -0.0092,  0.0200,  0.0148,  0.0142,  0.0170,\n",
      "         0.0050,  0.0157,  0.0150,  0.0142, -0.0168,  0.0162, -0.0197, -0.0185,\n",
      "         0.0087, -0.0109, -0.0145,  0.0206, -0.0132, -0.0189, -0.0129,  0.0189,\n",
      "         0.0155, -0.0110, -0.0120, -0.0175, -0.0061, -0.0163, -0.0099,  0.0117,\n",
      "        -0.0012,  0.0022,  0.0094, -0.0069, -0.0053,  0.0028, -0.0068, -0.0173,\n",
      "         0.0135,  0.0071, -0.0122,  0.0075, -0.0115,  0.0160,  0.0089, -0.0018,\n",
      "         0.0026,  0.0100,  0.0105,  0.0079, -0.0184, -0.0021, -0.0030,  0.0201,\n",
      "         0.0029, -0.0096, -0.0173, -0.0053, -0.0186,  0.0196,  0.0203,  0.0109,\n",
      "         0.0048,  0.0151,  0.0132,  0.0204,  0.0047,  0.0191,  0.0195,  0.0163,\n",
      "         0.0011,  0.0005,  0.0014,  0.0082, -0.0084,  0.0079, -0.0093, -0.0106,\n",
      "         0.0112,  0.0201, -0.0173,  0.0191,  0.0159,  0.0145,  0.0136, -0.0131,\n",
      "        -0.0125, -0.0141, -0.0208, -0.0096,  0.0178, -0.0077, -0.0096, -0.0056,\n",
      "        -0.0088,  0.0030,  0.0039, -0.0124,  0.0185, -0.0084, -0.0081, -0.0174,\n",
      "         0.0146,  0.0036, -0.0017,  0.0072,  0.0115,  0.0011,  0.0001, -0.0114,\n",
      "        -0.0192, -0.0152,  0.0015,  0.0013,  0.0108,  0.0136,  0.0107,  0.0048,\n",
      "        -0.0073, -0.0082,  0.0203,  0.0049,  0.0109, -0.0111,  0.0143, -0.0159,\n",
      "         0.0051,  0.0135,  0.0064,  0.0090, -0.0031, -0.0073, -0.0041, -0.0095,\n",
      "        -0.0189, -0.0190,  0.0046,  0.0163, -0.0142, -0.0113,  0.0068,  0.0029,\n",
      "         0.0112, -0.0115, -0.0146, -0.0040, -0.0165, -0.0052, -0.0045, -0.0133,\n",
      "         0.0012,  0.0022, -0.0129,  0.0060,  0.0074, -0.0028,  0.0088,  0.0025,\n",
      "         0.0018, -0.0192, -0.0120, -0.0049, -0.0130, -0.0171, -0.0114,  0.0139,\n",
      "        -0.0049,  0.0122, -0.0004, -0.0006,  0.0179,  0.0134, -0.0132,  0.0062,\n",
      "         0.0014,  0.0140,  0.0165,  0.0006,  0.0104, -0.0129, -0.0180,  0.0050,\n",
      "         0.0048,  0.0189,  0.0129, -0.0092, -0.0159,  0.0157, -0.0153, -0.0140,\n",
      "         0.0054, -0.0099,  0.0128,  0.0119, -0.0148,  0.0019, -0.0183, -0.0047])\n",
      "Layer: layer4.0.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer4.0.conv2.bias, Biases: tensor([ 1.3765e-02,  1.4740e-02, -1.0612e-02,  5.3405e-03, -1.4006e-02,\n",
      "        -1.4072e-02, -1.4589e-02,  5.5985e-03, -1.0747e-02,  1.2954e-02,\n",
      "         1.4810e-02,  8.5028e-03, -1.0144e-02, -8.8501e-04,  1.8904e-02,\n",
      "         4.1408e-04,  1.9543e-02,  6.7378e-03,  1.1489e-02, -6.0997e-03,\n",
      "         1.2529e-02,  6.0398e-03, -8.6779e-03,  9.1714e-03, -1.6724e-02,\n",
      "         1.3025e-02,  8.3447e-03, -1.2607e-02, -2.4949e-03, -1.8692e-03,\n",
      "        -3.3616e-03, -9.9822e-03, -2.5778e-03, -8.5697e-03, -1.9821e-02,\n",
      "         1.4874e-02,  1.0860e-02, -1.0898e-02, -1.0420e-03, -9.0418e-03,\n",
      "         9.3423e-03, -2.0206e-02, -5.9144e-03, -1.2887e-03, -6.0565e-03,\n",
      "        -1.4179e-02,  7.6591e-03,  1.1822e-02,  1.6661e-02, -1.5591e-03,\n",
      "        -1.4037e-02, -2.4817e-04, -1.1394e-02,  3.6062e-03,  1.1334e-02,\n",
      "         2.0711e-04, -7.0387e-03, -4.3614e-03, -1.9107e-02, -6.6943e-04,\n",
      "        -1.3076e-02,  8.4202e-03, -1.0331e-02, -1.5092e-02,  2.4391e-03,\n",
      "        -2.0625e-02,  1.5745e-02, -6.5826e-03,  1.1383e-03, -1.2469e-02,\n",
      "         7.2087e-03, -6.7719e-03,  1.3967e-02,  6.8879e-03, -1.6473e-02,\n",
      "        -2.0548e-03,  1.5653e-02, -8.4883e-03, -1.2644e-03,  1.5236e-02,\n",
      "         1.8748e-02, -1.8904e-02,  5.3334e-03,  9.3238e-03,  2.0443e-02,\n",
      "         1.3596e-02,  1.7373e-02, -1.7306e-02,  2.0198e-02,  9.0604e-04,\n",
      "        -9.7101e-03,  8.9334e-03,  6.5540e-04,  7.7206e-03, -9.5750e-03,\n",
      "         4.8307e-03,  1.9209e-03, -1.0782e-02, -3.5669e-03,  1.5785e-02,\n",
      "        -4.7239e-03, -8.0004e-04, -8.8003e-03,  5.4633e-03,  1.2802e-02,\n",
      "        -1.4958e-02,  3.9261e-03,  1.0355e-02, -1.9185e-02, -5.1981e-03,\n",
      "        -9.1720e-03, -5.1231e-04,  4.1761e-03, -1.5050e-02, -1.9658e-02,\n",
      "        -1.1443e-02, -4.8362e-03, -2.3485e-03,  7.0310e-03, -8.1487e-04,\n",
      "        -1.9056e-02, -1.4284e-02,  9.0242e-03,  8.1498e-03, -4.7087e-03,\n",
      "        -3.6681e-03,  1.2781e-03, -1.2114e-02, -1.4573e-02,  1.7561e-02,\n",
      "         7.1350e-04, -2.0094e-02, -4.9356e-04, -4.5779e-03, -1.0728e-02,\n",
      "         1.3131e-02,  6.3346e-03,  1.9654e-02,  3.9554e-03, -1.6532e-02,\n",
      "        -8.4765e-03, -6.4274e-03, -1.7248e-02, -7.1218e-03,  8.2186e-03,\n",
      "         1.0673e-03, -1.1483e-02, -1.8431e-02, -1.2295e-02, -1.0556e-02,\n",
      "         9.8414e-03,  2.3929e-04,  2.0489e-02, -1.6819e-02, -1.6616e-02,\n",
      "        -1.7102e-02,  8.6025e-03, -5.6181e-03,  1.1124e-02,  1.1401e-02,\n",
      "         1.5858e-02, -1.9234e-02, -9.5521e-03, -1.8900e-02,  1.9348e-02,\n",
      "        -1.6560e-02, -1.8506e-03, -1.1400e-02, -4.9161e-03,  6.2820e-03,\n",
      "         1.5493e-02,  1.2441e-02,  1.9347e-02, -4.3297e-03, -4.8346e-03,\n",
      "        -1.6049e-02,  1.0056e-02, -1.6035e-02, -1.4795e-02, -3.8736e-03,\n",
      "        -1.1239e-02,  1.8005e-03, -1.0707e-02,  1.8687e-02,  1.1702e-02,\n",
      "        -4.5258e-03, -1.9768e-02, -7.2835e-03, -8.0871e-03, -1.5942e-02,\n",
      "         5.8234e-05,  1.5770e-02,  4.7803e-03, -1.1392e-02, -8.0352e-03,\n",
      "        -1.5413e-02,  1.8295e-02,  4.7079e-03, -5.1080e-03,  1.3183e-03,\n",
      "        -1.3373e-02, -1.9240e-02, -1.8885e-02,  3.0935e-03,  1.5318e-02,\n",
      "         1.0021e-03, -2.7508e-03,  4.7382e-03, -3.3127e-03, -3.4358e-03,\n",
      "        -2.0104e-02,  4.9270e-03, -1.3446e-02,  1.1774e-02,  1.6282e-02,\n",
      "         1.1243e-02,  1.2479e-03, -4.3122e-03, -4.8551e-03, -1.4516e-02,\n",
      "        -2.0036e-02, -1.6356e-02,  1.6837e-02, -2.0584e-02, -1.7420e-02,\n",
      "        -1.8508e-02, -9.7048e-03, -9.4348e-03, -1.8065e-02,  6.4259e-04,\n",
      "         6.4040e-03,  9.2064e-03,  4.4181e-04, -1.9832e-02,  1.7110e-02,\n",
      "         1.8496e-02,  7.9995e-03,  6.6219e-03, -1.2371e-02, -1.8167e-02,\n",
      "         1.3801e-03,  1.6743e-03, -1.0333e-03, -1.8637e-02,  9.2196e-03,\n",
      "         9.2114e-03, -1.8730e-02, -1.8822e-02,  7.4687e-03, -1.8595e-02,\n",
      "         3.1122e-03, -1.2381e-02,  1.3572e-02, -1.3799e-02, -1.1581e-02,\n",
      "        -1.2192e-02,  5.5247e-03, -3.8770e-03,  6.3422e-03, -1.1541e-02,\n",
      "        -7.4007e-03, -2.1756e-03,  8.5100e-03,  8.9742e-03, -8.8919e-03,\n",
      "        -1.8095e-02,  5.4255e-03,  1.0158e-03, -8.6299e-03, -2.0527e-02,\n",
      "         2.3395e-03,  2.5950e-03,  5.3275e-03, -5.3729e-03, -1.3664e-02,\n",
      "        -5.1580e-03, -1.3276e-02,  1.5484e-03, -1.2227e-02,  4.4610e-03,\n",
      "        -3.2442e-03,  2.0710e-02, -4.9940e-03, -1.4479e-02, -2.0231e-02,\n",
      "         2.6924e-03,  2.2443e-03,  3.9226e-03, -1.9882e-02, -1.4446e-02,\n",
      "         7.2234e-03, -1.0117e-02, -1.0697e-02, -2.0337e-02, -1.1067e-02,\n",
      "        -2.6405e-03, -1.3873e-02, -1.2675e-02, -2.7979e-03,  5.3264e-03,\n",
      "         1.6696e-02, -1.7699e-02, -1.7335e-02,  5.0459e-03, -9.6493e-03,\n",
      "        -1.5406e-02,  3.7970e-03,  1.2076e-02, -8.7230e-03, -1.9069e-02,\n",
      "         1.7447e-02, -1.1538e-02, -1.1205e-02, -4.0315e-03, -1.0695e-02,\n",
      "         1.0876e-02,  9.9132e-03, -1.5872e-02,  5.0351e-03,  1.5353e-02,\n",
      "        -1.1342e-02, -1.5305e-02, -2.4841e-03, -1.8439e-02,  1.0181e-02,\n",
      "        -7.0469e-03, -1.9387e-02, -9.8514e-03,  7.2191e-03, -1.2505e-02,\n",
      "         5.0574e-03,  1.0519e-02, -6.4575e-03,  6.9265e-04,  1.0941e-02,\n",
      "        -1.2686e-02,  1.9460e-02,  1.7205e-02,  4.2942e-03,  8.9667e-03,\n",
      "         6.1975e-03,  7.7496e-03,  6.3748e-03, -1.1436e-03, -1.7614e-02,\n",
      "        -1.4556e-02,  7.5850e-03,  1.1666e-02,  1.4619e-02,  1.7430e-02,\n",
      "         2.5961e-04, -1.8887e-02,  8.4324e-03,  4.5902e-03,  1.2051e-02,\n",
      "        -2.1837e-03,  3.9792e-03,  9.0464e-03, -1.2149e-02,  1.8697e-02,\n",
      "        -6.3611e-03, -2.6573e-03,  1.2327e-03,  3.8743e-03,  8.1947e-03,\n",
      "        -6.4972e-03, -1.4597e-02,  1.2999e-02, -3.4712e-04, -1.3620e-02,\n",
      "         1.4964e-02,  1.8796e-02,  9.7886e-03,  5.4402e-03,  9.1265e-03,\n",
      "        -2.2813e-03,  1.8548e-02,  1.2258e-03,  1.2513e-02, -1.0208e-02,\n",
      "        -2.0377e-02,  5.6791e-03, -7.2058e-03, -6.8761e-03, -1.3989e-02,\n",
      "        -1.5121e-02, -6.1496e-03, -5.2970e-03,  1.0164e-02,  1.8080e-03,\n",
      "        -7.0349e-03,  3.6639e-03,  6.2537e-03, -2.0776e-02, -4.9341e-03,\n",
      "        -1.9391e-02, -1.2467e-02, -4.2863e-03, -5.7746e-03, -1.4545e-02,\n",
      "         3.6721e-04, -2.0372e-02,  7.6001e-05,  1.4328e-02,  1.1458e-02,\n",
      "        -4.4046e-03, -8.6268e-03, -1.9175e-02,  1.6897e-02, -1.4049e-02,\n",
      "         1.8070e-02, -8.3619e-03,  1.0985e-02, -2.4514e-04,  4.8579e-03,\n",
      "        -9.9065e-03,  9.5098e-03, -4.7014e-03,  1.2842e-02,  5.5555e-03,\n",
      "         9.9394e-03, -2.9825e-03, -1.8072e-02,  2.9496e-03,  5.7335e-03,\n",
      "         1.4346e-02, -1.8048e-03,  4.7216e-03, -2.6316e-03,  1.7022e-02,\n",
      "         1.5616e-02,  9.0634e-03, -1.7934e-02,  5.9353e-03,  1.6176e-02,\n",
      "        -3.7825e-03, -2.6396e-03,  1.6078e-02,  1.5053e-02, -1.3446e-02,\n",
      "        -6.5521e-03,  1.6458e-02,  5.5307e-03, -2.0663e-02, -1.8515e-02,\n",
      "        -8.2555e-03, -2.2021e-03,  2.9561e-03,  8.4843e-03,  1.6016e-02,\n",
      "        -3.8868e-03, -6.1778e-03,  7.1046e-04,  1.8611e-02, -3.1721e-03,\n",
      "         2.0423e-02,  9.4763e-03, -1.2274e-02,  1.8492e-02, -1.3232e-02,\n",
      "         4.6243e-03,  9.0784e-03,  1.0567e-02,  1.2425e-02,  2.3199e-03,\n",
      "         1.8075e-02,  9.3749e-04, -3.1953e-03,  1.1383e-03,  1.6535e-02,\n",
      "        -1.6662e-02, -7.4809e-04,  1.6739e-02,  1.9643e-02,  4.0353e-03,\n",
      "        -1.3952e-02,  7.0739e-03, -1.3923e-03,  6.8560e-03, -1.5295e-02,\n",
      "        -1.2426e-02, -1.9489e-03,  1.8579e-02, -1.1944e-03,  1.7120e-03,\n",
      "         4.2446e-03,  3.4032e-03,  1.8927e-02,  6.2192e-03, -1.6324e-02,\n",
      "        -1.1113e-02, -9.9293e-03, -2.2351e-04,  7.9249e-03, -1.7175e-03,\n",
      "        -1.7140e-02, -1.7385e-02, -1.6873e-02, -1.8768e-02, -2.3811e-03,\n",
      "         7.5900e-03,  8.0644e-03,  3.7940e-03,  1.2882e-02,  6.9170e-03,\n",
      "        -1.6810e-02, -1.2771e-02, -4.5806e-03,  1.7705e-02,  3.9141e-03,\n",
      "         2.3223e-03,  1.0286e-02])\n",
      "Layer: layer4.0.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer4.0.identity_downsample.0.bias, Biases: tensor([ 0.0603, -0.0548,  0.0147,  0.0522, -0.0479,  0.0449, -0.0147,  0.0071,\n",
      "        -0.0032,  0.0532,  0.0164, -0.0021, -0.0357,  0.0175,  0.0190, -0.0453,\n",
      "         0.0170,  0.0273,  0.0145, -0.0319,  0.0348,  0.0008, -0.0535,  0.0025,\n",
      "        -0.0167,  0.0188,  0.0382,  0.0105, -0.0586,  0.0452, -0.0223,  0.0226,\n",
      "         0.0209, -0.0422,  0.0113, -0.0510,  0.0437, -0.0311,  0.0518,  0.0439,\n",
      "         0.0335,  0.0601,  0.0019, -0.0443,  0.0268, -0.0338,  0.0113,  0.0313,\n",
      "        -0.0426, -0.0356,  0.0242, -0.0236, -0.0085,  0.0364, -0.0416, -0.0091,\n",
      "         0.0469,  0.0549, -0.0138,  0.0573,  0.0444,  0.0548, -0.0149,  0.0345,\n",
      "        -0.0416,  0.0456,  0.0619, -0.0080, -0.0168, -0.0287, -0.0252,  0.0084,\n",
      "        -0.0054, -0.0462,  0.0473,  0.0206,  0.0477,  0.0418, -0.0516,  0.0602,\n",
      "         0.0012,  0.0566, -0.0191,  0.0105,  0.0325,  0.0502, -0.0607, -0.0448,\n",
      "         0.0414,  0.0233, -0.0217, -0.0536, -0.0600,  0.0304, -0.0609, -0.0043,\n",
      "         0.0004,  0.0329, -0.0615,  0.0270,  0.0392, -0.0187,  0.0306, -0.0206,\n",
      "         0.0124, -0.0070,  0.0230, -0.0603,  0.0043,  0.0590,  0.0505,  0.0537,\n",
      "        -0.0465,  0.0587,  0.0243,  0.0610, -0.0208, -0.0426,  0.0614,  0.0565,\n",
      "         0.0100, -0.0625, -0.0519,  0.0433,  0.0227, -0.0121, -0.0416, -0.0440,\n",
      "         0.0205,  0.0625,  0.0112, -0.0207,  0.0253,  0.0163, -0.0239, -0.0323,\n",
      "         0.0357, -0.0096, -0.0450, -0.0374,  0.0343, -0.0342,  0.0259,  0.0600,\n",
      "        -0.0570,  0.0520,  0.0121, -0.0612, -0.0004, -0.0464, -0.0584,  0.0037,\n",
      "         0.0265, -0.0582, -0.0008,  0.0486, -0.0345,  0.0242,  0.0327, -0.0583,\n",
      "        -0.0091, -0.0256, -0.0195,  0.0550, -0.0555,  0.0387,  0.0325, -0.0307,\n",
      "        -0.0426, -0.0249, -0.0333, -0.0533, -0.0071, -0.0141,  0.0131, -0.0170,\n",
      "         0.0184, -0.0577,  0.0604,  0.0456,  0.0183,  0.0600, -0.0575,  0.0212,\n",
      "         0.0570, -0.0024, -0.0539, -0.0157, -0.0331,  0.0194,  0.0162, -0.0576,\n",
      "         0.0597, -0.0393,  0.0607,  0.0124,  0.0426,  0.0045,  0.0335, -0.0466,\n",
      "         0.0390, -0.0374,  0.0146, -0.0613,  0.0365,  0.0085, -0.0520,  0.0299,\n",
      "         0.0151,  0.0432, -0.0222, -0.0008,  0.0212, -0.0574,  0.0485,  0.0516,\n",
      "        -0.0068, -0.0485, -0.0594, -0.0336,  0.0594,  0.0517,  0.0025,  0.0292,\n",
      "        -0.0559,  0.0182,  0.0310, -0.0337,  0.0208, -0.0088,  0.0013,  0.0215,\n",
      "         0.0126,  0.0424, -0.0200, -0.0370, -0.0007,  0.0321,  0.0111,  0.0549,\n",
      "        -0.0051, -0.0565,  0.0446,  0.0596,  0.0463, -0.0415, -0.0586, -0.0546,\n",
      "         0.0552, -0.0618,  0.0460, -0.0009,  0.0511,  0.0167, -0.0391,  0.0524,\n",
      "         0.0554,  0.0490,  0.0064, -0.0154,  0.0540, -0.0141,  0.0430, -0.0018,\n",
      "        -0.0578, -0.0475,  0.0099,  0.0547, -0.0309,  0.0104,  0.0068,  0.0131,\n",
      "        -0.0594, -0.0407, -0.0437,  0.0204,  0.0073, -0.0569,  0.0137,  0.0112,\n",
      "         0.0209,  0.0456,  0.0013, -0.0132,  0.0103,  0.0223, -0.0019, -0.0168,\n",
      "        -0.0531,  0.0072,  0.0420,  0.0239,  0.0153, -0.0027,  0.0055, -0.0618,\n",
      "         0.0476,  0.0523,  0.0324,  0.0453, -0.0558, -0.0407, -0.0536,  0.0291,\n",
      "         0.0501, -0.0076, -0.0527, -0.0173,  0.0189,  0.0141,  0.0284, -0.0156,\n",
      "         0.0439, -0.0272,  0.0004, -0.0084, -0.0548,  0.0289, -0.0054,  0.0140,\n",
      "        -0.0289, -0.0473,  0.0552,  0.0033, -0.0320, -0.0064,  0.0532,  0.0514,\n",
      "         0.0274,  0.0066, -0.0009, -0.0290, -0.0364,  0.0180, -0.0619, -0.0100,\n",
      "        -0.0475,  0.0578,  0.0279,  0.0326, -0.0035,  0.0249,  0.0565, -0.0395,\n",
      "        -0.0344,  0.0065,  0.0239, -0.0023, -0.0037, -0.0435, -0.0011, -0.0216,\n",
      "        -0.0175,  0.0056,  0.0364,  0.0191,  0.0364, -0.0310, -0.0069, -0.0204,\n",
      "         0.0464,  0.0544,  0.0297,  0.0422,  0.0356,  0.0194, -0.0619,  0.0384,\n",
      "         0.0341, -0.0563,  0.0224, -0.0079,  0.0538, -0.0333, -0.0524, -0.0222,\n",
      "        -0.0179,  0.0071, -0.0565, -0.0612,  0.0571,  0.0202,  0.0489,  0.0073,\n",
      "        -0.0330, -0.0330, -0.0002,  0.0175, -0.0311,  0.0597,  0.0448, -0.0421,\n",
      "        -0.0507, -0.0218, -0.0273, -0.0519, -0.0325, -0.0586, -0.0167,  0.0573,\n",
      "        -0.0255,  0.0270,  0.0325, -0.0311, -0.0572,  0.0568,  0.0250,  0.0084,\n",
      "         0.0378,  0.0492,  0.0487,  0.0275, -0.0215, -0.0467,  0.0058, -0.0122,\n",
      "        -0.0512, -0.0440, -0.0158, -0.0232, -0.0053, -0.0564,  0.0354,  0.0542,\n",
      "         0.0528, -0.0549,  0.0155, -0.0573,  0.0500, -0.0435,  0.0503, -0.0021,\n",
      "         0.0554,  0.0134,  0.0236,  0.0404, -0.0240,  0.0079, -0.0136,  0.0590,\n",
      "         0.0005, -0.0204, -0.0422, -0.0561, -0.0192, -0.0289,  0.0005,  0.0495,\n",
      "        -0.0149, -0.0102,  0.0116, -0.0167,  0.0035,  0.0456,  0.0166,  0.0117,\n",
      "         0.0122,  0.0099,  0.0198,  0.0150, -0.0009,  0.0579,  0.0333,  0.0597,\n",
      "         0.0097, -0.0289, -0.0467,  0.0433,  0.0400,  0.0361,  0.0239,  0.0489,\n",
      "         0.0489,  0.0434, -0.0267,  0.0295,  0.0150, -0.0231,  0.0112,  0.0381,\n",
      "         0.0090,  0.0203,  0.0235, -0.0619, -0.0411,  0.0244, -0.0162, -0.0076,\n",
      "         0.0386,  0.0182, -0.0524,  0.0357,  0.0244,  0.0457,  0.0566, -0.0383,\n",
      "         0.0468,  0.0549,  0.0560,  0.0065, -0.0105, -0.0489,  0.0360,  0.0479,\n",
      "        -0.0333,  0.0434,  0.0098, -0.0235,  0.0334, -0.0608,  0.0010, -0.0156])\n",
      "Layer: layer4.0.identity_downsample.1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer4.1.conv1.bias, Biases: tensor([-0.0014, -0.0105,  0.0009, -0.0062,  0.0098, -0.0077,  0.0079, -0.0006,\n",
      "         0.0083, -0.0031,  0.0128,  0.0117,  0.0026, -0.0085, -0.0028, -0.0135,\n",
      "         0.0138,  0.0061,  0.0129, -0.0093, -0.0079,  0.0050, -0.0031,  0.0039,\n",
      "         0.0011, -0.0064,  0.0129,  0.0123, -0.0054,  0.0133, -0.0145,  0.0034,\n",
      "        -0.0141, -0.0130, -0.0125,  0.0014,  0.0134,  0.0053, -0.0098,  0.0140,\n",
      "        -0.0089, -0.0047,  0.0076, -0.0049,  0.0106,  0.0076,  0.0054,  0.0094,\n",
      "         0.0037, -0.0106, -0.0129,  0.0110, -0.0069, -0.0084, -0.0080,  0.0088,\n",
      "         0.0062, -0.0063, -0.0113, -0.0049,  0.0135, -0.0116, -0.0073,  0.0124,\n",
      "         0.0039,  0.0135, -0.0066, -0.0068,  0.0130, -0.0039,  0.0046,  0.0133,\n",
      "        -0.0032, -0.0115, -0.0039, -0.0058,  0.0130,  0.0052, -0.0050,  0.0072,\n",
      "        -0.0030, -0.0003,  0.0120, -0.0130, -0.0116,  0.0037, -0.0016, -0.0017,\n",
      "         0.0035,  0.0121, -0.0008,  0.0059, -0.0010, -0.0068, -0.0083, -0.0016,\n",
      "        -0.0036,  0.0051,  0.0085, -0.0080, -0.0054,  0.0085, -0.0101, -0.0145,\n",
      "        -0.0109, -0.0132,  0.0038, -0.0067, -0.0136,  0.0141, -0.0071,  0.0048,\n",
      "         0.0094,  0.0057, -0.0051, -0.0078,  0.0011, -0.0074,  0.0116, -0.0041,\n",
      "        -0.0057,  0.0118, -0.0038,  0.0040,  0.0104,  0.0088, -0.0119,  0.0080,\n",
      "        -0.0122,  0.0114, -0.0136,  0.0047,  0.0130, -0.0099, -0.0138,  0.0137,\n",
      "         0.0141,  0.0033,  0.0012,  0.0094, -0.0079,  0.0039,  0.0139, -0.0008,\n",
      "        -0.0063,  0.0015, -0.0035,  0.0033, -0.0072, -0.0122, -0.0070,  0.0002,\n",
      "         0.0061,  0.0074,  0.0093, -0.0001, -0.0143,  0.0026,  0.0100,  0.0048,\n",
      "        -0.0059,  0.0016, -0.0083, -0.0138, -0.0090, -0.0015,  0.0007, -0.0064,\n",
      "         0.0125, -0.0141, -0.0110,  0.0113,  0.0081,  0.0004, -0.0065,  0.0040,\n",
      "         0.0135,  0.0094,  0.0077, -0.0025,  0.0064,  0.0030,  0.0031,  0.0008,\n",
      "         0.0029,  0.0128,  0.0084, -0.0124,  0.0032, -0.0087, -0.0049, -0.0020,\n",
      "        -0.0044,  0.0090, -0.0029, -0.0002, -0.0141, -0.0091, -0.0119, -0.0060,\n",
      "         0.0080,  0.0030, -0.0124, -0.0068, -0.0007,  0.0072,  0.0050,  0.0005,\n",
      "         0.0133,  0.0021,  0.0035,  0.0014, -0.0097, -0.0066, -0.0139,  0.0066,\n",
      "        -0.0136, -0.0021,  0.0044, -0.0092, -0.0006, -0.0092, -0.0058, -0.0022,\n",
      "         0.0117, -0.0098, -0.0009, -0.0117, -0.0062, -0.0122, -0.0099, -0.0134,\n",
      "        -0.0007,  0.0113,  0.0044, -0.0065,  0.0090, -0.0037, -0.0022,  0.0107,\n",
      "        -0.0069,  0.0123, -0.0074,  0.0003,  0.0082, -0.0096, -0.0103,  0.0102,\n",
      "        -0.0107, -0.0105,  0.0013,  0.0142,  0.0037, -0.0050, -0.0098,  0.0084])\n",
      "Layer: layer4.1.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer4.1.conv2.bias, Biases: tensor([ 1.5199e-02, -1.7845e-02, -1.2302e-03,  1.8389e-02,  1.4679e-02,\n",
      "         1.0126e-02, -6.7330e-03,  2.8789e-03, -5.2023e-04, -1.1249e-02,\n",
      "        -2.0630e-02, -1.2865e-02,  1.4928e-02, -1.0698e-02,  2.7326e-03,\n",
      "         1.7858e-02,  1.0339e-02,  1.3588e-03, -8.8313e-03,  1.8634e-05,\n",
      "         1.8681e-02, -7.6699e-04,  2.0378e-02,  5.5464e-03,  4.7132e-03,\n",
      "         1.6310e-02,  2.8418e-03,  1.8339e-02, -4.6599e-03, -1.0583e-02,\n",
      "        -2.8947e-03, -1.6340e-02,  7.7409e-04,  9.4357e-03,  1.2128e-02,\n",
      "         1.6681e-02, -4.7698e-04, -1.3928e-02,  7.8701e-03, -9.9335e-03,\n",
      "        -1.2024e-02,  4.7494e-03,  6.8651e-03, -1.4287e-02, -2.0388e-02,\n",
      "        -9.9072e-03, -5.3231e-03,  2.9970e-04,  1.5850e-02, -1.1193e-02,\n",
      "         3.7410e-03,  3.4584e-03,  2.0587e-02, -1.9989e-02,  1.9618e-02,\n",
      "         1.9853e-02, -5.5679e-03, -8.0877e-03, -8.4240e-03, -2.0530e-02,\n",
      "         1.9877e-02,  1.8411e-02, -4.6636e-03, -1.8490e-02,  6.5969e-03,\n",
      "        -7.6987e-03, -7.7523e-03, -1.4087e-02,  1.3799e-02, -6.9336e-03,\n",
      "        -4.6651e-03, -1.0068e-02, -8.8951e-04,  1.8533e-02, -3.7944e-03,\n",
      "        -7.0824e-03, -8.8618e-03,  1.7237e-02, -1.5750e-02,  5.7479e-03,\n",
      "         1.6544e-02,  1.8505e-02, -1.9949e-02, -1.8009e-02,  2.0401e-02,\n",
      "        -3.5286e-03, -1.9622e-02, -1.7467e-02,  9.7061e-03, -4.7585e-03,\n",
      "         2.0529e-02,  1.0820e-03,  1.2405e-02,  6.2519e-03,  7.6276e-03,\n",
      "        -1.7221e-02,  5.7093e-03, -2.0010e-03, -1.7947e-02,  2.0675e-02,\n",
      "         2.1743e-03, -1.0266e-02, -2.0273e-02, -2.0319e-02, -1.6587e-02,\n",
      "        -1.4293e-02, -1.9216e-02,  1.8505e-02, -8.5688e-03, -1.7458e-02,\n",
      "        -1.2263e-02,  8.4034e-03,  1.6635e-02, -1.1107e-02, -1.0465e-02,\n",
      "         2.0372e-02,  1.4297e-02,  1.3502e-02, -7.0105e-03, -1.2388e-02,\n",
      "        -1.1002e-03,  1.0740e-02,  1.6941e-02,  1.5103e-02,  1.7817e-03,\n",
      "        -5.5391e-03, -7.5897e-04, -1.8044e-02,  6.6154e-03, -5.6459e-03,\n",
      "        -1.1337e-02, -6.6461e-03,  7.8663e-03, -1.6224e-02,  7.4371e-03,\n",
      "         4.6686e-03, -3.9692e-04,  5.3003e-03, -1.3528e-02,  1.9497e-02,\n",
      "        -9.6083e-03, -1.4149e-02, -1.8910e-03,  1.2719e-02,  1.9770e-03,\n",
      "         1.0183e-02, -2.0108e-03, -1.0149e-02,  1.3280e-02, -1.4485e-02,\n",
      "         5.8515e-03, -3.1844e-04,  1.1515e-02, -1.6692e-02,  7.8074e-03,\n",
      "        -1.8231e-02,  2.0677e-02, -1.1843e-02, -3.8705e-03, -1.6911e-02,\n",
      "        -1.1030e-02,  1.7143e-02, -6.0158e-03,  1.3801e-02, -6.4536e-03,\n",
      "         1.4956e-02,  3.7527e-03,  4.0629e-03, -1.0985e-02,  1.1386e-02,\n",
      "         9.4957e-03,  9.1982e-03,  2.0232e-03, -8.2831e-03, -1.2837e-02,\n",
      "         1.6699e-02, -1.1197e-02, -9.1491e-03,  1.5976e-02,  1.9699e-02,\n",
      "         5.8728e-03, -7.1714e-03, -1.4590e-02,  9.6196e-03,  5.6582e-03,\n",
      "         6.6573e-03, -1.5361e-02, -1.2008e-02,  8.6295e-03,  1.5349e-02,\n",
      "         1.7532e-02,  2.2412e-03,  6.8870e-03, -7.8333e-05,  6.6079e-03,\n",
      "         1.6502e-02, -5.2655e-03, -8.7834e-03,  1.3932e-02,  7.9928e-03,\n",
      "         8.6016e-03,  1.7758e-02, -7.6458e-03, -6.3599e-03, -6.5547e-03,\n",
      "        -1.8251e-02, -6.2944e-03,  1.3595e-02,  1.5262e-02, -1.4109e-02,\n",
      "         1.3104e-02, -1.0481e-02, -8.8311e-03,  1.9870e-02, -1.6765e-02,\n",
      "         1.5260e-02,  7.9959e-03, -6.2018e-04,  1.2449e-02,  9.0802e-03,\n",
      "        -4.2137e-03,  1.2244e-02, -1.2982e-02,  1.5426e-02,  1.9602e-02,\n",
      "         1.6253e-02,  1.9391e-02, -8.4033e-03,  1.6260e-02,  9.4408e-03,\n",
      "        -8.7211e-03, -1.1923e-02, -6.5543e-03, -8.4319e-03, -1.9171e-02,\n",
      "        -1.3858e-03, -4.3631e-03, -2.7452e-03, -9.3483e-03, -1.4289e-02,\n",
      "        -1.1547e-02, -8.6914e-04,  1.9869e-02, -6.5116e-03,  1.0400e-02,\n",
      "        -1.9542e-02,  5.2907e-03,  1.8921e-02, -1.1704e-02, -1.4470e-02,\n",
      "        -5.3121e-03, -4.1770e-03, -1.3433e-02,  1.1781e-02, -1.1512e-02,\n",
      "        -2.0543e-02, -1.5300e-03, -5.6021e-03,  4.1037e-03,  2.0811e-02,\n",
      "         1.3331e-03, -1.3048e-02,  1.7437e-02, -6.7229e-03, -1.8654e-02,\n",
      "         2.8644e-03, -1.0481e-02,  9.7219e-03, -6.7284e-03, -5.1352e-03,\n",
      "        -1.3436e-02,  1.7542e-02, -8.5321e-03,  1.6516e-02,  1.4134e-02,\n",
      "        -1.9874e-03,  1.8233e-02,  1.6429e-04,  8.3448e-03, -8.2851e-03,\n",
      "         1.5333e-02, -1.7760e-02,  9.6508e-03, -2.8475e-03, -2.0492e-02,\n",
      "         1.7869e-02,  4.5285e-03,  1.1881e-02,  2.2716e-03,  1.6855e-03,\n",
      "         1.8153e-03,  5.7342e-03, -1.1716e-02,  1.6625e-03, -1.9461e-02,\n",
      "        -4.0353e-03, -2.4885e-03,  1.3999e-02, -1.4506e-02,  3.3930e-03,\n",
      "        -1.4246e-02, -1.0809e-02, -1.9184e-02,  1.3646e-02,  1.0632e-02,\n",
      "        -1.6535e-02,  1.2548e-02, -1.6266e-02,  2.0248e-02, -1.1516e-02,\n",
      "         6.9100e-03,  4.3763e-03, -1.8814e-02, -7.2331e-03,  6.9754e-03,\n",
      "        -1.9591e-02,  1.1927e-02, -1.8668e-02, -6.5083e-04, -1.6375e-02,\n",
      "        -1.8479e-02, -1.5096e-02, -2.0408e-02, -2.1758e-03, -1.1270e-02,\n",
      "        -1.3373e-02,  1.2201e-02, -6.5582e-03,  1.6726e-02,  1.7532e-03,\n",
      "        -5.9435e-03,  8.4382e-03,  3.3490e-03, -3.4680e-03, -1.0848e-02,\n",
      "         1.7291e-03, -1.4651e-02,  7.6517e-03,  8.1926e-03, -3.4837e-03,\n",
      "        -4.9623e-03, -3.2728e-03,  7.5832e-04,  1.9316e-02, -1.6669e-02,\n",
      "        -3.3885e-03, -1.9707e-02, -2.0064e-02, -2.0260e-02, -4.9156e-03,\n",
      "         1.4530e-02, -9.8136e-03, -1.6802e-03,  6.5839e-03, -7.0433e-03,\n",
      "         7.3269e-03,  1.1118e-02, -1.5859e-02,  1.0063e-02, -5.5207e-03,\n",
      "        -1.2525e-02, -2.0790e-02, -1.3203e-02, -1.1214e-02, -4.0928e-03,\n",
      "         2.0122e-02,  1.6512e-02, -3.9849e-03, -9.8084e-03,  1.5220e-02,\n",
      "         4.7219e-03, -1.9072e-03, -1.0791e-02, -3.2901e-04,  1.0105e-02,\n",
      "         5.2218e-03,  1.8927e-02, -7.9182e-03,  1.9001e-02, -8.9973e-03,\n",
      "         9.7786e-03,  2.0818e-02, -1.3551e-03, -1.7642e-02, -1.7639e-02,\n",
      "         9.6187e-03,  1.0727e-02, -6.7381e-03, -1.5057e-02, -2.0757e-02,\n",
      "         1.3374e-02, -1.9689e-02,  7.4301e-03,  5.5712e-03, -1.8396e-02,\n",
      "         4.5863e-03, -1.4855e-02, -2.5962e-03, -7.1699e-03, -1.9862e-02,\n",
      "        -5.1535e-03, -1.7162e-02,  5.9075e-03, -5.1983e-03,  1.6623e-02,\n",
      "        -1.2050e-02, -5.7831e-03, -2.7077e-03,  7.4264e-03,  1.4043e-02,\n",
      "        -2.9895e-03,  6.6679e-03, -1.2698e-02,  4.0192e-03, -1.0580e-02,\n",
      "         2.6095e-04, -3.7442e-03, -1.6151e-02, -1.8532e-03, -1.8198e-02,\n",
      "         4.1131e-03,  1.1213e-02,  1.9046e-02,  1.6770e-03,  6.6138e-04,\n",
      "        -3.0662e-03, -1.7538e-02, -9.3687e-03,  4.1212e-04,  1.7972e-02,\n",
      "        -4.7957e-03, -1.5760e-02, -1.4787e-02,  2.0536e-02,  3.1961e-03,\n",
      "        -2.0786e-02, -1.9294e-02,  1.9608e-02,  4.8551e-03,  6.1610e-03,\n",
      "        -1.8318e-02, -1.9648e-02, -6.7418e-03,  1.3237e-02, -7.5655e-03,\n",
      "        -7.0309e-03,  1.8014e-02,  1.7237e-02, -1.3669e-02, -2.0375e-02,\n",
      "        -1.0827e-02,  7.8010e-04, -8.0985e-03, -1.9246e-02,  1.5032e-02,\n",
      "         4.8546e-03, -6.0320e-03, -4.7077e-04,  5.2280e-03, -3.1147e-03,\n",
      "        -6.1512e-03, -1.7492e-02,  7.4064e-03,  1.0628e-02, -1.4206e-02,\n",
      "        -1.9203e-02,  8.0680e-04,  1.9552e-02,  1.6971e-02,  1.0413e-02,\n",
      "        -1.3105e-02,  3.5203e-03, -1.1537e-02,  5.3479e-04, -1.0533e-02,\n",
      "         1.3296e-02, -2.2163e-03,  3.6777e-03, -1.3687e-02,  6.7510e-03,\n",
      "        -3.7010e-03, -1.6627e-02, -1.4447e-02, -1.3303e-02,  2.0143e-02,\n",
      "         2.3178e-03,  1.2011e-03, -5.0032e-03,  1.1172e-02, -1.5335e-02,\n",
      "         1.2193e-02,  5.6855e-04,  6.0827e-03, -1.5148e-02, -1.7229e-03,\n",
      "         6.1980e-03,  1.6827e-02, -1.5083e-02, -4.4956e-03,  5.1165e-03,\n",
      "         1.4178e-02,  1.5819e-02,  3.5476e-03,  7.1251e-03, -5.3680e-03,\n",
      "        -1.2167e-02, -3.3101e-03,  2.0484e-03,  5.8315e-03,  6.0849e-03,\n",
      "        -6.4637e-03,  2.0779e-03])\n",
      "Layer: layer4.1.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer4.2.conv1.bias, Biases: tensor([ 9.3682e-03, -4.1722e-03,  1.1581e-02, -6.5032e-03,  8.5188e-03,\n",
      "        -1.4465e-02,  2.7675e-03,  1.7784e-03, -3.5415e-03, -4.4809e-03,\n",
      "        -1.2476e-02,  4.7020e-03,  4.8121e-03, -9.0379e-03, -5.3337e-03,\n",
      "        -1.1648e-04,  8.8740e-03,  2.5221e-03, -6.0579e-03, -7.8028e-03,\n",
      "        -1.4678e-02,  4.1180e-04,  5.7607e-04,  1.0646e-02, -1.0732e-02,\n",
      "         5.1202e-03,  1.3185e-02,  5.9123e-03, -6.9511e-03, -9.9030e-03,\n",
      "         3.7117e-03, -6.4812e-03, -5.0738e-04,  5.4789e-03, -1.2654e-02,\n",
      "         6.4852e-03,  8.2620e-04,  6.7317e-03,  8.8991e-03, -2.6900e-03,\n",
      "        -9.8081e-03,  1.0107e-02,  3.5049e-03, -6.6572e-03, -5.4737e-03,\n",
      "        -5.2805e-03,  1.0772e-02, -9.6076e-03, -4.8382e-03, -4.4192e-03,\n",
      "        -2.0151e-03,  9.3656e-03, -4.1239e-04, -3.0010e-03, -1.2432e-02,\n",
      "        -7.6654e-03,  1.0297e-02,  1.0828e-02,  8.5451e-04,  8.5574e-03,\n",
      "         2.2586e-03,  9.7317e-03, -3.5670e-03,  5.3622e-03,  1.4701e-03,\n",
      "        -1.3225e-02,  6.5341e-04,  5.4426e-03,  6.5854e-03,  1.3513e-02,\n",
      "        -1.2973e-02,  5.5535e-03,  1.1064e-02, -8.8265e-03, -1.8307e-03,\n",
      "        -8.9820e-03,  1.1772e-02, -7.2158e-03,  1.0078e-02, -1.4559e-02,\n",
      "         6.6436e-03,  5.9174e-03, -6.5715e-03, -1.0725e-02, -6.9630e-03,\n",
      "         1.0035e-02, -2.3657e-03, -1.0964e-02,  1.0779e-02,  9.4368e-04,\n",
      "         1.4720e-02,  4.6343e-03,  1.4466e-02, -1.2860e-02, -8.8880e-03,\n",
      "         1.4172e-02,  1.4126e-02,  9.6772e-03,  5.9545e-03,  2.6833e-03,\n",
      "         2.8268e-03,  1.1961e-02,  3.2284e-03, -1.2752e-02, -1.2962e-02,\n",
      "         1.1049e-02, -1.0915e-02,  7.0900e-04, -5.1425e-03, -5.1972e-03,\n",
      "         1.3245e-02,  5.6160e-03, -4.3190e-03,  6.2175e-03,  4.0538e-03,\n",
      "         2.4936e-03,  8.7042e-05,  2.0318e-05, -1.2433e-02, -1.0175e-02,\n",
      "        -7.7498e-03, -8.5263e-03, -1.0648e-02,  9.9828e-03, -5.7858e-03,\n",
      "         3.8584e-03,  8.6061e-03,  1.3306e-02, -1.1310e-02, -1.0412e-02,\n",
      "         1.0194e-02,  6.5201e-03,  1.4601e-02, -4.2934e-04, -1.4431e-02,\n",
      "         1.0284e-02, -1.9305e-03,  9.9015e-03,  4.1064e-03, -7.4343e-03,\n",
      "         1.2795e-02,  6.6004e-03,  7.7797e-03,  1.2748e-02,  1.3024e-02,\n",
      "         1.1349e-02, -4.8070e-03,  9.9124e-03,  1.2213e-02, -1.4706e-02,\n",
      "        -1.0974e-02, -1.3671e-02, -8.4236e-03, -8.3290e-03,  1.3509e-02,\n",
      "        -6.3432e-03, -1.4146e-02,  6.7011e-03, -6.8542e-03,  8.3136e-03,\n",
      "         7.9543e-03, -8.0232e-03, -8.4805e-03,  7.5118e-03,  1.0186e-02,\n",
      "        -1.0073e-03,  9.6844e-03,  4.4169e-03,  8.4589e-03,  1.1427e-02,\n",
      "        -1.1419e-03, -1.3404e-02, -4.7945e-04, -2.0723e-03, -2.7359e-03,\n",
      "         1.1756e-02,  7.9305e-03, -1.0805e-02,  6.1801e-03,  8.6720e-03,\n",
      "         5.4760e-03, -3.0642e-03,  9.2983e-03,  2.1435e-03,  9.3125e-03,\n",
      "         1.5753e-03, -2.8755e-03,  7.8167e-03, -8.1654e-03, -3.3734e-04,\n",
      "        -6.8223e-04, -5.5195e-03, -8.9993e-03, -1.0867e-02, -7.1313e-03,\n",
      "        -1.1492e-02, -1.1427e-02,  6.6959e-03, -1.0872e-02, -4.4127e-03,\n",
      "        -1.4020e-02,  1.0864e-03,  1.3827e-02, -1.1915e-02, -1.1127e-02,\n",
      "        -2.9082e-03,  3.5561e-03, -3.3063e-03,  8.5146e-03,  9.9396e-06,\n",
      "         7.9865e-03,  9.7709e-03, -7.4984e-03, -8.1169e-03, -8.6347e-03,\n",
      "         3.1856e-03,  6.6530e-03, -1.3582e-02, -1.2583e-02,  9.6625e-03,\n",
      "        -6.0676e-04,  2.9404e-03,  1.2988e-02, -1.3035e-03, -1.2481e-02,\n",
      "         6.6326e-03,  1.4235e-02,  1.9370e-04,  2.7450e-03,  4.3707e-03,\n",
      "        -1.3169e-02, -1.0045e-02,  5.4926e-03, -8.0911e-03, -1.4583e-02,\n",
      "        -1.1954e-02,  1.0369e-02, -4.4241e-03, -1.1244e-02,  1.0844e-02,\n",
      "         2.6647e-03,  4.2774e-03, -3.0725e-03, -1.0878e-02, -9.9914e-03,\n",
      "         2.3674e-03,  1.6288e-03,  7.6080e-03,  5.5499e-03,  4.8731e-03,\n",
      "         8.4742e-03, -7.2862e-03, -1.4564e-02,  1.1443e-02,  1.1796e-02,\n",
      "        -7.8289e-03])\n",
      "Layer: layer4.2.bn1.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: layer4.2.conv2.bias, Biases: tensor([ 1.9676e-02, -1.9132e-02,  1.1069e-02,  2.0015e-02,  1.8590e-03,\n",
      "         1.6306e-03,  1.5938e-03,  1.8004e-02,  7.7140e-03, -1.2875e-02,\n",
      "        -1.8989e-02,  1.6305e-02, -8.9685e-03,  1.2839e-02,  5.9523e-03,\n",
      "         4.6983e-03, -5.5649e-03,  6.9436e-03,  2.7493e-04,  2.0112e-02,\n",
      "        -1.9753e-02, -1.6472e-02,  9.0152e-04,  1.3232e-02, -5.3037e-03,\n",
      "        -4.6682e-03,  1.7773e-02, -1.7708e-02, -1.0816e-02, -1.2850e-02,\n",
      "        -4.5491e-03,  2.0809e-03,  7.1818e-03,  5.3426e-04,  1.2996e-02,\n",
      "         5.0701e-03,  2.0380e-03, -7.0342e-03,  8.2998e-04, -3.9411e-03,\n",
      "        -1.2916e-02,  1.6457e-02,  9.5291e-04,  7.2171e-03,  1.0698e-02,\n",
      "        -1.5331e-02,  2.0496e-03, -1.5722e-02, -1.1192e-02,  1.9181e-02,\n",
      "         1.6571e-02, -7.6390e-03, -1.7936e-03,  1.4673e-02,  1.6150e-02,\n",
      "         6.3749e-04,  3.2057e-03, -1.8343e-03, -1.9373e-02, -1.4114e-02,\n",
      "         1.0970e-02, -1.7302e-03,  1.2638e-02, -4.7656e-05,  2.2526e-03,\n",
      "        -7.0429e-03,  2.3888e-03, -6.0247e-03, -1.4658e-02, -1.4050e-02,\n",
      "        -9.2675e-03,  1.7701e-02,  2.4060e-03,  2.0544e-02, -1.6972e-02,\n",
      "        -1.0368e-02, -8.3638e-03, -6.4179e-03, -1.5351e-02,  3.6817e-03,\n",
      "         4.5902e-03,  1.8467e-02, -3.6965e-03, -1.4005e-02, -1.6961e-02,\n",
      "         8.2940e-03,  1.4960e-02, -2.3740e-03,  8.4103e-03,  1.1498e-02,\n",
      "         1.9593e-02,  6.1878e-03,  4.6711e-03, -1.4239e-02, -1.1959e-02,\n",
      "         9.1427e-03, -7.1469e-03,  7.6384e-03,  1.8201e-03, -1.1583e-03,\n",
      "         1.2150e-02, -2.0178e-02, -4.3117e-03,  7.5883e-03,  1.1834e-02,\n",
      "        -1.1640e-02,  1.1544e-02, -1.1933e-02, -1.9020e-02,  9.7383e-03,\n",
      "        -2.0133e-03,  1.9665e-02, -3.5888e-03,  2.0390e-03, -4.3218e-03,\n",
      "        -1.5398e-02,  1.6221e-02, -1.1678e-02, -9.3182e-03, -1.0004e-02,\n",
      "         1.1601e-02, -4.1868e-03,  9.2863e-03,  9.4348e-03, -7.1209e-03,\n",
      "         2.6140e-03,  4.7186e-03,  4.8087e-03,  5.2264e-03,  2.2278e-03,\n",
      "        -1.9203e-03, -2.0582e-02, -6.7119e-03, -1.2860e-05,  1.1684e-02,\n",
      "        -2.0154e-02,  5.6391e-03, -1.6779e-02,  3.5336e-03,  1.7969e-02,\n",
      "        -6.2027e-03,  1.7830e-02, -5.4155e-03, -1.0870e-02,  5.5656e-03,\n",
      "         1.5803e-02, -3.8117e-04,  8.9901e-03,  1.6352e-02,  4.5267e-03,\n",
      "         1.5811e-02,  1.9460e-02,  7.7137e-03,  1.2920e-02, -1.8580e-03,\n",
      "        -5.9031e-03,  3.2656e-03,  1.9296e-02, -6.5503e-04,  9.6576e-03,\n",
      "         1.2409e-02, -4.8426e-03,  2.7610e-03, -8.4659e-03,  4.2251e-03,\n",
      "        -1.7675e-02, -1.6851e-02,  1.9318e-03,  1.6925e-02, -1.8659e-02,\n",
      "         6.3621e-03,  1.0350e-02, -4.8945e-03,  1.3244e-02,  5.7373e-03,\n",
      "         1.4152e-02,  1.3590e-02, -1.5692e-03, -1.3938e-02,  3.0832e-03,\n",
      "        -9.7224e-03, -5.5564e-03, -1.6785e-02, -7.5485e-03, -7.2226e-03,\n",
      "        -1.8881e-02, -1.1846e-02,  1.9528e-02, -1.4358e-02,  6.1887e-03,\n",
      "         1.9481e-02, -9.7865e-03, -1.9353e-03, -1.7763e-02,  1.1641e-02,\n",
      "         1.0573e-02, -1.3732e-02,  1.3516e-02, -1.9373e-02, -1.6821e-02,\n",
      "        -6.1225e-03, -1.2213e-02, -7.3546e-03,  9.4954e-03,  1.1378e-03,\n",
      "        -1.3351e-03, -6.8740e-04, -1.3542e-02, -2.0438e-02,  1.5994e-02,\n",
      "        -1.7932e-02, -5.6519e-04,  1.3931e-02,  1.9589e-02,  5.8870e-03,\n",
      "        -1.5626e-02, -1.0555e-02,  9.5421e-03,  1.1102e-02, -1.4226e-02,\n",
      "         1.4018e-02, -4.4831e-03,  1.1314e-02,  1.9425e-02,  1.5914e-03,\n",
      "         1.4511e-02, -8.4878e-03,  7.7654e-03, -1.9972e-02,  4.4482e-03,\n",
      "        -1.3090e-02,  8.4041e-03,  1.3050e-02, -1.0565e-02, -4.1750e-03,\n",
      "         8.4134e-03,  2.0193e-02, -1.8733e-02, -1.3014e-02,  1.8955e-02,\n",
      "        -8.7161e-03,  6.9519e-03, -1.8161e-02, -1.8615e-02, -1.4363e-02,\n",
      "        -2.0188e-02, -3.9084e-03, -1.1382e-02,  1.3458e-02,  8.0871e-03,\n",
      "        -1.7825e-02, -1.3859e-02, -1.0042e-02,  6.8084e-03,  1.0842e-02,\n",
      "        -1.2485e-02, -1.8214e-02,  1.2897e-02, -1.8917e-02,  1.5330e-02,\n",
      "        -6.2920e-03, -3.2482e-04,  8.8173e-03, -9.2277e-03,  1.0951e-02,\n",
      "         2.0217e-02, -3.3509e-03, -1.8839e-02, -8.6774e-03,  1.9889e-02,\n",
      "         8.8177e-03, -7.2710e-03,  1.2150e-02,  1.7553e-03,  8.5406e-03,\n",
      "         2.0480e-02, -1.2960e-02, -1.5171e-02,  1.1974e-02,  1.2412e-02,\n",
      "         1.2388e-02, -1.0781e-02,  1.1342e-02,  1.2026e-02,  7.7685e-03,\n",
      "         5.2827e-03,  7.9564e-03,  1.5134e-03, -1.0119e-02,  2.4227e-03,\n",
      "         1.9521e-02,  1.9712e-02,  1.0870e-02,  5.3188e-03,  8.7529e-03,\n",
      "         1.6915e-02,  1.1224e-02, -4.1335e-03,  1.1307e-02, -8.9113e-03,\n",
      "         2.0265e-02, -1.3325e-02, -2.6089e-03,  1.1797e-02, -9.3126e-03,\n",
      "        -9.8561e-03,  1.1771e-02, -2.0627e-02,  4.2191e-03, -1.8053e-02,\n",
      "        -2.0277e-02,  8.1203e-03,  1.9934e-02, -8.1301e-03,  1.2967e-02,\n",
      "        -1.2033e-02,  9.0368e-03, -1.8620e-02, -1.5313e-02, -2.0548e-02,\n",
      "        -1.7864e-02, -1.9037e-03,  1.0928e-02, -1.2836e-02, -1.4980e-02,\n",
      "        -9.5913e-03,  1.1349e-03,  1.4689e-02,  9.3633e-03,  1.7616e-02,\n",
      "        -6.0883e-03,  6.0395e-04,  4.1950e-03, -9.1472e-03, -1.4024e-02,\n",
      "        -1.2507e-02, -5.3172e-03,  1.8124e-02,  1.3813e-02,  8.5361e-03,\n",
      "         1.7052e-02,  1.0071e-02,  2.0208e-02, -2.3965e-03,  1.7137e-02,\n",
      "         9.7978e-03, -9.1545e-03,  5.3260e-03, -3.9308e-03,  1.9629e-02,\n",
      "        -4.3704e-03, -4.9021e-03, -2.0209e-02,  8.1870e-03,  1.7602e-02,\n",
      "         9.6315e-03,  3.4292e-03, -1.7346e-02,  1.5424e-02, -9.5810e-04,\n",
      "         3.8639e-03,  7.2184e-03, -1.0248e-02,  5.1627e-04,  2.8095e-04,\n",
      "         1.4008e-02, -4.0049e-03,  1.8819e-02,  1.3732e-02,  1.0278e-02,\n",
      "        -1.9338e-02,  7.1009e-03,  1.2691e-04,  6.0651e-03,  1.9580e-02,\n",
      "        -1.7147e-02, -7.8351e-03, -1.4333e-02, -1.6092e-02,  3.6434e-03,\n",
      "         7.8077e-03,  5.6623e-03, -1.9759e-02, -1.3051e-02,  8.6858e-03,\n",
      "         1.7776e-02,  4.2477e-04, -3.5984e-03,  7.8574e-04, -2.0703e-02,\n",
      "        -2.6894e-03, -5.0318e-04,  1.6213e-02,  1.4290e-02,  2.0771e-03,\n",
      "         7.0437e-03,  1.0242e-02, -1.4219e-02,  1.5681e-02,  2.0438e-02,\n",
      "         3.2626e-03, -1.6024e-02,  6.0750e-03,  1.9675e-02, -1.8078e-03,\n",
      "        -2.0110e-02, -5.3582e-03,  2.0422e-02,  1.2561e-02,  1.2273e-02,\n",
      "         2.0426e-02, -1.1573e-02, -1.7763e-02, -2.0797e-02,  8.9059e-03,\n",
      "         8.9929e-03, -1.4221e-02,  7.9976e-04, -1.6091e-02, -6.4666e-03,\n",
      "        -7.5963e-03, -3.8970e-03,  1.9371e-02,  1.4322e-02,  1.7688e-02,\n",
      "         1.2319e-02,  8.5255e-03, -1.5005e-02, -5.0069e-03,  5.0722e-03,\n",
      "         1.1086e-03,  1.0181e-02, -5.4396e-03,  9.4542e-03, -3.5443e-03,\n",
      "         1.6708e-03, -6.8464e-03,  8.5191e-03, -1.1110e-02,  1.2166e-02,\n",
      "         9.2774e-03, -1.9123e-02, -9.0813e-03, -2.7920e-03, -2.0394e-02,\n",
      "         9.2952e-03, -1.3572e-02,  2.0720e-02,  1.1542e-02, -7.7217e-04,\n",
      "         3.9156e-03,  1.4967e-02, -4.7084e-03, -9.4954e-03,  1.9929e-02,\n",
      "         1.1252e-04,  9.1381e-03,  9.8932e-03, -3.7219e-03,  1.8853e-02,\n",
      "         9.0345e-03,  1.4199e-02, -1.6056e-02,  8.5559e-03,  7.9940e-05,\n",
      "        -7.7756e-03, -1.0710e-02,  1.7670e-02, -8.7790e-03, -6.6371e-03,\n",
      "         7.6036e-03,  1.3414e-02, -1.0780e-03,  9.6259e-03, -1.3659e-02,\n",
      "         5.1409e-03, -1.9122e-02,  5.6131e-04,  1.9030e-02,  1.4086e-02,\n",
      "        -1.4515e-02,  1.4358e-02, -1.8574e-02, -2.0166e-02, -1.6251e-03,\n",
      "         3.8393e-03, -8.5676e-03, -1.7384e-02,  1.8879e-02,  1.6862e-02,\n",
      "         5.5954e-03, -1.0047e-02, -1.0624e-02,  2.0056e-02, -8.3059e-03,\n",
      "         1.0325e-02, -1.0289e-02,  6.3456e-03,  1.8034e-02, -1.2089e-02,\n",
      "         1.0874e-02, -9.4900e-03, -1.1823e-02, -1.5061e-02,  1.9269e-02,\n",
      "        -1.1768e-02, -1.6916e-02,  1.9984e-02,  2.8954e-03,  1.6810e-02,\n",
      "         1.9568e-02, -1.9695e-02])\n",
      "Layer: layer4.2.bn2.bias, Biases: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Layer: KANClassifier.1.fouriercoeffs, Fourier Coefficients:\n",
      "tensor([[[[ 6.3978e-05,  2.3308e-04, -9.8701e-05,  ...,  9.4420e-05,\n",
      "            1.8240e-04, -1.1672e-04],\n",
      "          [ 1.3392e-04, -1.2945e-04,  5.5385e-04,  ..., -6.5121e-05,\n",
      "           -5.5909e-04, -1.1596e-04],\n",
      "          [-2.8131e-04, -4.4702e-04,  2.5195e-05,  ..., -3.2260e-04,\n",
      "            1.5400e-04, -6.4543e-04],\n",
      "          ...,\n",
      "          [ 3.1799e-04, -4.5881e-05,  3.6122e-04,  ..., -1.8465e-04,\n",
      "            2.4467e-04, -4.3425e-04],\n",
      "          [-6.2980e-04, -1.4404e-05, -3.9743e-05,  ...,  1.5673e-04,\n",
      "           -2.3028e-04, -5.8721e-05],\n",
      "          [ 4.3111e-04, -4.4426e-04,  2.6224e-04,  ...,  1.5621e-04,\n",
      "           -3.2919e-04, -5.6566e-04]],\n",
      "\n",
      "         [[ 4.9611e-04, -3.6887e-04,  3.2105e-04,  ...,  6.7401e-04,\n",
      "            5.3724e-04, -2.9038e-04],\n",
      "          [ 3.0975e-04,  3.2453e-04,  5.2635e-04,  ..., -5.4016e-04,\n",
      "           -6.4446e-04, -6.5777e-04],\n",
      "          [-6.6132e-05, -6.3281e-04, -4.7097e-04,  ...,  2.1075e-04,\n",
      "           -1.1257e-04,  4.3876e-04],\n",
      "          ...,\n",
      "          [-5.9296e-04, -5.6050e-04,  1.1641e-04,  ...,  3.6884e-04,\n",
      "            2.9400e-04, -6.2494e-04],\n",
      "          [ 2.3180e-04, -3.5406e-05, -8.3096e-05,  ..., -3.7477e-04,\n",
      "            4.2182e-04,  9.5085e-05],\n",
      "          [ 3.9024e-04, -1.5111e-04, -5.1184e-04,  ...,  1.8754e-04,\n",
      "            6.1071e-04,  1.2653e-04]],\n",
      "\n",
      "         [[ 5.8536e-04, -3.4328e-04,  6.0701e-04,  ...,  5.9528e-05,\n",
      "           -1.2853e-04,  2.0627e-04],\n",
      "          [ 2.8984e-05, -1.9066e-04, -2.0959e-05,  ...,  2.2694e-04,\n",
      "           -3.8272e-04, -4.8148e-04],\n",
      "          [-1.5598e-04,  6.1480e-04,  5.6431e-04,  ..., -3.5748e-04,\n",
      "           -5.3492e-05,  5.1331e-04],\n",
      "          ...,\n",
      "          [-5.2646e-04,  5.6417e-04, -4.7598e-04,  ..., -1.8709e-04,\n",
      "            6.7250e-04, -1.4324e-04],\n",
      "          [ 1.2058e-04,  5.7980e-04,  3.2335e-04,  ..., -4.2249e-04,\n",
      "            4.8160e-04, -2.7869e-04],\n",
      "          [-3.7331e-04, -6.3885e-04, -6.6226e-04,  ..., -5.8061e-05,\n",
      "           -5.9209e-04,  5.0145e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7509e-04,  2.7499e-04, -1.7304e-04,  ...,  6.4367e-04,\n",
      "           -2.4284e-04,  6.4076e-04],\n",
      "          [-2.7080e-04,  4.5046e-04,  6.4417e-04,  ...,  4.1192e-04,\n",
      "            1.1821e-04, -4.9468e-04],\n",
      "          [-2.7057e-04, -1.3019e-04, -3.6537e-05,  ..., -5.5924e-04,\n",
      "            4.1405e-04, -1.9050e-05],\n",
      "          ...,\n",
      "          [ 1.3379e-04, -2.7424e-04, -5.8416e-04,  ..., -4.7216e-04,\n",
      "           -2.3165e-04, -1.4166e-04],\n",
      "          [-3.1121e-04, -5.6394e-04, -4.5055e-07,  ...,  4.6593e-04,\n",
      "            1.6324e-04,  4.3871e-04],\n",
      "          [-2.0093e-04,  1.1646e-04,  2.1191e-04,  ..., -8.4016e-05,\n",
      "           -3.4356e-05, -1.2219e-04]],\n",
      "\n",
      "         [[-6.4735e-04, -3.5905e-04,  1.0860e-04,  ...,  5.6365e-04,\n",
      "           -3.4209e-04,  6.4380e-04],\n",
      "          [-2.0044e-04,  4.4684e-04,  3.1241e-04,  ..., -3.6885e-04,\n",
      "            6.6229e-04, -6.5454e-06],\n",
      "          [ 2.1440e-04,  5.6698e-04, -6.2749e-04,  ..., -6.5724e-05,\n",
      "           -2.9382e-04,  4.5778e-04],\n",
      "          ...,\n",
      "          [-3.1274e-04, -1.3124e-04, -6.4122e-04,  ..., -5.8382e-04,\n",
      "           -5.1082e-04, -1.2183e-04],\n",
      "          [ 2.4910e-04,  1.1692e-04, -6.3411e-04,  ..., -1.7068e-04,\n",
      "           -6.2492e-04,  1.4144e-04],\n",
      "          [ 5.3576e-04, -5.9994e-04,  4.5176e-04,  ..., -2.4376e-04,\n",
      "            4.9680e-04,  4.1812e-04]],\n",
      "\n",
      "         [[ 4.3484e-04,  4.0708e-04, -4.4372e-05,  ...,  8.4906e-05,\n",
      "           -9.4596e-05,  8.9246e-05],\n",
      "          [ 6.0124e-04, -7.1734e-05, -5.3099e-04,  ...,  6.1247e-04,\n",
      "            2.1987e-04, -1.0428e-04],\n",
      "          [-5.4162e-04, -4.9251e-05, -2.8000e-04,  ..., -6.5655e-04,\n",
      "            2.2631e-04, -3.9827e-04],\n",
      "          ...,\n",
      "          [-3.6330e-04,  5.4121e-04,  4.6446e-04,  ...,  3.7555e-04,\n",
      "            5.4723e-04, -6.3963e-04],\n",
      "          [-2.4627e-04, -9.4020e-05,  6.3640e-04,  ...,  3.2385e-04,\n",
      "           -3.2253e-05, -1.8732e-04],\n",
      "          [ 3.9729e-04,  6.9838e-05, -5.3346e-04,  ..., -1.7785e-04,\n",
      "           -2.4302e-04, -2.4889e-05]]],\n",
      "\n",
      "\n",
      "        [[[-6.6866e-04, -2.2606e-04,  4.3154e-04,  ..., -1.6320e-04,\n",
      "            1.1883e-04,  5.9185e-04],\n",
      "          [-2.6806e-04,  3.5762e-04,  4.2095e-04,  ...,  4.9412e-04,\n",
      "            2.8365e-04,  5.4495e-04],\n",
      "          [ 3.3020e-04, -5.0262e-04,  5.0745e-04,  ...,  4.8513e-04,\n",
      "            4.6908e-04, -4.9786e-04],\n",
      "          ...,\n",
      "          [-5.2886e-04,  5.9009e-04,  3.6193e-04,  ...,  4.1052e-04,\n",
      "           -3.6436e-04,  6.7166e-04],\n",
      "          [-5.5136e-04, -4.2658e-04,  5.3271e-05,  ...,  2.2488e-04,\n",
      "            6.6602e-04,  6.6599e-04],\n",
      "          [-4.8954e-05, -6.6455e-04, -6.0025e-04,  ...,  3.5366e-05,\n",
      "            3.1346e-04,  4.4194e-04]],\n",
      "\n",
      "         [[-8.9616e-06,  5.3512e-04, -4.0650e-04,  ..., -7.2851e-05,\n",
      "            1.9309e-05,  5.3098e-04],\n",
      "          [ 2.7624e-04, -2.0725e-04, -5.3757e-05,  ..., -1.8412e-04,\n",
      "            3.4977e-04, -5.2076e-04],\n",
      "          [-2.3514e-04,  4.3317e-04, -5.2021e-04,  ...,  3.4043e-04,\n",
      "            1.8995e-04,  6.0746e-04],\n",
      "          ...,\n",
      "          [-6.3680e-04, -5.8644e-04, -4.5065e-04,  ..., -4.2386e-05,\n",
      "           -3.2916e-04,  3.5397e-05],\n",
      "          [ 5.2510e-04, -5.6667e-04,  2.1614e-04,  ..., -5.6965e-04,\n",
      "            2.7662e-05,  5.2557e-04],\n",
      "          [-4.1360e-04, -6.2944e-04,  1.8772e-05,  ...,  8.7325e-05,\n",
      "            3.3468e-04,  2.8983e-04]],\n",
      "\n",
      "         [[ 1.2027e-04, -5.4781e-05, -6.6811e-04,  ...,  2.1439e-04,\n",
      "            1.9955e-04,  4.5803e-04],\n",
      "          [-1.0527e-04,  2.3150e-04, -3.1792e-04,  ...,  3.2250e-04,\n",
      "            1.2219e-04,  6.2686e-04],\n",
      "          [-4.3287e-04, -1.0488e-04, -6.2663e-04,  ..., -9.9573e-05,\n",
      "           -3.5598e-04, -4.6297e-04],\n",
      "          ...,\n",
      "          [ 3.9579e-05,  6.5774e-04, -1.8289e-04,  ..., -6.0231e-04,\n",
      "           -6.3206e-04,  5.6951e-04],\n",
      "          [-2.4247e-04,  8.9606e-05,  1.4905e-06,  ...,  2.4967e-04,\n",
      "            1.7666e-04,  1.4960e-04],\n",
      "          [ 3.2977e-04,  4.3391e-04, -1.9410e-04,  ..., -1.9758e-04,\n",
      "            3.0377e-04, -6.2353e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.4455e-04, -4.5616e-04, -4.5219e-04,  ...,  6.8155e-06,\n",
      "            3.7631e-04,  1.1513e-04],\n",
      "          [-6.2376e-04,  4.8252e-04,  4.3154e-04,  ...,  5.1337e-04,\n",
      "           -5.6500e-05, -2.8315e-04],\n",
      "          [ 2.8528e-04, -3.5591e-04, -2.9911e-04,  ...,  8.7182e-05,\n",
      "            1.9612e-04,  4.9840e-04],\n",
      "          ...,\n",
      "          [ 3.8054e-04,  5.6263e-04, -8.6080e-05,  ..., -4.8665e-04,\n",
      "            5.5413e-05,  1.9101e-04],\n",
      "          [ 2.3737e-04, -3.9530e-04,  4.6587e-04,  ..., -6.5418e-04,\n",
      "           -1.8492e-04,  5.5093e-05],\n",
      "          [ 7.8088e-05,  2.0966e-04, -4.3057e-04,  ..., -4.1414e-04,\n",
      "            2.9659e-04, -6.0115e-05]],\n",
      "\n",
      "         [[ 5.7259e-05, -4.5442e-06, -4.5211e-04,  ..., -6.0786e-04,\n",
      "           -4.0574e-04,  3.3679e-04],\n",
      "          [-3.9212e-04,  3.7185e-04,  3.0759e-04,  ...,  4.7378e-04,\n",
      "            1.8712e-04,  1.9966e-04],\n",
      "          [ 6.4649e-05,  3.5609e-04,  5.4497e-05,  ..., -1.0952e-04,\n",
      "            2.9492e-04,  6.1537e-04],\n",
      "          ...,\n",
      "          [ 1.8113e-04, -2.0548e-04,  4.1656e-04,  ..., -1.4405e-04,\n",
      "           -2.4698e-04, -3.2382e-04],\n",
      "          [-9.1607e-05,  5.3634e-04,  4.2993e-04,  ..., -4.5429e-04,\n",
      "           -6.6248e-04, -2.4402e-04],\n",
      "          [-7.8688e-05, -8.5138e-05, -5.3537e-04,  ...,  5.8810e-04,\n",
      "            5.6498e-04, -3.6014e-04]],\n",
      "\n",
      "         [[ 3.2668e-04,  2.6296e-05,  3.6885e-04,  ...,  2.9996e-04,\n",
      "           -2.2319e-04, -4.2542e-05],\n",
      "          [ 4.4677e-04, -2.2727e-04, -2.4142e-04,  ...,  5.7291e-04,\n",
      "            8.7014e-05,  3.1593e-04],\n",
      "          [ 4.2042e-04, -1.1396e-04,  5.0536e-04,  ..., -2.8122e-04,\n",
      "            2.5336e-04,  5.7375e-04],\n",
      "          ...,\n",
      "          [-4.9490e-07, -6.3284e-04, -1.1826e-04,  ..., -4.6002e-05,\n",
      "            1.7219e-06, -1.8045e-04],\n",
      "          [ 6.1537e-05, -2.5966e-04,  6.1978e-04,  ...,  9.2962e-05,\n",
      "            4.8513e-04, -3.2694e-04],\n",
      "          [-6.4580e-04,  1.4274e-04, -5.0458e-04,  ..., -3.9644e-06,\n",
      "           -1.5495e-04,  5.7504e-04]]]])\n",
      "\n",
      "Layer: KANClassifier.1.bias, Biases: tensor([[ 2.9345e-02,  4.5927e-02,  1.8174e-02, -3.4845e-02,  3.8175e-02,\n",
      "         -4.5270e-02,  2.7121e-02, -1.3747e-01, -2.0378e-02, -8.5368e-02,\n",
      "         -2.9900e-02, -6.4528e-02, -8.4944e-03, -9.7197e-02,  2.0515e-02,\n",
      "         -3.7362e-02,  1.3610e-02,  5.2184e-02, -7.0279e-02, -3.2340e-03,\n",
      "          6.3943e-02,  2.6575e-02, -2.1395e-02, -3.6436e-02,  8.4798e-03,\n",
      "         -5.3878e-02,  9.0245e-02,  6.0223e-03, -1.1406e-01,  4.5293e-02,\n",
      "         -5.5604e-02, -2.8784e-02, -7.3753e-02, -2.6227e-02, -3.0062e-02,\n",
      "          8.5129e-02, -5.4364e-02,  7.2302e-03,  4.5771e-03,  8.7095e-03,\n",
      "         -2.8587e-02,  3.2481e-02,  4.0841e-02, -2.1795e-02,  2.2172e-02,\n",
      "         -1.2222e-01,  6.9286e-02,  1.9375e-02,  5.8190e-02,  6.1677e-02,\n",
      "         -3.0314e-02,  4.8765e-02,  9.9441e-02, -2.9069e-02, -2.0435e-02,\n",
      "          5.9093e-02,  2.5902e-02,  3.0480e-02, -2.3986e-02,  6.6327e-02,\n",
      "          1.1667e-01, -8.4043e-02,  6.3448e-02, -4.2561e-02, -1.0809e-01,\n",
      "          4.4825e-03,  1.2274e-01, -1.4435e-01, -6.4021e-02, -7.6348e-02,\n",
      "          4.4705e-02,  2.5559e-02,  2.2742e-02, -8.0274e-03, -6.1441e-02,\n",
      "          9.7887e-02,  8.3478e-02, -5.2031e-02,  1.1474e-01,  6.1990e-03,\n",
      "          6.4087e-03, -3.4630e-02,  3.7585e-02,  8.9173e-02,  1.8234e-02,\n",
      "         -7.4944e-02,  1.3921e-01,  7.8469e-02,  1.2300e-03, -5.7890e-02,\n",
      "         -4.1320e-02, -1.8497e-02, -4.1325e-02, -2.2524e-02,  3.3884e-02,\n",
      "         -4.4409e-02,  1.0313e-01, -8.4253e-02, -6.4802e-02,  2.7720e-03,\n",
      "         -4.4916e-02, -2.0568e-02, -9.1503e-02, -1.3777e-02, -1.8702e-02,\n",
      "          1.5963e-01,  1.2364e-02,  6.1077e-02,  8.2233e-02, -6.0106e-02,\n",
      "          2.1964e-02, -1.4016e-01, -9.5651e-03,  2.0603e-03,  1.2730e-01,\n",
      "          9.4696e-02,  5.5195e-02, -1.3497e-02, -2.7087e-03, -7.2618e-03,\n",
      "         -5.0458e-02, -6.1726e-02,  1.1409e-02, -3.1942e-02,  6.9332e-03,\n",
      "          7.5269e-02,  8.4429e-02, -3.1253e-02, -9.0497e-02, -1.5132e-02,\n",
      "          4.9475e-02,  1.9625e-02,  5.8063e-02, -1.0958e-01, -1.0123e-01,\n",
      "         -3.0826e-02,  3.1382e-02,  7.2859e-02,  8.0211e-02, -4.0762e-02,\n",
      "         -5.2809e-02,  5.3243e-02,  5.3090e-02,  1.4405e-01, -8.3589e-02,\n",
      "          2.3303e-02, -1.1849e-02, -9.5192e-02,  1.0765e-02, -4.5323e-03,\n",
      "          3.9906e-02,  5.3557e-02, -5.7552e-02, -1.4623e-02,  7.2623e-02,\n",
      "          7.7427e-02, -1.3403e-02,  6.9595e-02, -7.6972e-03, -9.3355e-02,\n",
      "         -9.3299e-03, -8.3011e-02,  4.9191e-02, -5.4260e-02, -8.1185e-02,\n",
      "         -7.6385e-02, -9.8201e-02,  8.2769e-02, -4.0024e-02, -1.7348e-01,\n",
      "          1.8204e-02,  9.5200e-03, -4.0402e-02, -1.0340e-01, -9.0174e-02,\n",
      "          1.3141e-02,  4.3798e-02,  3.6781e-02,  1.1343e-02, -6.8694e-02,\n",
      "         -4.0584e-02, -5.3160e-02,  2.6189e-02,  1.3112e-02,  6.3966e-02,\n",
      "         -1.1870e-01,  1.1940e-04, -4.3978e-03, -1.7964e-02,  6.8538e-02,\n",
      "         -1.2528e-01, -3.1572e-02,  9.1733e-02,  5.5933e-02,  8.1966e-04,\n",
      "          4.7466e-02,  1.0476e-01, -6.4039e-02, -1.1145e-03, -1.9113e-02,\n",
      "         -9.6173e-02, -6.9087e-03, -8.7138e-02, -5.1184e-02,  4.8113e-02,\n",
      "          1.0064e-02,  3.0523e-02,  7.9112e-02, -1.0308e-02, -3.5956e-02,\n",
      "         -1.7363e-01, -5.8750e-02,  2.4071e-02, -8.0685e-02,  5.1673e-02,\n",
      "          4.7619e-04, -6.4606e-02,  8.7935e-02,  7.8069e-02,  3.1253e-02,\n",
      "          2.9714e-02,  5.6935e-02, -5.0922e-02, -2.7804e-02,  2.1020e-02,\n",
      "          1.7635e-01, -2.2509e-02, -3.9153e-02, -1.6774e-02, -5.3692e-02,\n",
      "         -3.6004e-02,  8.4247e-02,  4.3878e-02,  1.3890e-02,  4.1276e-02,\n",
      "          2.9123e-02,  3.6138e-02,  1.1296e-01, -1.0378e-03,  3.7267e-02,\n",
      "          7.9759e-02,  1.5415e-02, -7.7985e-02,  2.2899e-02, -1.6872e-01,\n",
      "         -4.4854e-02, -1.3102e-02,  1.2765e-01,  7.2075e-02,  3.5592e-03,\n",
      "          9.4486e-03, -8.8053e-02, -5.4100e-02, -1.9269e-02, -1.8411e-02,\n",
      "         -3.3812e-02, -1.0123e-02,  2.6583e-03,  6.5111e-02, -1.7934e-02,\n",
      "          9.7632e-02,  4.6777e-02, -3.9026e-03,  2.6349e-02, -7.5671e-02,\n",
      "          7.4151e-02,  4.4457e-02,  1.9298e-01, -7.9133e-02, -2.8129e-02,\n",
      "         -4.5496e-02,  1.4558e-01,  8.8295e-02,  7.2369e-02,  4.1542e-02,\n",
      "          5.0963e-02,  1.5654e-01,  5.6362e-02,  4.4522e-02, -4.1116e-02,\n",
      "          6.8426e-02, -4.8655e-02, -3.2206e-02,  3.8944e-02,  3.2581e-02,\n",
      "         -6.0510e-02, -3.9915e-02, -4.8217e-02,  7.3811e-02, -1.7349e-03,\n",
      "         -7.4416e-02, -7.4600e-02, -4.5503e-02, -7.9725e-02, -2.7587e-02,\n",
      "         -1.7229e-02, -4.3888e-02,  1.3863e-02, -4.9175e-02, -1.8320e-02,\n",
      "         -7.0595e-02, -3.5696e-02, -6.7897e-02,  4.2046e-02,  6.6403e-02,\n",
      "         -4.5590e-03, -2.0511e-02, -4.6554e-02,  9.2445e-02, -3.4970e-02,\n",
      "          4.9407e-02, -1.3833e-02, -2.7854e-02, -1.8578e-01,  7.8327e-02,\n",
      "          2.9545e-02,  5.6224e-02,  6.0495e-02,  1.6652e-02,  8.5692e-03,\n",
      "         -9.3814e-02, -1.9426e-03,  1.0706e-01,  1.1214e-01, -1.0251e-01,\n",
      "         -2.1785e-02,  5.8435e-02, -4.0059e-02,  4.3032e-02,  4.7540e-02,\n",
      "          2.4509e-02, -1.0579e-01,  3.9690e-02, -1.4435e-03,  1.9559e-03,\n",
      "         -3.6301e-02,  8.9292e-02, -5.0972e-02,  7.9288e-02,  1.4508e-02,\n",
      "         -4.2885e-02, -8.3662e-04, -5.6733e-02,  3.5398e-02,  1.3654e-02,\n",
      "          7.5589e-02, -1.7096e-01,  1.0367e-01, -4.8127e-03,  5.9026e-02,\n",
      "         -4.9126e-02,  2.1688e-02, -7.9966e-02, -1.9821e-02,  9.1160e-02,\n",
      "          4.3785e-02, -1.2950e-02,  3.4965e-02, -7.0492e-02,  9.8807e-02,\n",
      "          7.0114e-02,  6.3621e-02,  5.7891e-02, -3.8800e-02, -8.6599e-02,\n",
      "         -4.9352e-02,  7.9063e-02,  1.9872e-02, -5.1656e-02,  4.2468e-02,\n",
      "         -5.7921e-04,  9.4490e-02, -1.3078e-01, -2.4700e-02, -5.6174e-02,\n",
      "         -9.4360e-03, -1.6137e-02,  3.1529e-02,  6.2451e-02,  2.7853e-02,\n",
      "          1.7787e-01,  7.1190e-02,  8.5578e-02, -2.9378e-02, -3.7037e-02,\n",
      "          2.4510e-02,  5.7540e-02,  2.7019e-03,  6.6580e-03,  4.3247e-02,\n",
      "          1.7246e-02, -4.0317e-02,  2.1970e-02,  8.1854e-02,  5.3865e-03,\n",
      "         -2.7786e-03,  7.3924e-02,  6.2093e-02, -5.6462e-02,  7.9899e-04,\n",
      "         -5.8764e-02, -1.6699e-01,  9.3763e-03, -2.6812e-02,  9.0591e-02,\n",
      "          9.4361e-03, -1.5192e-01, -1.2284e-01, -1.4658e-01, -1.2188e-02,\n",
      "          5.4110e-02,  3.7468e-02,  6.5100e-03,  1.1516e-02,  2.1465e-02,\n",
      "         -4.5362e-02,  2.2087e-03, -4.1703e-03, -1.4195e-01, -1.0166e-01,\n",
      "         -5.8586e-03,  8.8601e-02,  7.7644e-03, -6.2691e-02,  3.6711e-02,\n",
      "         -7.6034e-02, -2.6550e-02, -1.4182e-01, -1.2829e-04, -4.2682e-03,\n",
      "          7.1095e-03, -8.7476e-02,  2.4215e-02,  1.0467e-03,  2.6848e-02,\n",
      "          9.8180e-03, -8.9399e-02, -1.9000e-03,  1.8403e-02,  1.7904e-03,\n",
      "          2.8257e-02, -6.9779e-02, -3.9710e-02,  1.2547e-01, -6.8353e-02,\n",
      "          2.3740e-02, -7.3415e-02, -2.0660e-02,  2.6375e-02,  3.2113e-02,\n",
      "         -7.4222e-03,  1.2811e-01, -2.1666e-03, -6.1181e-02,  4.4989e-02,\n",
      "          4.2366e-02,  1.4421e-01, -1.1843e-01,  4.2329e-02,  1.9983e-02,\n",
      "         -3.8463e-02,  1.0780e-02,  6.5939e-02, -9.1845e-03, -3.7742e-02,\n",
      "          2.0942e-02,  3.1457e-02, -2.3460e-02,  7.9716e-02, -1.8558e-02,\n",
      "          2.8565e-02, -7.4567e-02,  7.1220e-02, -5.1303e-02,  6.7456e-02,\n",
      "          4.7114e-02,  4.9159e-02, -8.9077e-02,  2.3860e-02, -8.7548e-02,\n",
      "          2.5866e-02,  1.5081e-01,  1.0869e-01, -8.4262e-02, -7.2414e-02,\n",
      "         -6.6100e-02, -1.1661e-01,  1.1894e-02,  8.2102e-02, -7.1157e-02,\n",
      "          2.4316e-02, -1.2996e-02,  2.2831e-02, -7.0337e-02, -2.6865e-03,\n",
      "         -9.6952e-02,  4.2466e-02,  9.0263e-03, -7.8650e-02, -2.0605e-02,\n",
      "          1.4043e-01,  5.0659e-03, -7.2181e-02,  1.0737e-01,  8.5096e-02,\n",
      "          1.0082e-01,  3.8351e-02, -8.5218e-02, -1.6554e-02,  1.1954e-02,\n",
      "          8.9649e-02,  4.9449e-03]])\n",
      "Layer: KANClassifier.2.fouriercoeffs, Fourier Coefficients:\n",
      "tensor([[[[-3.7025e-05,  4.2193e-04,  1.0916e-03,  ...,  9.8317e-04,\n",
      "            1.4730e-03,  1.7936e-03],\n",
      "          [-1.1711e-03,  1.8440e-03, -2.8752e-03,  ..., -2.8652e-03,\n",
      "           -8.4842e-04,  1.2675e-04],\n",
      "          [ 2.3464e-03, -2.0497e-03, -1.4948e-03,  ..., -2.5454e-04,\n",
      "           -2.3492e-03,  2.4235e-03],\n",
      "          ...,\n",
      "          [-2.1143e-03, -7.0433e-04, -2.5486e-03,  ...,  8.4482e-04,\n",
      "           -1.9266e-03, -2.5195e-03],\n",
      "          [ 7.2254e-04, -1.6174e-03, -8.7543e-04,  ..., -1.1229e-03,\n",
      "           -1.4425e-03,  8.1729e-04],\n",
      "          [ 8.0328e-04,  2.4050e-03,  2.7386e-03,  ..., -4.0800e-04,\n",
      "           -1.8325e-03,  2.3070e-03]],\n",
      "\n",
      "         [[ 1.2693e-03,  3.3693e-04,  1.2309e-03,  ...,  1.6530e-03,\n",
      "           -2.8322e-03,  2.8147e-03],\n",
      "          [ 9.6566e-04,  1.5538e-03,  1.8957e-03,  ..., -2.1112e-03,\n",
      "           -1.5560e-03,  2.5962e-03],\n",
      "          [ 1.0886e-03, -2.7926e-03,  1.8338e-03,  ...,  1.0991e-03,\n",
      "           -1.4837e-03,  5.2438e-05],\n",
      "          ...,\n",
      "          [-7.5158e-04,  1.4736e-03,  2.3647e-03,  ..., -1.2913e-03,\n",
      "            8.2947e-04,  1.9992e-03],\n",
      "          [ 5.9014e-04,  1.9666e-04, -7.2985e-04,  ...,  2.3212e-04,\n",
      "           -6.6431e-04, -1.5550e-03],\n",
      "          [-2.5118e-03, -2.6970e-03,  1.0420e-03,  ...,  1.5113e-03,\n",
      "           -2.5736e-03, -2.3828e-03]],\n",
      "\n",
      "         [[ 1.7299e-03, -2.8828e-03,  4.1956e-04,  ..., -1.9211e-03,\n",
      "           -2.0445e-04,  6.2442e-04],\n",
      "          [ 6.1339e-04,  8.4458e-04,  1.4137e-04,  ...,  2.3703e-03,\n",
      "            2.7799e-03, -2.6379e-04],\n",
      "          [-9.7621e-04, -3.2882e-04, -1.0821e-03,  ...,  1.7894e-03,\n",
      "            1.8642e-03,  2.6878e-03],\n",
      "          ...,\n",
      "          [ 1.6497e-03,  3.9329e-04, -2.0815e-03,  ...,  2.5312e-03,\n",
      "           -2.3490e-03,  1.6308e-03],\n",
      "          [-2.7784e-03,  1.6630e-03, -1.3149e-03,  ...,  2.5564e-03,\n",
      "           -1.8634e-03, -1.2820e-03],\n",
      "          [ 1.4347e-03,  4.5415e-05,  1.1068e-04,  ..., -2.5497e-04,\n",
      "            2.4832e-04, -2.4637e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.6740e-04,  2.5613e-03, -2.5227e-03,  ..., -2.2438e-03,\n",
      "            2.2662e-03, -2.7027e-03],\n",
      "          [ 3.1225e-04,  1.2022e-03,  4.6324e-04,  ..., -2.5094e-03,\n",
      "           -6.5084e-04, -1.6402e-03],\n",
      "          [ 9.5584e-04,  2.8950e-04,  1.9001e-03,  ..., -1.3547e-03,\n",
      "           -2.2741e-03,  2.1905e-03],\n",
      "          ...,\n",
      "          [-1.8356e-03, -6.7997e-04, -1.4204e-03,  ...,  4.6023e-04,\n",
      "           -2.5933e-04,  2.1775e-03],\n",
      "          [ 1.4647e-03,  2.0820e-03, -2.8507e-03,  ..., -1.5274e-03,\n",
      "            1.8462e-03,  1.9052e-03],\n",
      "          [-1.9049e-03,  4.0393e-04,  1.2227e-03,  ...,  6.4788e-04,\n",
      "           -1.6674e-04,  2.3765e-03]],\n",
      "\n",
      "         [[ 1.9046e-03, -2.2614e-03,  5.8736e-04,  ..., -2.8249e-03,\n",
      "           -1.9768e-03,  1.8995e-03],\n",
      "          [-1.2708e-03,  2.5963e-03, -2.4375e-03,  ..., -1.1968e-03,\n",
      "            1.5143e-04, -2.0481e-03],\n",
      "          [-8.8019e-04,  1.3210e-03, -1.1469e-03,  ...,  1.4403e-03,\n",
      "           -2.6063e-03,  2.1943e-03],\n",
      "          ...,\n",
      "          [ 1.5456e-03,  2.4739e-03,  2.1160e-03,  ..., -2.0426e-03,\n",
      "            1.3188e-03, -1.4460e-03],\n",
      "          [ 4.8836e-04,  2.0623e-03, -2.0755e-03,  ...,  2.3038e-04,\n",
      "            2.5669e-03,  1.6451e-03],\n",
      "          [ 1.3551e-03,  2.8780e-03, -2.8027e-04,  ..., -2.2404e-03,\n",
      "           -1.3244e-03,  2.0700e-03]],\n",
      "\n",
      "         [[-8.5578e-04,  2.0997e-03, -1.6471e-03,  ..., -1.0593e-03,\n",
      "            7.5508e-04, -4.9342e-04],\n",
      "          [ 1.7926e-03,  2.5524e-03,  2.6851e-03,  ...,  6.3528e-04,\n",
      "            4.3616e-04, -2.7253e-03],\n",
      "          [ 1.7852e-03, -1.0447e-03, -8.4558e-04,  ...,  1.1833e-03,\n",
      "           -2.0973e-03, -2.0240e-03],\n",
      "          ...,\n",
      "          [-2.0971e-03, -1.6054e-03, -1.8843e-03,  ..., -7.2369e-04,\n",
      "           -2.5222e-03,  2.6562e-03],\n",
      "          [-2.1991e-03,  2.5946e-03,  9.4681e-04,  ..., -2.4600e-04,\n",
      "            1.8596e-03,  1.7900e-03],\n",
      "          [ 2.2311e-04,  4.7352e-04,  2.6405e-03,  ..., -1.0647e-03,\n",
      "           -1.2188e-03,  2.1102e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.8135e-03, -1.0900e-03, -2.4588e-03,  ...,  1.5376e-03,\n",
      "           -1.3284e-03, -9.7804e-04],\n",
      "          [-5.1070e-04, -1.9354e-03,  1.1703e-03,  ...,  2.2405e-03,\n",
      "           -6.0023e-04, -2.6213e-03],\n",
      "          [-1.9165e-03,  2.8470e-03,  1.5113e-03,  ...,  1.5168e-03,\n",
      "           -5.2810e-04, -6.4678e-06],\n",
      "          ...,\n",
      "          [-2.5464e-03, -1.6230e-03,  1.5267e-03,  ..., -2.3131e-04,\n",
      "            2.2528e-03, -1.5976e-03],\n",
      "          [ 2.3585e-03, -1.7105e-03, -2.4260e-03,  ..., -2.0985e-03,\n",
      "            8.0707e-04, -2.4749e-03],\n",
      "          [ 2.5997e-03, -7.9942e-04,  1.4352e-03,  ...,  2.3200e-03,\n",
      "           -1.5972e-03,  1.7265e-03]],\n",
      "\n",
      "         [[ 9.7897e-04, -2.6952e-03,  2.7174e-03,  ..., -8.1293e-04,\n",
      "           -2.8123e-03, -5.3063e-05],\n",
      "          [-2.4816e-03,  2.8723e-03,  1.8905e-03,  ..., -1.0765e-03,\n",
      "           -1.3631e-05,  1.7173e-03],\n",
      "          [ 1.6709e-03,  2.3767e-03,  8.0474e-04,  ...,  2.4551e-03,\n",
      "            9.1626e-04, -1.5811e-03],\n",
      "          ...,\n",
      "          [ 2.1000e-03,  2.1379e-03, -4.2126e-04,  ...,  1.8860e-03,\n",
      "            2.8278e-03, -2.7317e-03],\n",
      "          [-1.8014e-03,  2.3983e-03,  4.4666e-04,  ...,  2.4009e-03,\n",
      "            1.2525e-03,  1.1788e-03],\n",
      "          [ 2.8163e-03,  8.2688e-04,  1.1674e-03,  ...,  2.5834e-03,\n",
      "            2.2769e-04, -3.0302e-04]],\n",
      "\n",
      "         [[-2.3219e-03,  1.2509e-03, -3.6276e-04,  ..., -2.6716e-03,\n",
      "            1.4948e-03, -2.6400e-03],\n",
      "          [-6.3660e-04, -1.8160e-03, -1.1251e-03,  ..., -1.9809e-03,\n",
      "            1.6614e-03,  3.0338e-04],\n",
      "          [ 1.5170e-04, -2.8440e-04,  5.5707e-04,  ...,  3.0908e-04,\n",
      "            2.6007e-03, -2.5338e-03],\n",
      "          ...,\n",
      "          [ 5.7184e-04,  4.1395e-04,  7.0188e-04,  ..., -2.7665e-03,\n",
      "           -2.6105e-03, -3.6377e-04],\n",
      "          [ 5.5018e-04,  1.7014e-03,  3.5603e-04,  ..., -1.7215e-03,\n",
      "            3.2838e-04, -2.3556e-04],\n",
      "          [ 1.3498e-03,  9.0891e-04,  2.4586e-03,  ...,  2.6449e-03,\n",
      "           -1.3374e-04,  1.3582e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2063e-03,  1.8908e-03,  1.7722e-03,  ...,  8.1351e-04,\n",
      "            1.3177e-03, -1.7506e-03],\n",
      "          [ 1.2144e-03,  4.6809e-04,  3.1516e-04,  ..., -7.3178e-04,\n",
      "           -9.8343e-04,  5.7316e-04],\n",
      "          [-2.2167e-03, -2.7091e-03, -2.2610e-03,  ...,  2.0837e-03,\n",
      "           -8.1803e-04, -2.5093e-03],\n",
      "          ...,\n",
      "          [-1.0151e-04,  7.9048e-04, -1.1632e-03,  ...,  9.8486e-05,\n",
      "            6.7565e-04,  1.4427e-03],\n",
      "          [-1.5524e-03, -1.5593e-03,  1.5556e-03,  ...,  1.8552e-03,\n",
      "            2.8033e-03, -2.7526e-03],\n",
      "          [-1.2411e-03, -5.5433e-04,  2.3493e-03,  ...,  1.4359e-03,\n",
      "           -2.6974e-03, -1.8272e-03]],\n",
      "\n",
      "         [[-2.1825e-03,  1.0278e-03, -2.3773e-03,  ..., -7.8278e-05,\n",
      "           -3.4941e-04,  5.1786e-04],\n",
      "          [ 8.9110e-04, -9.2807e-04, -1.9594e-03,  ...,  8.0184e-04,\n",
      "            1.4437e-03, -2.6625e-03],\n",
      "          [-7.8664e-05,  4.5465e-04, -1.9246e-03,  ..., -2.3323e-03,\n",
      "            2.8391e-03, -1.0119e-03],\n",
      "          ...,\n",
      "          [-2.4850e-03, -1.7874e-03, -2.6928e-03,  ..., -2.7158e-03,\n",
      "           -2.7609e-03,  2.1177e-03],\n",
      "          [ 2.6583e-03, -2.8149e-04, -1.2360e-03,  ...,  3.9357e-04,\n",
      "            2.8899e-03, -2.3956e-03],\n",
      "          [-1.0328e-03,  2.1675e-03,  4.7504e-04,  ..., -1.7305e-03,\n",
      "            5.0911e-04, -1.0237e-03]],\n",
      "\n",
      "         [[ 7.6053e-04,  2.3907e-03,  9.7863e-04,  ...,  3.5461e-04,\n",
      "           -1.8679e-03, -1.5721e-03],\n",
      "          [-8.4288e-04, -2.1164e-04,  1.7051e-03,  ...,  1.9415e-03,\n",
      "           -2.8085e-03,  2.1185e-03],\n",
      "          [ 2.0726e-04,  2.6636e-03, -7.2749e-05,  ..., -2.8102e-04,\n",
      "            1.9273e-03,  9.7540e-04],\n",
      "          ...,\n",
      "          [ 2.7244e-03,  1.0745e-03,  2.5583e-03,  ...,  1.4256e-03,\n",
      "           -1.6089e-04, -1.6386e-03],\n",
      "          [ 1.4590e-03,  3.5354e-04,  9.4288e-04,  ...,  9.2140e-04,\n",
      "           -2.5712e-03,  2.8005e-03],\n",
      "          [ 1.1565e-03, -1.4871e-03,  2.5625e-03,  ...,  6.1833e-04,\n",
      "            2.7241e-03, -1.8105e-03]]]])\n",
      "\n",
      "Layer: KANClassifier.2.bias, Biases: tensor([[ 0.1172, -0.0860,  0.2700,  0.2198, -0.5460, -0.7233,  0.0012,  0.1588,\n",
      "         -0.2863,  0.0824,  0.0211, -0.0283, -0.0369,  0.4879,  0.1513, -0.5101,\n",
      "         -0.1711,  0.3719,  0.2771, -0.2989, -0.0080,  0.0536, -0.3841,  0.3246,\n",
      "         -0.1394, -0.0064]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in fsl_model.named_parameters():\n",
    "  if \"fouriercoeffs\" in name:\n",
    "    print(f\"Layer: {name}, Fourier Coefficients:\\n{param.data}\\n\")\n",
    "  # if \"weight\" in name:\n",
    "  #   print(f\"Layer: {name}:, Weights: {param.data}\")\n",
    "  elif \"bias\" in name:\n",
    "    print(f\"Layer: {name}, Biases: {param.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128])\n",
      "tensor([[ 0.2318,  0.0547,  0.6426, -0.0577, -0.4347, -0.6254,  0.0017, -0.1285,\n",
      "         -0.3610,  0.3880,  0.3038, -0.2632, -0.0245,  0.8562,  0.0191, -0.2727,\n",
      "         -0.2240,  0.4684, -0.0673, -0.1873,  0.2119, -0.1177, -0.5432,  0.2457,\n",
      "          0.1707,  0.0375]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn(size=(3, 128, 128))  # Single image (C, H, W)\n",
    "print(tensor1.shape)\n",
    "\n",
    "output = fsl_model(tensor1.unsqueeze(dim=0))  # Add batch dimension\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: KANClassifier.1.fouriercoeffs, Fourier Coefficients:\n",
      "tensor([[[[-3.4559e-03, -2.1262e-02, -9.9435e-03,  ...,  1.6428e-02,\n",
      "            4.7876e-03,  9.2377e-03],\n",
      "          [-1.2113e-02, -1.9879e-02,  6.0552e-03,  ..., -1.0994e-02,\n",
      "           -9.2924e-03, -9.6265e-03],\n",
      "          [-3.7348e-03, -8.3083e-03,  9.9061e-04,  ..., -8.4609e-04,\n",
      "           -1.7406e-02, -1.5949e-03],\n",
      "          ...,\n",
      "          [ 1.8264e-03,  1.5777e-03, -1.9887e-03,  ...,  1.3169e-03,\n",
      "            1.2873e-02,  6.6389e-03],\n",
      "          [-8.6265e-04,  6.7001e-04, -1.5453e-02,  ...,  1.8155e-02,\n",
      "            2.4868e-03, -6.7170e-03],\n",
      "          [ 1.8978e-03,  4.0801e-03, -5.2635e-03,  ...,  1.5672e-02,\n",
      "           -1.3830e-02,  6.7259e-03]],\n",
      "\n",
      "         [[-4.9398e-03,  1.3222e-03, -1.7405e-02,  ..., -4.0871e-03,\n",
      "           -1.2426e-02,  4.2763e-03],\n",
      "          [-5.5842e-03, -1.0265e-02,  1.2770e-02,  ...,  4.6642e-03,\n",
      "           -1.5909e-02,  1.4144e-02],\n",
      "          [ 6.1552e-03,  1.3524e-02,  4.9043e-03,  ..., -3.9339e-03,\n",
      "            5.6631e-03,  5.3981e-03],\n",
      "          ...,\n",
      "          [-6.4075e-05, -1.0007e-02, -1.1058e-02,  ..., -4.4879e-05,\n",
      "            1.2401e-04, -2.7541e-04],\n",
      "          [-3.5638e-03, -2.3098e-03, -4.9780e-03,  ...,  1.2032e-02,\n",
      "           -5.3673e-03,  6.6105e-03],\n",
      "          [-2.2035e-03,  1.1991e-02,  3.2959e-03,  ...,  8.4816e-03,\n",
      "           -5.3778e-03,  1.1158e-02]],\n",
      "\n",
      "         [[ 6.8574e-03, -4.1592e-04, -5.9675e-03,  ..., -2.8007e-03,\n",
      "            8.4004e-03,  4.8035e-04],\n",
      "          [ 6.7042e-03, -2.9214e-03, -1.2185e-03,  ...,  1.0292e-02,\n",
      "           -6.9407e-03,  7.0481e-03],\n",
      "          [ 1.3808e-02, -4.5668e-03, -6.4027e-03,  ..., -9.1375e-03,\n",
      "            6.1706e-03,  1.0119e-03],\n",
      "          ...,\n",
      "          [ 1.0239e-02, -1.2638e-03,  7.8803e-03,  ..., -1.1274e-02,\n",
      "            2.8397e-03,  9.7702e-03],\n",
      "          [-1.4313e-02,  6.1605e-03, -1.1281e-02,  ..., -1.2874e-02,\n",
      "           -3.4314e-03,  1.1552e-02],\n",
      "          [-2.5306e-03, -1.8621e-02, -8.0862e-03,  ...,  8.1381e-03,\n",
      "           -1.7130e-02, -7.4548e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2354e-03,  1.2515e-02, -2.1365e-03,  ...,  1.0490e-02,\n",
      "           -9.0490e-03,  5.0679e-04],\n",
      "          [-1.3124e-03,  7.3235e-04, -4.8175e-03,  ..., -4.9147e-03,\n",
      "           -3.8997e-03,  3.8939e-03],\n",
      "          [ 4.7957e-03,  7.9565e-04, -1.1142e-03,  ..., -3.7343e-03,\n",
      "            2.8734e-03,  5.1538e-03],\n",
      "          ...,\n",
      "          [ 4.4995e-03, -1.7972e-02,  2.5948e-04,  ...,  4.8895e-03,\n",
      "            1.1110e-02, -4.6725e-03],\n",
      "          [-2.9709e-03, -6.2571e-03, -5.0165e-03,  ..., -1.4996e-02,\n",
      "            9.1907e-03,  1.0840e-02],\n",
      "          [ 1.3294e-02, -1.2523e-03,  2.0483e-02,  ...,  4.8480e-03,\n",
      "           -4.1369e-03,  6.8393e-03]],\n",
      "\n",
      "         [[-4.9021e-03, -7.5130e-05, -8.8701e-03,  ...,  2.4439e-05,\n",
      "           -1.7307e-02,  1.1973e-03],\n",
      "          [ 3.6054e-03, -1.6000e-03,  6.8142e-03,  ...,  1.4199e-02,\n",
      "            1.3308e-03, -6.7078e-03],\n",
      "          [ 2.9111e-03, -2.6860e-03, -6.8879e-03,  ..., -2.2888e-03,\n",
      "            9.0747e-03, -7.0413e-03],\n",
      "          ...,\n",
      "          [-1.3847e-02,  6.9073e-03,  5.4964e-03,  ..., -5.9138e-03,\n",
      "            1.0287e-02,  7.0473e-03],\n",
      "          [-1.3096e-02,  5.1547e-03, -7.5601e-03,  ...,  5.9155e-03,\n",
      "           -9.1066e-03, -5.1103e-03],\n",
      "          [-3.0986e-03, -5.1980e-03,  1.4103e-03,  ..., -6.9784e-03,\n",
      "           -1.0605e-02,  3.4004e-03]],\n",
      "\n",
      "         [[ 1.9899e-02,  2.8159e-03, -3.4806e-03,  ...,  1.4270e-02,\n",
      "            1.0300e-03, -3.2993e-03],\n",
      "          [-7.5311e-03, -2.9285e-04,  1.3277e-02,  ...,  3.9661e-03,\n",
      "           -5.2030e-03,  1.5613e-02],\n",
      "          [ 2.3303e-03, -6.9154e-03, -5.0971e-03,  ..., -1.1615e-02,\n",
      "            2.4428e-03, -2.7955e-02],\n",
      "          ...,\n",
      "          [ 2.1616e-03,  7.3945e-03,  6.5579e-03,  ..., -4.0344e-03,\n",
      "            2.3806e-03, -6.9216e-03],\n",
      "          [-1.3080e-02, -1.3293e-03, -7.5586e-03,  ...,  1.5669e-02,\n",
      "            3.1958e-03,  2.6311e-03],\n",
      "          [-1.1554e-02, -6.0180e-03,  2.1308e-03,  ...,  8.7971e-04,\n",
      "           -9.9546e-03,  9.9350e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.5483e-03,  1.5948e-03,  1.1176e-02,  ..., -1.5845e-02,\n",
      "           -6.4302e-03, -3.6591e-03],\n",
      "          [-8.9015e-03, -6.0439e-04,  2.2444e-03,  ..., -2.6261e-03,\n",
      "            1.8270e-02, -6.9725e-03],\n",
      "          [-1.3000e-02, -1.1331e-02, -2.7410e-03,  ..., -1.9213e-03,\n",
      "            9.2563e-03, -8.9777e-03],\n",
      "          ...,\n",
      "          [-7.8165e-04, -2.3071e-03, -1.7473e-03,  ...,  5.5570e-03,\n",
      "            1.0612e-02, -7.6399e-03],\n",
      "          [-1.3242e-02,  1.4047e-03,  5.8024e-03,  ...,  8.9210e-03,\n",
      "           -1.7881e-03, -4.1427e-03],\n",
      "          [-1.4559e-02, -1.3955e-02, -1.3229e-02,  ...,  2.6972e-04,\n",
      "            6.6826e-04,  3.2882e-03]],\n",
      "\n",
      "         [[ 1.3014e-03, -1.3622e-02,  4.4526e-03,  ..., -8.7074e-03,\n",
      "           -2.1218e-03,  2.8582e-03],\n",
      "          [-3.5446e-03, -6.5032e-03,  4.9511e-03,  ...,  2.8888e-03,\n",
      "           -2.3081e-03, -3.9614e-03],\n",
      "          [ 9.3318e-03, -1.7034e-03, -4.5691e-03,  ...,  5.7108e-04,\n",
      "            5.5897e-03, -6.6973e-03],\n",
      "          ...,\n",
      "          [-6.0663e-03,  1.1520e-03, -1.4211e-02,  ...,  7.6816e-03,\n",
      "            3.1575e-03,  7.9096e-03],\n",
      "          [-9.4085e-04, -6.4660e-03,  1.4107e-03,  ...,  3.1091e-03,\n",
      "           -1.7726e-04, -6.0790e-03],\n",
      "          [-9.7315e-04, -2.6200e-03, -2.1128e-03,  ...,  1.4681e-02,\n",
      "            1.2644e-02, -2.0937e-04]],\n",
      "\n",
      "         [[ 4.3900e-03,  3.0687e-03,  1.2345e-02,  ...,  6.6344e-03,\n",
      "            5.4280e-03, -1.0380e-03],\n",
      "          [ 1.2187e-02, -1.5564e-02,  4.4410e-03,  ..., -1.2956e-02,\n",
      "           -2.3123e-03, -5.6334e-03],\n",
      "          [-6.5445e-03, -1.9010e-03,  3.9584e-03,  ..., -9.5205e-03,\n",
      "            5.5388e-03,  1.1082e-03],\n",
      "          ...,\n",
      "          [-8.4704e-03, -1.6158e-02, -6.6268e-03,  ..., -8.4732e-03,\n",
      "           -1.6787e-02,  8.2767e-03],\n",
      "          [ 1.8517e-02,  1.4461e-03,  5.5267e-04,  ..., -1.0385e-02,\n",
      "            1.0620e-02,  4.0168e-03],\n",
      "          [ 3.5775e-03, -2.3751e-05,  5.3498e-03,  ..., -5.1628e-03,\n",
      "            1.1498e-02, -5.8255e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1986e-02, -3.0030e-04, -1.0624e-04,  ..., -7.9473e-03,\n",
      "            3.3394e-03, -8.5754e-03],\n",
      "          [-8.9353e-03, -1.1390e-02,  3.3132e-03,  ...,  2.7824e-03,\n",
      "            2.4071e-04, -1.2787e-03],\n",
      "          [-5.3496e-03, -1.8558e-03,  1.5716e-03,  ..., -6.3154e-03,\n",
      "            6.5239e-03, -8.0610e-03],\n",
      "          ...,\n",
      "          [ 5.6375e-03, -6.7807e-03, -5.8026e-03,  ...,  7.2608e-03,\n",
      "            2.2054e-03,  8.1068e-03],\n",
      "          [-5.2574e-05, -1.0386e-02, -1.4026e-02,  ..., -1.3200e-02,\n",
      "            2.2542e-03,  8.2773e-03],\n",
      "          [-4.6919e-03,  4.6941e-03, -8.8554e-03,  ..., -1.0580e-02,\n",
      "           -1.3046e-02,  1.3974e-02]],\n",
      "\n",
      "         [[-9.4542e-04,  8.7763e-03,  3.5604e-03,  ...,  1.5365e-03,\n",
      "           -1.8879e-02,  4.2269e-04],\n",
      "          [ 2.3393e-03,  4.5852e-03, -1.6319e-03,  ...,  8.9635e-04,\n",
      "            8.0186e-03,  1.4044e-02],\n",
      "          [-3.4263e-03, -5.2017e-03, -1.7249e-02,  ...,  1.0965e-03,\n",
      "            1.2966e-03, -1.0709e-02],\n",
      "          ...,\n",
      "          [ 1.5906e-02, -1.4100e-03,  4.1148e-03,  ..., -2.2146e-03,\n",
      "            1.1593e-02, -1.0885e-02],\n",
      "          [ 8.7116e-03,  7.0489e-03, -1.3765e-02,  ...,  2.0007e-03,\n",
      "           -3.5180e-03, -7.6533e-03],\n",
      "          [ 1.7482e-02, -3.3188e-03,  6.9728e-03,  ..., -1.0035e-02,\n",
      "           -4.5228e-03, -5.4916e-03]],\n",
      "\n",
      "         [[ 1.0221e-02,  1.1848e-02, -2.5785e-03,  ..., -1.3762e-02,\n",
      "           -1.7211e-02,  1.2421e-03],\n",
      "          [ 8.7580e-03,  6.2626e-03,  4.3459e-03,  ...,  4.0264e-03,\n",
      "            5.0162e-03, -1.2530e-02],\n",
      "          [-1.9657e-03, -4.9956e-03, -1.7915e-03,  ..., -2.1669e-02,\n",
      "           -1.0741e-02, -1.0137e-02],\n",
      "          ...,\n",
      "          [-3.7132e-03, -2.6904e-03, -4.5322e-03,  ...,  1.6782e-03,\n",
      "            1.7198e-02, -1.0178e-02],\n",
      "          [-3.9337e-03,  2.6735e-03, -7.3814e-03,  ...,  1.6836e-03,\n",
      "            1.9435e-03,  2.4666e-03],\n",
      "          [-3.5597e-03, -1.0542e-03, -5.4628e-03,  ..., -1.1372e-02,\n",
      "           -2.5777e-03, -8.8814e-03]]]])\n",
      "\n",
      "Layer: KANClassifier.2.fouriercoeffs, Fourier Coefficients:\n",
      "tensor([[[[-1.2056e-02,  9.2639e-03,  3.2626e-02,  ..., -6.3407e-02,\n",
      "           -1.9300e-03,  3.4612e-03],\n",
      "          [-5.0652e-02,  1.2677e-02, -1.4997e-02,  ...,  2.9430e-02,\n",
      "            5.9365e-03,  4.8268e-02],\n",
      "          [ 6.4207e-02,  3.7493e-02,  3.1270e-02,  ...,  4.7124e-03,\n",
      "            1.7578e-03,  7.7001e-02],\n",
      "          ...,\n",
      "          [-2.6847e-03,  2.7426e-02,  3.9331e-02,  ..., -2.7356e-03,\n",
      "           -3.1827e-02,  7.1475e-03],\n",
      "          [-3.4994e-02, -7.1565e-02, -2.6075e-02,  ..., -6.1595e-02,\n",
      "            2.2613e-02,  7.9938e-02],\n",
      "          [ 1.2207e-02,  3.2698e-02,  2.1068e-02,  ..., -5.0006e-02,\n",
      "           -5.9997e-02,  5.2722e-02]],\n",
      "\n",
      "         [[ 1.8029e-02,  3.1918e-04,  5.3149e-02,  ..., -7.8389e-03,\n",
      "            2.8270e-02, -4.4853e-03],\n",
      "          [-4.8432e-02,  1.7718e-02, -3.8828e-02,  ..., -9.7312e-03,\n",
      "           -2.5264e-02,  1.5578e-02],\n",
      "          [ 5.1564e-02,  3.0337e-03,  3.9809e-02,  ...,  4.8178e-02,\n",
      "            1.6707e-02, -1.4809e-02],\n",
      "          ...,\n",
      "          [-5.3199e-02, -1.9230e-02, -4.4660e-02,  ...,  3.2378e-02,\n",
      "            5.2605e-02, -4.3812e-02],\n",
      "          [-2.4902e-02,  1.4773e-02,  3.1109e-02,  ..., -6.3807e-03,\n",
      "            7.5379e-02,  3.2995e-02],\n",
      "          [ 2.7052e-02,  3.5942e-02,  2.7292e-02,  ...,  2.8243e-03,\n",
      "            2.2196e-03,  2.0045e-02]],\n",
      "\n",
      "         [[ 1.0527e-02, -2.3914e-02, -3.8730e-02,  ..., -1.8452e-02,\n",
      "           -4.9145e-02, -2.8651e-02],\n",
      "          [ 2.0892e-03,  6.1931e-03,  1.1840e-02,  ..., -3.3644e-02,\n",
      "           -4.6019e-03,  5.2295e-02],\n",
      "          [ 5.6309e-02,  2.7393e-02,  1.3113e-02,  ..., -1.8872e-02,\n",
      "           -1.8353e-02, -1.2460e-03],\n",
      "          ...,\n",
      "          [-2.4668e-02, -9.8465e-03,  3.7362e-02,  ..., -2.4582e-02,\n",
      "           -5.4891e-02, -2.4627e-02],\n",
      "          [ 2.4603e-02,  6.5480e-02,  6.1601e-02,  ...,  1.8988e-03,\n",
      "            1.7808e-02, -1.3795e-02],\n",
      "          [ 3.1537e-02, -5.1083e-02, -2.5044e-02,  ...,  2.2054e-02,\n",
      "           -2.0482e-02,  1.8677e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.2990e-03,  2.0072e-03, -2.7293e-02,  ...,  3.3025e-02,\n",
      "           -8.4268e-02,  8.3223e-03],\n",
      "          [-1.0000e-01, -1.2428e-02,  3.1594e-02,  ...,  1.0910e-02,\n",
      "            2.3062e-02,  3.2375e-02],\n",
      "          [-9.0648e-02, -5.4210e-02,  2.8236e-02,  ..., -4.4818e-03,\n",
      "            1.1972e-02, -3.3351e-03],\n",
      "          ...,\n",
      "          [-1.6391e-03,  9.8934e-02,  6.1161e-02,  ...,  3.3817e-02,\n",
      "            2.5457e-02,  1.4590e-02],\n",
      "          [-4.3002e-02,  3.7533e-02, -4.6302e-02,  ...,  4.6257e-02,\n",
      "           -2.4604e-02,  6.7698e-02],\n",
      "          [ 1.3000e-02, -4.4146e-02, -2.5630e-02,  ..., -1.7989e-02,\n",
      "           -2.3598e-02,  3.0439e-02]],\n",
      "\n",
      "         [[-1.9147e-02, -2.4227e-02, -5.3208e-02,  ..., -1.9946e-02,\n",
      "           -2.7236e-02, -9.3744e-03],\n",
      "          [ 2.1999e-02, -5.0927e-02, -1.3882e-03,  ...,  1.3263e-02,\n",
      "            7.0355e-02, -7.5421e-03],\n",
      "          [ 5.0945e-02, -7.1943e-03,  1.5903e-02,  ...,  7.6836e-02,\n",
      "            2.1056e-03,  1.5856e-02],\n",
      "          ...,\n",
      "          [ 1.1861e-02,  2.5462e-02, -3.9274e-02,  ...,  1.5005e-02,\n",
      "           -9.4501e-03, -2.6977e-02],\n",
      "          [-5.9979e-02,  4.2142e-03,  6.3953e-03,  ...,  1.5250e-02,\n",
      "            3.4333e-02, -5.9247e-02],\n",
      "          [-6.4686e-02, -5.1764e-02, -2.9919e-02,  ...,  3.8691e-02,\n",
      "           -1.0098e-03,  3.7067e-02]],\n",
      "\n",
      "         [[ 5.5239e-03, -5.3160e-02,  7.6344e-02,  ...,  5.0315e-03,\n",
      "           -2.6280e-02,  4.5282e-02],\n",
      "          [ 6.6655e-03,  2.4496e-02,  3.1192e-02,  ..., -6.1605e-02,\n",
      "           -2.7846e-02, -1.8489e-02],\n",
      "          [-3.6085e-02, -4.2126e-02,  6.3536e-02,  ..., -6.6241e-02,\n",
      "           -1.6518e-02,  8.0490e-03],\n",
      "          ...,\n",
      "          [ 7.1810e-04,  3.9232e-02,  3.6676e-02,  ..., -6.8149e-02,\n",
      "            1.3674e-02, -4.8715e-02],\n",
      "          [ 1.1514e-02, -3.4991e-02,  6.1187e-02,  ...,  1.5774e-02,\n",
      "           -4.5660e-02,  9.7319e-02],\n",
      "          [-7.1030e-02, -4.2985e-02, -8.8469e-03,  ...,  2.0850e-02,\n",
      "            5.5973e-02, -2.7379e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1769e-02, -1.1167e-02, -5.7405e-03,  ...,  2.6804e-02,\n",
      "           -2.2666e-02,  2.8687e-02],\n",
      "          [-8.2063e-03, -1.1010e-02,  5.4911e-02,  ..., -4.7678e-02,\n",
      "           -3.0951e-02,  1.3525e-02],\n",
      "          [ 9.6019e-03, -4.7307e-02, -1.9872e-02,  ..., -6.7785e-03,\n",
      "            5.4610e-02, -7.3737e-03],\n",
      "          ...,\n",
      "          [ 6.7983e-02,  3.6385e-02,  1.2750e-02,  ..., -6.8576e-02,\n",
      "            8.6908e-03,  4.3000e-03],\n",
      "          [-2.3499e-02, -6.5757e-03,  4.4805e-02,  ..., -1.1031e-02,\n",
      "           -2.0166e-04, -1.3128e-02],\n",
      "          [-1.5179e-02, -2.3093e-02, -8.7345e-02,  ...,  1.5553e-02,\n",
      "           -3.6461e-02,  3.2988e-03]],\n",
      "\n",
      "         [[ 6.2366e-02,  3.8077e-02,  2.2149e-03,  ..., -2.8893e-02,\n",
      "            5.2460e-02,  5.7662e-02],\n",
      "          [ 2.3475e-02,  3.6824e-03, -1.4210e-02,  ..., -2.7986e-02,\n",
      "            1.4340e-02, -3.1243e-02],\n",
      "          [ 2.2341e-02,  6.3564e-02, -1.2652e-02,  ..., -2.4019e-02,\n",
      "           -2.8304e-02,  3.6539e-03],\n",
      "          ...,\n",
      "          [-8.9406e-02, -6.1032e-02, -7.4228e-02,  ...,  5.2102e-03,\n",
      "           -4.1041e-02,  7.1477e-02],\n",
      "          [-2.8645e-02, -3.3591e-02,  1.0655e-02,  ..., -1.7488e-02,\n",
      "            1.1484e-02, -5.3676e-02],\n",
      "          [ 3.6097e-02,  5.7380e-02,  2.0090e-02,  ..., -6.6283e-02,\n",
      "            3.3221e-02, -3.9489e-02]],\n",
      "\n",
      "         [[ 1.3697e-02,  6.6438e-02, -3.1237e-03,  ...,  5.3518e-02,\n",
      "           -2.1891e-02, -5.1378e-02],\n",
      "          [ 2.9912e-02, -7.0687e-03,  7.6258e-03,  ...,  3.5082e-02,\n",
      "            1.0892e-02,  9.6124e-02],\n",
      "          [ 8.5044e-02, -8.4718e-02, -4.5024e-02,  ..., -1.9205e-03,\n",
      "           -4.6874e-02, -6.1394e-03],\n",
      "          ...,\n",
      "          [-1.6803e-02,  1.3835e-02,  6.5313e-03,  ..., -9.7145e-03,\n",
      "           -4.6123e-02,  2.0538e-02],\n",
      "          [-2.3733e-02,  2.7593e-02, -3.7383e-02,  ...,  1.0382e-01,\n",
      "           -1.4616e-02, -1.7701e-02],\n",
      "          [ 1.7318e-02, -3.1576e-02, -5.4794e-02,  ..., -3.3659e-02,\n",
      "            5.5381e-02, -1.0176e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.5353e-02, -4.1057e-02, -1.9858e-02,  ...,  1.8801e-02,\n",
      "           -4.0979e-02,  5.5450e-02],\n",
      "          [-3.8898e-02,  4.3164e-02,  1.5128e-02,  ...,  1.5402e-02,\n",
      "            5.2116e-02,  1.5618e-02],\n",
      "          [ 2.5878e-02, -6.8985e-03,  5.2563e-02,  ..., -7.8139e-03,\n",
      "           -1.0654e-02,  1.0923e-02],\n",
      "          ...,\n",
      "          [-3.2688e-03,  7.4370e-02, -6.5463e-03,  ..., -2.5189e-02,\n",
      "           -7.0845e-02,  5.1532e-03],\n",
      "          [-8.9555e-02,  4.5531e-02,  6.1225e-02,  ..., -2.6013e-02,\n",
      "            2.7032e-02,  1.9001e-02],\n",
      "          [ 1.1743e-02,  7.3933e-03,  2.7833e-02,  ...,  1.5285e-02,\n",
      "           -3.8337e-02,  3.7614e-02]],\n",
      "\n",
      "         [[ 3.2988e-02, -4.2103e-02,  7.0850e-03,  ..., -9.1061e-03,\n",
      "            1.9136e-02,  3.9209e-02],\n",
      "          [ 3.4168e-02, -1.8250e-02, -9.1856e-02,  ..., -5.5769e-02,\n",
      "           -2.0296e-02,  5.3098e-05],\n",
      "          [ 3.0047e-02,  5.8669e-03, -1.6842e-02,  ..., -5.5131e-02,\n",
      "            1.4183e-02,  1.9838e-02],\n",
      "          ...,\n",
      "          [ 2.4300e-02,  6.0326e-02, -8.3824e-02,  ..., -1.5326e-02,\n",
      "            5.9549e-02,  4.5015e-02],\n",
      "          [-1.6333e-02,  3.9904e-02, -8.6297e-02,  ...,  2.9992e-02,\n",
      "            3.2486e-02, -4.1994e-02],\n",
      "          [-1.3217e-01,  3.3973e-02,  8.5180e-02,  ..., -2.7556e-02,\n",
      "            3.0979e-02, -2.9414e-02]],\n",
      "\n",
      "         [[-2.0600e-02,  1.0142e-02,  1.7277e-03,  ...,  6.0612e-02,\n",
      "           -3.5069e-02, -2.0641e-02],\n",
      "          [-4.6761e-02,  8.2994e-03, -1.4733e-02,  ...,  6.4064e-02,\n",
      "           -2.7990e-02, -2.3611e-02],\n",
      "          [-4.1921e-03, -4.8430e-02,  4.3116e-02,  ...,  2.2415e-02,\n",
      "            3.9048e-02,  4.6265e-02],\n",
      "          ...,\n",
      "          [-4.2809e-02,  4.2199e-02, -4.3055e-02,  ...,  9.9935e-03,\n",
      "            2.0759e-02, -3.5766e-02],\n",
      "          [-2.1139e-04,  6.4916e-02, -1.4831e-02,  ...,  1.4398e-02,\n",
      "           -2.6083e-02,  1.1574e-02],\n",
      "          [-2.2677e-02,  2.7970e-02, -4.8711e-02,  ...,  9.3334e-02,\n",
      "           -2.3845e-02,  2.2786e-02]]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in fsl_model.named_parameters():\n",
    "    if param.requires_grad:  # Ensures we only inspect trainable parameters\n",
    "        if \"fouriercoeffs\" in name:\n",
    "            print(f\"Layer: {name}, Fourier Coefficients:\\n{param.data}\\n\")\n",
    "        # elif \"bias\" in name:\n",
    "        #     print(f\"Layer: {name}, Biases:\\n{param.data}\\n\")\n",
    "        # elif \"weight\" in name:\n",
    "        #     print(f\"Layer: {name}, Weights:\\n{param.data}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet_KAN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (KANClassifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): NaiveFourierKANLayer()\n",
      "    (4): Linear(in_features=128, out_features=26, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 11058299\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NaiveFourierKANLayer(nn.Module):\n",
    "    def __init__(self, inputdim, outdim, initial_gridsize, addbias=True):\n",
    "        super(NaiveFourierKANLayer, self).__init__()\n",
    "        self.addbias = addbias\n",
    "        self.inputdim = inputdim\n",
    "        self.outdim = outdim\n",
    "        self.gridsize_param = nn.Parameter(torch.tensor(initial_gridsize, dtype=torch.float32)) #adjusted during training\n",
    "        self.fouriercoeffs = nn.Parameter(torch.empty(2, outdim, inputdim, initial_gridsize))\n",
    "        nn.init.xavier_uniform_(self.fouriercoeffs)\n",
    "        if self.addbias:\n",
    "            self.bias = nn.Parameter(torch.zeros(1, outdim))\n",
    "            nn.init.kaiming_normal_(self.bias)\n",
    "\n",
    "    def forward(self, x): #Combines cosine/sine terms with learnable coefficents for Fourier expansion and sums them + bias\n",
    "        gridsize = torch.clamp(self.gridsize_param, min=1).round().int()\n",
    "        outshape = x.shape[:-1] + (self.outdim,)\n",
    "        x = torch.reshape(x, (-1, self.inputdim))\n",
    "        k = torch.reshape(torch.arange(1, gridsize + 1, device=x.device), (1, 1, 1, gridsize))\n",
    "        xrshp = torch.reshape(x, (x.shape[0], 1, x.shape[1], 1))\n",
    "        c = torch.cos(k * xrshp)\n",
    "        s = torch.sin(k * xrshp)\n",
    "        y = torch.sum(c * self.fouriercoeffs[0:1, :, :, :gridsize], (-2, -1))\n",
    "        y += torch.sum(s * self.fouriercoeffs[1:2, :, :, :gridsize], (-2, -1))\n",
    "        if self.addbias:\n",
    "            y += self.bias\n",
    "        y = torch.reshape(y, outshape)\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    expansion: int = 2\n",
    "    def __init__(self, input, output, identity_downsample=None, stride=(1,1)):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input, output, kernel_size=(3,3), stride=(1,1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(output)\n",
    "        self.conv2 = nn.Conv2d(output, output*self.expansion, \n",
    "                               kernel_size=(3,3), stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(output*self.expansion,)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet_KAN(nn.Module):\n",
    "    def __init__(self, block: Block, layers, input_shape, output_shape):\n",
    "        super(ResNet_KAN, self).__init__()\n",
    "        self.input = 32\n",
    "        self.conv1 = nn.Conv2d(input_shape, 32, kernel_size=(5,5), stride=(2,2), padding=(2,2))\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(3,3), stride=(2,2), padding=(1,1))\n",
    "\n",
    "        ###ResNet Layers\n",
    "        self.layer1 = self.make_layer(block,layers[0], output=32, stride=(1,1))\n",
    "        self.layer2 = self.make_layer(block,layers[1], output=64, stride=(2,2))\n",
    "        self.layer3 = self.make_layer(block,layers[2], output=128, stride=(2,2))\n",
    "        self.layer4 = self.make_layer(block,layers[3], output=256, stride=(2,2))\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        self.KANClassifier = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(256*block.expansion, 256),\n",
    "            nn.ReLU(),\n",
    "            NaiveFourierKANLayer(256, 128, initial_gridsize=30),\n",
    "            nn.Linear(128, output_shape),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = self.KANClassifier(x)\n",
    "        return x\n",
    "    \n",
    "    def make_layer(self, block, num_residual_MyBlocks, output, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.input != output*block.expansion:\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(\n",
    "                self.input, output * block.expansion, kernel_size=(1,1), stride=stride),\n",
    "                nn.BatchNorm2d(output*block.expansion)\n",
    "            )\n",
    "        layers.append(block(self.input, output, identity_downsample, stride))\n",
    "        self.input = output*block.expansion\n",
    "\n",
    "        for i in range(num_residual_MyBlocks-1):\n",
    "            layers.append(block(self.input, output))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "torch.manual_seed(23)\n",
    "fsl_model = ResNet_KAN(Block, [3,4,3,3], input_shape=3, \n",
    "                       output_shape=26)\n",
    "print(f\"{fsl_model}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in fsl_model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet_KAN(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (KANClassifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=64, out_features=50, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): NaiveFourierKANLayer()\n",
      "    (4): NaiveFourierKANLayer()\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 147856\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NaiveFourierKANLayer(nn.Module):\n",
    "    def __init__(self, inputdim, outdim, initial_gridsize, addbias=True):\n",
    "        super(NaiveFourierKANLayer, self).__init__()\n",
    "        self.addbias = addbias\n",
    "        self.inputdim = inputdim\n",
    "        self.outdim = outdim\n",
    "        self.gridsize_param = nn.Parameter(torch.tensor(initial_gridsize, dtype=torch.float32)) #adjusted during training\n",
    "        self.fouriercoeffs = nn.Parameter(torch.empty(2, outdim, inputdim, initial_gridsize))\n",
    "        nn.init.xavier_uniform_(self.fouriercoeffs)\n",
    "        if self.addbias:\n",
    "            self.bias = nn.Parameter(torch.zeros(1, outdim))\n",
    "            nn.init.kaiming_normal_(self.bias)\n",
    "\n",
    "    def forward(self, x): #Combines cosine/sine terms with learnable coefficents for Fourier expansion and sums them + bias\n",
    "        gridsize = torch.clamp(self.gridsize_param, min=1).round().int()\n",
    "        outshape = x.shape[:-1] + (self.outdim,)\n",
    "        x = torch.reshape(x, (-1, self.inputdim))\n",
    "        k = torch.reshape(torch.arange(1, gridsize + 1, device=x.device), (1, 1, 1, gridsize))\n",
    "        xrshp = torch.reshape(x, (x.shape[0], 1, x.shape[1], 1))\n",
    "        c = torch.cos(k * xrshp)\n",
    "        s = torch.sin(k * xrshp)\n",
    "        y = torch.sum(c * self.fouriercoeffs[0:1, :, :, :gridsize], (-2, -1))\n",
    "        y += torch.sum(s * self.fouriercoeffs[1:2, :, :, :gridsize], (-2, -1))\n",
    "        if self.addbias:\n",
    "            y += self.bias\n",
    "        y = torch.reshape(y, outshape)\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    expansion: int = 2\n",
    "    def __init__(self, input, output, identity_downsample=None, stride=(1,1)):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input, output, kernel_size=(3,3), stride=(1,1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(output)\n",
    "        self.conv2 = nn.Conv2d(output, output*self.expansion, \n",
    "                               kernel_size=(3,3), stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(output*self.expansion,)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet_KAN(nn.Module):\n",
    "    def __init__(self, block: Block, layers, input_shape, output_shape):\n",
    "        super(ResNet_KAN, self).__init__()\n",
    "        self.input = 16\n",
    "        self.conv1 = nn.Conv2d(input_shape, 16, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "\n",
    "        ###ResNet Layers\n",
    "        self.layer1 = self.make_layer(block,layers[0], output=16, stride=(2,2))\n",
    "        self.layer2 = self.make_layer(block,layers[1], output=32, stride=(2,2))\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        self.KANClassifier = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(32*block.expansion, 50),  # MLP first\n",
    "            nn.ReLU(),\n",
    "            NaiveFourierKANLayer(50, 50, initial_gridsize=10),\n",
    "            NaiveFourierKANLayer(50, output_shape, initial_gridsize=10)\n",
    "\n",
    "        )\n",
    "    \n",
    "    def forward(self,x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = self.KANClassifier(x)\n",
    "        return x\n",
    "    \n",
    "    def make_layer(self, block, num_residual_MyBlocks, output, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.input != output*block.expansion:\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(\n",
    "                self.input, output * block.expansion, kernel_size=(1,1), stride=stride),\n",
    "                nn.BatchNorm2d(output*block.expansion)\n",
    "            )\n",
    "        layers.append(block(self.input, output, identity_downsample, stride))\n",
    "        self.input = output*block.expansion\n",
    "\n",
    "        for _ in range(num_residual_MyBlocks-1):\n",
    "            layers.append(block(self.input, output))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(23)\n",
    "fsl_model = ResNet_KAN(Block, [2,2,2], input_shape=1, \n",
    "                       output_shape=10)\n",
    "print(f\"{fsl_model}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in fsl_model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "#Adan: Adaptive Nesterov Momentum Algorithm (2024)\n",
    "\n",
    "import math\n",
    "from typing import List\n",
    "from torch import Tensor\n",
    "\n",
    "class MultiTensorApply(object):\n",
    "    available = False\n",
    "    warned = False\n",
    "\n",
    "    def __init__(self, chunk_size):\n",
    "        try:\n",
    "            MultiTensorApply.available = True\n",
    "            self.chunk_size = chunk_size\n",
    "        except ImportError as err:\n",
    "            MultiTensorApply.available = False\n",
    "            MultiTensorApply.import_err = err\n",
    "\n",
    "    def __call__(self, op, noop_flag_buffer, tensor_lists, *args):\n",
    "        return op(self.chunk_size, noop_flag_buffer, tensor_lists, *args)\n",
    "\n",
    "\n",
    "class Adan(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Implements a pytorch variant of Adan\n",
    "    Adan was proposed in\n",
    "    Adan: Adaptive Nesterov Momentum Algorithm for\n",
    "        Faster Optimizing Deep Models[J].arXiv preprint arXiv:2208.06677, 2022.\n",
    "    https://arxiv.org/abs/2208.06677\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or\n",
    "            dicts defining parameter groups.\n",
    "        lr (float, optional): learning rate. (default: 1e-3)\n",
    "        betas (Tuple[float, float, flot], optional): coefficients used for\n",
    "            first- and second-order moments. (default: (0.98, 0.92, 0.99))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability. (default: 1e-8)\n",
    "        weight_decay (float, optional): decoupled weight decay\n",
    "            (L2 penalty) (default: 0)\n",
    "        max_grad_norm (float, optional): value used to clip\n",
    "            global grad norm (default: 0.0 no clip)\n",
    "        no_prox (bool): how to perform the decoupled weight decay\n",
    "            (default: False)\n",
    "        foreach (bool): if True would use torch._foreach implementation.\n",
    "            It's faster but uses slightly more memory. (default: True)\n",
    "        fused (bool, optional): whether fused implementation is used.\n",
    "            (default: False)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-3,\n",
    "                 betas=(0.98, 0.92, 0.99),\n",
    "                 eps=1e-8,\n",
    "                 weight_decay=0.0,\n",
    "                 max_grad_norm=0.0,\n",
    "                 no_prox=False,\n",
    "                 foreach: bool = True,\n",
    "                 fused: bool = False):\n",
    "        if not 0.0 <= max_grad_norm:\n",
    "            raise ValueError('Invalid Max grad norm: {}'.format(max_grad_norm))\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError('Invalid learning rate: {}'.format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError('Invalid epsilon value: {}'.format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 0: {}'.format(\n",
    "                betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 1: {}'.format(\n",
    "                betas[1]))\n",
    "        if not 0.0 <= betas[2] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 2: {}'.format(\n",
    "                betas[2]))\n",
    "        if fused:\n",
    "            _check_fused_available()\n",
    "\n",
    "        defaults = dict(lr=lr,\n",
    "                        betas=betas,\n",
    "                        eps=eps,\n",
    "                        weight_decay=weight_decay,\n",
    "                        max_grad_norm=max_grad_norm,\n",
    "                        no_prox=no_prox,\n",
    "                        foreach=foreach,\n",
    "                        fused=fused)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adan, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('no_prox', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restart_opt(self):\n",
    "        for group in self.param_groups:\n",
    "            group['step'] = 0\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    state = self.state[p]\n",
    "                    # State initialization\n",
    "\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of gradient difference\n",
    "                    state['exp_avg_diff'] = torch.zeros_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\"\"\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        if self.defaults['max_grad_norm'] > 0:\n",
    "            device = self.param_groups[0]['params'][0].device\n",
    "            global_grad_norm = torch.zeros(1, device=device)\n",
    "\n",
    "            max_grad_norm = torch.tensor(self.defaults['max_grad_norm'],\n",
    "                                         device=device)\n",
    "            for group in self.param_groups:\n",
    "\n",
    "                for p in group['params']:\n",
    "                    if p.grad is not None:\n",
    "                        grad = p.grad\n",
    "                        global_grad_norm.add_(grad.pow(2).sum())\n",
    "\n",
    "            global_grad_norm = torch.sqrt(global_grad_norm)\n",
    "\n",
    "            clip_global_grad_norm = torch.clamp(\n",
    "                max_grad_norm / (global_grad_norm + group['eps']),\n",
    "                max=1.0).item()\n",
    "        else:\n",
    "            clip_global_grad_norm = 1.0\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            exp_avg_diffs = []\n",
    "            neg_pre_grads = []\n",
    "\n",
    "            beta1, beta2, beta3 = group['betas']\n",
    "            # assume same# Since we added LogSoftmax, use NLLLoss instead step across group now to simplify things\n",
    "            # per parameter step can be easily support\n",
    "            # by making it tensor, or pass list into kernel\n",
    "            if 'step' in group:\n",
    "                group['step'] += 1\n",
    "            else:\n",
    "                group['step'] = 1\n",
    "\n",
    "            bias_correction1 = 1.0 - beta1**group['step']\n",
    "            bias_correction2 = 1.0 - beta2**group['step']\n",
    "            bias_correction3 = 1.0 - beta3**group['step']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                params_with_grad.append(p)\n",
    "                grads.append(p.grad)\n",
    "\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "                    state['exp_avg_diff'] = torch.zeros_like(p)\n",
    "\n",
    "                if 'neg_pre_grad' not in state or group['step'] == 1:\n",
    "                    state['neg_pre_grad'] = p.grad.clone().mul_(\n",
    "                        -clip_global_grad_norm)\n",
    "\n",
    "                exp_avgs.append(state['exp_avg'])\n",
    "                exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "                exp_avg_diffs.append(state['exp_avg_diff'])\n",
    "                neg_pre_grads.append(state['neg_pre_grad'])\n",
    "\n",
    "            if not params_with_grad:\n",
    "                continue\n",
    "\n",
    "            kwargs = dict(\n",
    "                params=params_with_grad,\n",
    "                grads=grads,\n",
    "                exp_avgs=exp_avgs,\n",
    "                exp_avg_sqs=exp_avg_sqs,\n",
    "                exp_avg_diffs=exp_avg_diffs,\n",
    "                neg_pre_grads=neg_pre_grads,\n",
    "                beta1=beta1,\n",
    "                beta2=beta2,\n",
    "                beta3=beta3,\n",
    "                bias_correction1=bias_correction1,\n",
    "                bias_correction2=bias_correction2,\n",
    "                bias_correction3_sqrt=math.sqrt(bias_correction3),\n",
    "                lr=group['lr'],\n",
    "                weight_decay=group['weight_decay'],\n",
    "                eps=group['eps'],\n",
    "                no_prox=group['no_prox'],\n",
    "                clip_global_grad_norm=clip_global_grad_norm,\n",
    "            )\n",
    "\n",
    "            if group['foreach']:\n",
    "                if group['fused']:\n",
    "                    if torch.cuda.is_available():\n",
    "                        _fused_adan_multi_tensor(**kwargs)\n",
    "                    else:\n",
    "                        raise ValueError('Fused Adan does not support CPU')\n",
    "                else:\n",
    "                    _multi_tensor_adan(**kwargs)\n",
    "            elif group['fused']:\n",
    "                if torch.cuda.is_available():\n",
    "                    _fused_adan_single_tensor(**kwargs)\n",
    "                else:\n",
    "                    raise ValueError('Fused Adan does not support CPU')\n",
    "            else:\n",
    "                _single_tensor_adan(**kwargs)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def _single_tensor_adan(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    for i, param in enumerate(params):\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        exp_avg_diff = exp_avg_diffs[i]\n",
    "        neg_grad_or_diff = neg_pre_grads[i]\n",
    "\n",
    "        grad.mul_(clip_global_grad_norm)\n",
    "\n",
    "        # for memory saving, we use `neg_grad_or_diff`\n",
    "        # to get some temp variable in a inplace way\n",
    "        neg_grad_or_diff.add_(grad)\n",
    "\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  # m_t\n",
    "        exp_avg_diff.mul_(beta2).add_(neg_grad_or_diff,\n",
    "                                      alpha=1 - beta2)  # diff_t\n",
    "\n",
    "        neg_grad_or_diff.mul_(beta2).add_(grad)\n",
    "        exp_avg_sq.mul_(beta3).addcmul_(neg_grad_or_diff,\n",
    "                                        neg_grad_or_diff,\n",
    "                                        value=1 - beta3)  # n_t\n",
    "\n",
    "        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)\n",
    "        step_size_diff = lr * beta2 / bias_correction2\n",
    "        step_size = lr / bias_correction1\n",
    "\n",
    "        if no_prox:\n",
    "            param.mul_(1 - lr * weight_decay)\n",
    "            param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)\n",
    "        else:\n",
    "            param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)\n",
    "            param.div_(1 + lr * weight_decay)\n",
    "\n",
    "        neg_grad_or_diff.zero_().add_(grad, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _multi_tensor_adan(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    if len(params) == 0:\n",
    "        return\n",
    "\n",
    "    torch._foreach_mul_(grads, clip_global_grad_norm)\n",
    "\n",
    "    # for memory saving, we use `neg_pre_grads`\n",
    "    # to get some temp variable in a inplace way\n",
    "    torch._foreach_add_(neg_pre_grads, grads)\n",
    "\n",
    "    torch._foreach_mul_(exp_avgs, beta1)\n",
    "    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  # m_t\n",
    "\n",
    "    torch._foreach_mul_(exp_avg_diffs, beta2)\n",
    "    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,\n",
    "                        alpha=1 - beta2)  # diff_t\n",
    "\n",
    "    torch._foreach_mul_(neg_pre_grads, beta2)\n",
    "    torch._foreach_add_(neg_pre_grads, grads)\n",
    "    torch._foreach_mul_(exp_avg_sqs, beta3)\n",
    "    torch._foreach_addcmul_(exp_avg_sqs,\n",
    "                            neg_pre_grads,\n",
    "                            neg_pre_grads,\n",
    "                            value=1 - beta3)  # n_t\n",
    "\n",
    "    denom = torch._foreach_sqrt(exp_avg_sqs)\n",
    "    torch._foreach_div_(denom, bias_correction3_sqrt)\n",
    "    torch._foreach_add_(denom, eps)\n",
    "\n",
    "    step_size_diff = lr * beta2 / bias_correction2\n",
    "    step_size = lr / bias_correction1\n",
    "\n",
    "    if no_prox:\n",
    "        torch._foreach_mul_(params, 1 - lr * weight_decay)\n",
    "        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)\n",
    "        torch._foreach_addcdiv_(params,\n",
    "                                exp_avg_diffs,\n",
    "                                denom,\n",
    "                                value=-step_size_diff)\n",
    "    else:\n",
    "        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)\n",
    "        torch._foreach_addcdiv_(params,\n",
    "                                exp_avg_diffs,\n",
    "                                denom,\n",
    "                                value=-step_size_diff)\n",
    "        torch._foreach_div_(params, 1 + lr * weight_decay)\n",
    "    torch._foreach_zero_(neg_pre_grads)\n",
    "    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _fused_adan_multi_tensor(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    import fused_adan\n",
    "    multi_tensor_applier = MultiTensorApply(2048 * 32)\n",
    "    _dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
    "    multi_tensor_applier(\n",
    "        fused_adan.adan_multi_tensor, _dummy_overflow_buf,\n",
    "        [params, grads, exp_avgs, exp_avg_sqs, exp_avg_diffs, neg_pre_grads],\n",
    "        beta1, beta2, beta3, bias_correction1, bias_correction2,\n",
    "        bias_correction3_sqrt, lr, weight_decay, eps, no_prox,\n",
    "        clip_global_grad_norm)\n",
    "    torch._foreach_zero_(neg_pre_grads)\n",
    "    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _fused_adan_single_tensor(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    for i, param in enumerate(params):\n",
    "        p_data_fp32 = param.data.float()\n",
    "        out_p = param.data\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        exp_avg_diff = exp_avg_diffs[i]\n",
    "        neg_grad = neg_pre_grads[i]\n",
    "        with torch.cuda.device(param.device):\n",
    "            import fused_adan\n",
    "            fused_adan.adan_single_tensor(\n",
    "                p_data_fp32,\n",
    "                out_p,\n",
    "                grad,\n",
    "                exp_avg,\n",
    "                exp_avg_sq,\n",
    "                exp_avg_diff,\n",
    "                neg_grad,\n",
    "                beta1,\n",
    "                beta2,\n",
    "                beta3,\n",
    "                bias_correction1,\n",
    "                bias_correction2,\n",
    "                bias_correction3_sqrt,\n",
    "                lr,\n",
    "                weight_decay,\n",
    "                eps,\n",
    "                no_prox,\n",
    "                clip_global_grad_norm,\n",
    "            )\n",
    "        neg_grad.zero_().add_(grad, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _check_fused_available():\n",
    "    try:\n",
    "        import fused_adan\n",
    "    except ImportError as exc:\n",
    "        if torch.cuda.is_available():\n",
    "            # The module should be available but isn't. Try to\n",
    "            # help the user in this case.\n",
    "            raise ImportError((\n",
    "                str(exc)\n",
    "                + (\n",
    "                    '\\nThis could be caused by not having compiled '\n",
    "                    'the CUDA extension during package installation. '\n",
    "                    'Please try to re-install the package with '\n",
    "                    'the environment flag `FORCE_CUDA=1` set.'\n",
    "                )\n",
    "            ))\n",
    "        else:\n",
    "            raise ImportError(\n",
    "                str(exc) + '\\nFused Adan does not support CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fb94b8ab2a4c82a56f7a4e07d484f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2064\n",
      "Epoch [2/5], Loss: 0.0725\n",
      "Epoch [3/5], Loss: 0.0598\n",
      "Epoch [4/5], Loss: 0.0564\n",
      "Epoch [5/5], Loss: 0.0501\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#  1 Load MNIST Dataset (Grayscale 1-Channel Images)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  # Resize to match model input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "#  2 Modify Model for MNIST (1-channel input, 10 output classes)\n",
    "model = ResNet_KAN(Block, layers=[2, 2, 2], input_shape=1, output_shape=10)\n",
    "\n",
    "#  3 Define Loss & Optimizer\n",
    "# criterion = nn.NLLLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = Adan(model.parameters(), lr=0.001, betas=(0.98,0.92,0.99), weight_decay=1e-3)\n",
    "\n",
    "#  4 Training Loop\n",
    "num_epochs = 5\n",
    "train_losses = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        # images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#  5 Plot Loss Curve\n",
    "plt.plot(train_losses, marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Loss Reduction Check\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SqueezeNet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 740554\n"
     ]
    }
   ],
   "source": [
    "class fire(torch.nn.Module):\n",
    "    def __init__(self, inplanes: int , squeeze_planes: int, expand1x1: int, expand3x3: int) -> None:\n",
    "        super(fire, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.squeeze = torch.nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
    "        self.act1 = torch.nn.ReLU(inplace=True)\n",
    "        self.expand1x1 = torch.nn.Conv2d(squeeze_planes, expand1x1, kernel_size=1)\n",
    "        self.expand1x1_act1 = torch.nn.ReLU(inplace=True)\n",
    "        self.expand3x3 = torch.nn.Conv2d(squeeze_planes, expand3x3, kernel_size=3, stride=1, padding=1)\n",
    "        self.expand3x3_act1 = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "        #using the initialization from Guo et al. (2024)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.normal_(m.weight, mean=-1.0, std=0.1)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.normal_(m.bias, mean=-1.0, std=0.1)\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        x = self.squeeze(x)\n",
    "        x = self.act1(x)\n",
    "        output1 = self.expand1x1(x)\n",
    "        output1 = self.expand1x1_act1(output1)\n",
    "        output2 = self.expand3x3(x)\n",
    "        output2 = self.expand3x3_act1(output2)\n",
    "        final_output = torch.cat((output1, output2), dim=1)\n",
    "        return final_output\n",
    "\n",
    "    \n",
    "class SqueezeNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes: int = 10) -> None:\n",
    "        super(SqueezeNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.features = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "            fire(96, 16, 64, 64),\n",
    "            fire(128, 16, 64, 64),\n",
    "            fire(128, 32, 128, 128),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "            fire(256, 32, 128, 128),\n",
    "            fire(256, 48, 192, 192),\n",
    "            fire(384, 48, 192, 192),\n",
    "            fire(384, 64, 256, 256),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            fire(512, 64, 256, 256)\n",
    "        )\n",
    "\n",
    "        final_conv = torch.nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            final_conv,\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        #using the initialization from Guo et al. (2024)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.normal_(m.weight, mean=-1.0, std=0.1)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.normal_(m.bias, mean=-1.0, std=0.1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "model = SqueezeNet(num_classes=10)\n",
    "\n",
    "dummy_tensor = torch.randn(1, 3, 32, 224)\n",
    "model(dummy_tensor)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Initialized weights and biases from $\\mathcal{N}(-1, 0.1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: features.0.weight:, Weights: tensor([[[[-1.0267, -0.9539, -1.1363,  ..., -1.0701, -1.1500, -1.0902],\n",
      "          [-0.8397, -0.9701, -0.7909,  ..., -1.0602, -0.8029, -0.9956],\n",
      "          [-1.1987, -0.8596, -0.7731,  ..., -1.2196, -0.9326, -0.9606],\n",
      "          ...,\n",
      "          [-1.0597, -1.0457, -0.8061,  ..., -0.9152, -1.1185, -0.8797],\n",
      "          [-0.9530, -0.9182, -0.9133,  ..., -0.9913, -1.0417, -1.0573],\n",
      "          [-0.9832, -0.9007, -0.9920,  ..., -0.8518, -0.9828, -1.1887]],\n",
      "\n",
      "         [[-1.0452, -0.9763, -1.0186,  ..., -1.0797, -1.1063, -0.9815],\n",
      "          [-0.9619, -0.9099, -0.9501,  ..., -1.0902, -1.0781, -0.9721],\n",
      "          [-1.0094, -0.8103, -0.8306,  ..., -1.3008, -1.0814, -0.8515],\n",
      "          ...,\n",
      "          [-0.8575, -1.0552, -1.0188,  ..., -1.3010, -1.1293, -0.9776],\n",
      "          [-0.9676, -1.0298, -0.8153,  ..., -0.9835, -1.1222, -1.0616],\n",
      "          [-1.0942, -1.0845, -0.8839,  ..., -1.2025, -1.0950, -1.0736]],\n",
      "\n",
      "         [[-0.7113, -0.8566, -1.1176,  ..., -0.9065, -0.7872, -1.0731],\n",
      "          [-0.8906, -1.0413, -1.0325,  ..., -1.0944, -0.8540, -0.9540],\n",
      "          [-0.8940, -0.9793, -1.0251,  ..., -0.8143, -1.0004, -0.9958],\n",
      "          ...,\n",
      "          [-0.8696, -0.9803, -0.9535,  ..., -1.0122, -1.0449, -0.7563],\n",
      "          [-1.1259, -1.0069, -1.0024,  ..., -0.9751, -1.1432, -0.8941],\n",
      "          [-0.9119, -0.9939, -1.0505,  ..., -0.9404, -1.0093, -1.2472]]],\n",
      "\n",
      "\n",
      "        [[[-1.0284, -0.9856, -1.1637,  ..., -1.1014, -1.1411, -1.0419],\n",
      "          [-0.8931, -0.9954, -1.1347,  ..., -1.2425, -1.1457, -0.8813],\n",
      "          [-1.1232, -0.9828, -1.0431,  ..., -1.2028, -0.9367, -0.7625],\n",
      "          ...,\n",
      "          [-0.9651, -0.7956, -1.0721,  ..., -1.0553, -0.9744, -0.9778],\n",
      "          [-1.0811, -1.0151, -1.1276,  ..., -1.0930, -0.9251, -0.9421],\n",
      "          [-1.0196, -1.0543, -1.1048,  ..., -1.1542, -1.0541, -1.2473]],\n",
      "\n",
      "         [[-0.9390, -0.8108, -1.0339,  ..., -1.0278, -0.9360, -1.0730],\n",
      "          [-1.1998, -1.0497, -0.9242,  ..., -1.0631, -1.0508, -1.0773],\n",
      "          [-1.0031, -0.9289, -1.0037,  ..., -1.2527, -0.9227, -0.8505],\n",
      "          ...,\n",
      "          [-0.8479, -1.1068, -1.0752,  ..., -1.1071, -1.0700, -1.1045],\n",
      "          [-0.9727, -1.1152, -0.9575,  ..., -0.9510, -1.0885, -0.9080],\n",
      "          [-1.2176, -1.0323, -0.9887,  ..., -0.9557, -1.1361, -1.0378]],\n",
      "\n",
      "         [[-0.9545, -0.8842, -1.1067,  ..., -0.9182, -0.9901, -1.1058],\n",
      "          [-0.9447, -1.1445, -0.9229,  ..., -1.1686, -0.9646, -0.9982],\n",
      "          [-0.9000, -0.8746, -0.9003,  ..., -0.9728, -1.0619, -1.1053],\n",
      "          ...,\n",
      "          [-0.9578, -0.9905, -0.8155,  ..., -1.0602, -1.1208, -1.0878],\n",
      "          [-1.0987, -0.9960, -1.0241,  ..., -1.0775, -0.8475, -1.1034],\n",
      "          [-1.0593, -0.8968, -1.1139,  ..., -1.0949, -0.9792, -0.9432]]],\n",
      "\n",
      "\n",
      "        [[[-1.0265, -0.9546, -0.7234,  ..., -1.0864, -0.9507, -0.9485],\n",
      "          [-1.1524, -0.7740, -0.8287,  ..., -1.0493, -0.6341, -0.9829],\n",
      "          [-1.0898, -0.9084, -1.0392,  ..., -1.0623, -1.0280, -1.0927],\n",
      "          ...,\n",
      "          [-0.9493, -0.9876, -0.8612,  ..., -1.0114, -0.9232, -0.7295],\n",
      "          [-0.9893, -0.8336, -1.0070,  ..., -1.1003, -0.7970, -1.0191],\n",
      "          [-1.0626, -1.0161, -1.1370,  ..., -1.0867, -1.2202, -1.0236]],\n",
      "\n",
      "         [[-1.0188, -0.8660, -1.0688,  ..., -0.8805, -0.9702, -0.9537],\n",
      "          [-0.9432, -0.9998, -1.0372,  ..., -1.0814, -0.9247, -0.9522],\n",
      "          [-1.1065, -1.2530, -1.0435,  ..., -0.9791, -1.0253, -1.0059],\n",
      "          ...,\n",
      "          [-1.0841, -0.9149, -1.0165,  ..., -1.0863, -1.1226, -1.1882],\n",
      "          [-1.0158, -0.9685, -0.9747,  ..., -1.1521, -0.9564, -0.8920],\n",
      "          [-0.9500, -0.8277, -1.0765,  ..., -0.9345, -0.9142, -1.0645]],\n",
      "\n",
      "         [[-0.9799, -1.1235, -0.9955,  ..., -1.2136, -0.9362, -1.1089],\n",
      "          [-1.0394, -1.0957, -0.9546,  ..., -1.0315, -0.9472, -0.8531],\n",
      "          [-0.9858, -0.9212, -0.8942,  ..., -1.0628, -0.9007, -1.0989],\n",
      "          ...,\n",
      "          [-0.9302, -0.9095, -0.9235,  ..., -0.9903, -0.9269, -0.9067],\n",
      "          [-1.0137, -1.0057, -0.9419,  ..., -1.0079, -0.9642, -1.0960],\n",
      "          [-1.1667, -0.8739, -1.0466,  ..., -0.9254, -0.8848, -0.7866]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0552, -1.0209, -1.1050,  ..., -1.0444, -1.0076, -1.0806],\n",
      "          [-0.9508, -0.9580, -0.8537,  ..., -1.0705, -1.0416, -1.0932],\n",
      "          [-1.0838, -0.8708, -1.1030,  ..., -1.0741, -1.0182, -1.0846],\n",
      "          ...,\n",
      "          [-0.9330, -0.8557, -0.9431,  ..., -0.9772, -1.0078, -1.0503],\n",
      "          [-0.9501, -1.0178, -0.8276,  ..., -0.9388, -0.9073, -0.9911],\n",
      "          [-1.0561, -0.8620, -0.9980,  ..., -1.1009, -1.0209, -1.0871]],\n",
      "\n",
      "         [[-0.9075, -0.9720, -1.0154,  ..., -0.8337, -1.0806, -1.0098],\n",
      "          [-1.0245, -0.9256, -1.0343,  ..., -1.0110, -1.0125, -1.0367],\n",
      "          [-0.9070, -1.0611, -1.0403,  ..., -0.7597, -1.0627, -1.0416],\n",
      "          ...,\n",
      "          [-1.0553, -0.9565, -1.1037,  ..., -1.0932, -0.9423, -1.1860],\n",
      "          [-1.1273, -1.0490, -1.0546,  ..., -0.9351, -1.0337, -1.0646],\n",
      "          [-1.0393, -0.9332, -0.8377,  ..., -1.0375, -1.0478, -1.1300]],\n",
      "\n",
      "         [[-1.2405, -0.9196, -1.0723,  ..., -1.0250, -1.0622, -1.0465],\n",
      "          [-1.0694, -0.8841, -0.8156,  ..., -1.1465, -0.7813, -0.9745],\n",
      "          [-0.8778, -1.0116, -0.9173,  ..., -0.9798, -0.9883, -0.8854],\n",
      "          ...,\n",
      "          [-1.2029, -1.0114, -1.2805,  ..., -1.0586, -0.9447, -0.9691],\n",
      "          [-1.0022, -1.0809, -0.9512,  ..., -1.0226, -0.9640, -1.0587],\n",
      "          [-0.8967, -1.1099, -1.0397,  ..., -0.9985, -1.0096, -0.9290]]],\n",
      "\n",
      "\n",
      "        [[[-0.9981, -1.0574, -0.8966,  ..., -1.0009, -0.8858, -0.9232],\n",
      "          [-1.0602, -1.1030, -0.9120,  ..., -1.0301, -1.0680, -0.9844],\n",
      "          [-0.7829, -0.9457, -0.9462,  ..., -0.8414, -1.1531, -0.9384],\n",
      "          ...,\n",
      "          [-0.8316, -0.9890, -0.8588,  ..., -0.8547, -0.9189, -0.8879],\n",
      "          [-0.8214, -0.9422, -0.8659,  ..., -1.0066, -1.1831, -0.9321],\n",
      "          [-1.0642, -1.1813, -1.0054,  ..., -1.0233, -1.1580, -0.9213]],\n",
      "\n",
      "         [[-1.0880, -1.1020, -0.8664,  ..., -1.0321, -1.0166, -0.9577],\n",
      "          [-0.9943, -0.9202, -1.0857,  ..., -0.9426, -1.2248, -0.9205],\n",
      "          [-1.1623, -0.9761, -0.9952,  ..., -0.9291, -1.0910, -1.0764],\n",
      "          ...,\n",
      "          [-0.9318, -1.1091, -0.9211,  ..., -1.0602, -1.0114, -1.0483],\n",
      "          [-1.0252, -0.8807, -0.9393,  ..., -1.1189, -0.7931, -0.9565],\n",
      "          [-1.0165, -1.2448, -0.9268,  ..., -1.0487, -0.9622, -1.0336]],\n",
      "\n",
      "         [[-1.0169, -1.0830, -0.9204,  ..., -1.0715, -0.8571, -1.0300],\n",
      "          [-1.1751, -1.0051, -1.1497,  ..., -0.9689, -0.9736, -1.0502],\n",
      "          [-0.9804, -1.0770, -0.8204,  ..., -1.0329, -1.0299, -1.0380],\n",
      "          ...,\n",
      "          [-1.0069, -0.7162, -1.0640,  ..., -1.1223, -0.8093, -1.0066],\n",
      "          [-1.1558, -0.9225, -0.9950,  ..., -0.8249, -0.9327, -0.9550],\n",
      "          [-1.1399, -0.9697, -1.0669,  ..., -0.9805, -0.8658, -0.8856]]],\n",
      "\n",
      "\n",
      "        [[[-0.7829, -0.9397, -1.0298,  ..., -0.9086, -1.0525, -0.9450],\n",
      "          [-0.8814, -0.8130, -0.9693,  ..., -0.9151, -0.8444, -0.8941],\n",
      "          [-1.0156, -0.8723, -1.1848,  ..., -0.9830, -1.0073, -0.9844],\n",
      "          ...,\n",
      "          [-0.9535, -0.9839, -1.0472,  ..., -1.0291, -0.9401, -1.0592],\n",
      "          [-1.0582, -1.0095, -0.9637,  ..., -1.0634, -1.0525, -0.8159],\n",
      "          [-1.0489, -1.0173, -1.0629,  ..., -1.0527, -0.9206, -1.0963]],\n",
      "\n",
      "         [[-1.0074, -0.8003, -0.9220,  ..., -0.8528, -1.0323, -0.8011],\n",
      "          [-1.0561, -0.8730, -1.0220,  ..., -0.8634, -0.9659, -1.0610],\n",
      "          [-0.7626, -0.9059, -1.0272,  ..., -0.8884, -1.0138, -0.9476],\n",
      "          ...,\n",
      "          [-0.9374, -0.9832, -1.0541,  ..., -0.9761, -0.9968, -1.0300],\n",
      "          [-1.1800, -0.9132, -0.9054,  ..., -1.1000, -0.9806, -0.9615],\n",
      "          [-0.9887, -1.0806, -0.9649,  ..., -0.9891, -1.0193, -0.9346]],\n",
      "\n",
      "         [[-1.0966, -1.0377, -1.0018,  ..., -1.0514, -1.0682, -0.9606],\n",
      "          [-1.2107, -1.0283, -0.9581,  ..., -0.9832, -1.1580, -0.9435],\n",
      "          [-1.0681, -1.1712, -0.8399,  ..., -0.9004, -0.9767, -0.8836],\n",
      "          ...,\n",
      "          [-0.9957, -0.9479, -0.8493,  ..., -0.9046, -0.9287, -0.9098],\n",
      "          [-1.0332, -0.9938, -0.8940,  ..., -0.9019, -0.9858, -0.8858],\n",
      "          [-0.9519, -1.1745, -0.9450,  ..., -1.0117, -0.9034, -0.9372]]]])\n",
      "Layer: features.0.bias, Biases: tensor([-1.0177, -1.2167, -1.0013, -0.9617, -1.2600, -1.0419, -1.1226, -0.9703,\n",
      "        -1.0009, -1.0602, -0.9784, -1.0545, -1.0286, -1.0855, -0.9407, -1.1326,\n",
      "        -1.0708, -0.9366, -0.9361, -0.9736, -0.9990, -1.1563, -0.9524, -0.8849,\n",
      "        -0.9893, -1.0616, -1.0903, -1.0910, -0.8839, -1.0563, -1.2042, -0.9483,\n",
      "        -1.0612, -0.9355, -0.7989, -1.0621, -1.0344, -0.9512, -0.9280, -0.9310,\n",
      "        -1.0032, -0.9044, -0.9622, -1.1925, -0.9142, -0.7697, -0.9757, -0.9118,\n",
      "        -1.1461, -0.8909, -0.9821, -0.9590, -0.9017, -0.8723, -1.2761, -1.0987,\n",
      "        -0.9189, -0.8730, -1.0862, -1.1004, -1.0177, -1.0364, -1.1066, -1.1573,\n",
      "        -1.1244, -0.9888, -0.9272, -1.0612, -1.1858, -1.0498, -1.0043, -1.1178,\n",
      "        -0.9381, -0.8869, -0.8130, -1.1609, -0.7930, -1.0587, -1.1459, -1.0346,\n",
      "        -0.9409, -0.9346, -0.7101, -1.1247, -0.9787, -0.9263, -1.1585, -0.9245,\n",
      "        -0.9032, -0.9941, -0.9159, -0.8649, -1.1122, -0.8356, -1.0055, -0.9017])\n",
      "Layer: features.3.squeeze.weight:, Weights: tensor([[[[-1.0357]],\n",
      "\n",
      "         [[-0.9791]],\n",
      "\n",
      "         [[-0.8489]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8739]],\n",
      "\n",
      "         [[-1.1147]],\n",
      "\n",
      "         [[-1.1102]]],\n",
      "\n",
      "\n",
      "        [[[-1.0804]],\n",
      "\n",
      "         [[-1.0061]],\n",
      "\n",
      "         [[-0.7863]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0531]],\n",
      "\n",
      "         [[-0.9077]],\n",
      "\n",
      "         [[-1.0750]]],\n",
      "\n",
      "\n",
      "        [[[-1.0093]],\n",
      "\n",
      "         [[-1.0809]],\n",
      "\n",
      "         [[-0.8527]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8894]],\n",
      "\n",
      "         [[-1.0745]],\n",
      "\n",
      "         [[-1.0061]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.9672]],\n",
      "\n",
      "         [[-1.0946]],\n",
      "\n",
      "         [[-0.9931]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9637]],\n",
      "\n",
      "         [[-1.0358]],\n",
      "\n",
      "         [[-1.0869]]],\n",
      "\n",
      "\n",
      "        [[[-1.0728]],\n",
      "\n",
      "         [[-0.9014]],\n",
      "\n",
      "         [[-1.0566]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1512]],\n",
      "\n",
      "         [[-1.0098]],\n",
      "\n",
      "         [[-0.9943]]],\n",
      "\n",
      "\n",
      "        [[[-0.9511]],\n",
      "\n",
      "         [[-1.1123]],\n",
      "\n",
      "         [[-0.9444]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9866]],\n",
      "\n",
      "         [[-1.1431]],\n",
      "\n",
      "         [[-0.8483]]]])\n",
      "Layer: features.3.squeeze.bias, Biases: tensor([-1.1126, -0.9574, -0.8856, -0.9551, -0.7656, -0.9364, -1.0386, -0.8961,\n",
      "        -1.0008, -1.0161, -0.9755, -0.9874, -1.1761, -0.9829, -0.9011, -0.7919])\n",
      "Layer: features.3.expand1x1.weight:, Weights: tensor([[[[-1.1244]],\n",
      "\n",
      "         [[-1.0263]],\n",
      "\n",
      "         [[-1.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8167]],\n",
      "\n",
      "         [[-0.9938]],\n",
      "\n",
      "         [[-0.8702]]],\n",
      "\n",
      "\n",
      "        [[[-0.9891]],\n",
      "\n",
      "         [[-1.1405]],\n",
      "\n",
      "         [[-1.0911]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9026]],\n",
      "\n",
      "         [[-1.0793]],\n",
      "\n",
      "         [[-0.8576]]],\n",
      "\n",
      "\n",
      "        [[[-1.0786]],\n",
      "\n",
      "         [[-1.1490]],\n",
      "\n",
      "         [[-1.0841]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1328]],\n",
      "\n",
      "         [[-1.0043]],\n",
      "\n",
      "         [[-1.0567]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0300]],\n",
      "\n",
      "         [[-1.1730]],\n",
      "\n",
      "         [[-0.9224]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0161]],\n",
      "\n",
      "         [[-0.7951]],\n",
      "\n",
      "         [[-1.0076]]],\n",
      "\n",
      "\n",
      "        [[[-0.9371]],\n",
      "\n",
      "         [[-1.1472]],\n",
      "\n",
      "         [[-0.7130]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8455]],\n",
      "\n",
      "         [[-1.1521]],\n",
      "\n",
      "         [[-0.9215]]],\n",
      "\n",
      "\n",
      "        [[[-0.9910]],\n",
      "\n",
      "         [[-0.9676]],\n",
      "\n",
      "         [[-1.0321]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1145]],\n",
      "\n",
      "         [[-0.8730]],\n",
      "\n",
      "         [[-1.0290]]]])\n",
      "Layer: features.3.expand1x1.bias, Biases: tensor([-1.0509, -0.9720, -1.0269, -1.2368, -0.8639, -1.0014, -0.8844, -0.8470,\n",
      "        -1.0852, -1.1592, -0.9674, -0.9075, -0.9839, -1.0528, -0.9421, -1.0598,\n",
      "        -1.0638, -1.0987, -1.0268, -1.0772, -0.9472, -0.9455, -0.9140, -1.0945,\n",
      "        -0.9842, -1.0930, -0.9959, -1.0771, -0.9753, -1.0695, -0.8311, -1.0640,\n",
      "        -0.8950, -1.0830, -0.9116, -1.2595, -0.9961, -0.8890, -1.1177, -1.0123,\n",
      "        -0.9571, -0.8967, -0.9159, -0.8246, -0.9146, -0.9139, -0.9405, -1.1139,\n",
      "        -0.8926, -0.8802, -0.8956, -1.0023, -1.1652, -0.9947, -0.9508, -0.9284,\n",
      "        -1.0770, -1.0357, -0.8984, -0.9301, -0.8284, -0.8225, -0.9754, -0.9566])\n",
      "Layer: features.3.expand3x3.weight:, Weights: tensor([[[[-0.8703, -0.9741, -0.9367],\n",
      "          [-0.9948, -1.1052, -0.9304],\n",
      "          [-1.1924, -1.1007, -0.9745]],\n",
      "\n",
      "         [[-0.9781, -0.9634, -0.9888],\n",
      "          [-1.1718, -0.9514, -0.9509],\n",
      "          [-1.1430, -1.0057, -0.8388]],\n",
      "\n",
      "         [[-1.0772, -0.9657, -1.1464],\n",
      "          [-1.0218, -1.0228, -0.8919],\n",
      "          [-0.8821, -1.0587, -1.1283]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1007, -0.9819, -0.9679],\n",
      "          [-1.0761, -0.8302, -1.1005],\n",
      "          [-0.9865, -1.0215, -1.1071]],\n",
      "\n",
      "         [[-0.8821, -1.0843, -0.8189],\n",
      "          [-0.9994, -0.8466, -0.7559],\n",
      "          [-0.9710, -1.1216, -0.7563]],\n",
      "\n",
      "         [[-1.0067, -0.9692, -1.0276],\n",
      "          [-1.0961, -1.0002, -0.9854],\n",
      "          [-0.8001, -1.2350, -1.0653]]],\n",
      "\n",
      "\n",
      "        [[[-1.0774, -0.9529, -0.9866],\n",
      "          [-0.8043, -1.0316, -0.9449],\n",
      "          [-0.9984, -1.0496, -1.0925]],\n",
      "\n",
      "         [[-0.9811, -0.8361, -1.1319],\n",
      "          [-0.8405, -0.8842, -0.9965],\n",
      "          [-0.8655, -1.0963, -1.1202]],\n",
      "\n",
      "         [[-0.9952, -1.0242, -0.8789],\n",
      "          [-1.0813, -0.9860, -0.9000],\n",
      "          [-1.0145, -1.0912, -0.8617]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8937, -1.0114, -1.1092],\n",
      "          [-0.7951, -0.8183, -0.9097],\n",
      "          [-0.9888, -1.0110, -1.0412]],\n",
      "\n",
      "         [[-1.0053, -1.1436, -0.9019],\n",
      "          [-0.8391, -1.1056, -0.8537],\n",
      "          [-0.8465, -0.9346, -0.8016]],\n",
      "\n",
      "         [[-1.0775, -1.0834, -0.9274],\n",
      "          [-1.0422, -1.1900, -0.9866],\n",
      "          [-1.2100, -0.9867, -0.8339]]],\n",
      "\n",
      "\n",
      "        [[[-0.9162, -1.0560, -0.9659],\n",
      "          [-0.9189, -0.8203, -1.0254],\n",
      "          [-1.0538, -0.7897, -0.9462]],\n",
      "\n",
      "         [[-1.1742, -1.0636, -0.9932],\n",
      "          [-0.8166, -0.9390, -0.9488],\n",
      "          [-1.0700, -0.8671, -0.9393]],\n",
      "\n",
      "         [[-0.8936, -1.1216, -0.9067],\n",
      "          [-1.2405, -0.8336, -0.9215],\n",
      "          [-0.7294, -1.1173, -0.9007]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9860, -1.1721, -0.8613],\n",
      "          [-0.9334, -1.1671, -0.9543],\n",
      "          [-0.8672, -1.0017, -1.2423]],\n",
      "\n",
      "         [[-1.1002, -1.0746, -0.9729],\n",
      "          [-0.9333, -1.1599, -0.9115],\n",
      "          [-0.9866, -1.0361, -1.1756]],\n",
      "\n",
      "         [[-0.9843, -0.9526, -0.9060],\n",
      "          [-1.0683, -1.0255, -0.9876],\n",
      "          [-1.0254, -0.9469, -1.0567]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.9306, -0.8826, -0.9138],\n",
      "          [-1.0507, -0.8250, -1.1667],\n",
      "          [-0.9952, -1.0544, -0.9592]],\n",
      "\n",
      "         [[-0.9583, -0.9844, -0.9624],\n",
      "          [-1.0833, -1.0255, -0.8467],\n",
      "          [-0.9641, -0.9161, -0.9343]],\n",
      "\n",
      "         [[-0.9173, -1.0551, -0.9516],\n",
      "          [-0.9946, -1.1169, -1.1319],\n",
      "          [-1.0623, -1.0214, -0.9456]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1558, -0.8408, -0.8998],\n",
      "          [-1.1171, -0.7136, -0.9918],\n",
      "          [-0.8448, -1.0331, -1.0799]],\n",
      "\n",
      "         [[-1.0771, -1.0861, -1.1065],\n",
      "          [-1.0453, -1.1251, -1.0554],\n",
      "          [-0.9948, -0.9616, -1.0034]],\n",
      "\n",
      "         [[-0.9971, -1.1788, -1.0343],\n",
      "          [-0.9283, -0.9992, -0.8946],\n",
      "          [-1.0050, -0.9944, -0.9082]]],\n",
      "\n",
      "\n",
      "        [[[-1.1071, -0.8767, -0.9897],\n",
      "          [-1.0281, -1.0917, -1.1187],\n",
      "          [-0.9421, -1.1970, -0.9631]],\n",
      "\n",
      "         [[-1.0849, -0.9774, -1.0730],\n",
      "          [-0.9552, -1.0138, -1.0176],\n",
      "          [-0.8852, -0.9738, -1.1289]],\n",
      "\n",
      "         [[-1.0819, -1.0491, -1.0042],\n",
      "          [-0.8015, -0.8765, -1.0826],\n",
      "          [-1.3064, -0.9734, -1.0253]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7934, -0.9122, -1.0525],\n",
      "          [-0.9522, -0.8959, -1.1312],\n",
      "          [-1.2010, -1.0533, -1.2546]],\n",
      "\n",
      "         [[-1.0668, -1.0970, -1.0174],\n",
      "          [-0.9680, -0.9360, -1.1154],\n",
      "          [-1.1228, -1.1298, -1.0533]],\n",
      "\n",
      "         [[-0.8762, -0.8937, -1.1023],\n",
      "          [-1.0325, -1.0155, -1.0733],\n",
      "          [-1.1210, -1.0598, -0.8943]]],\n",
      "\n",
      "\n",
      "        [[[-0.8014, -1.3294, -0.9498],\n",
      "          [-1.1083, -0.9689, -1.0279],\n",
      "          [-0.8979, -1.0097, -0.9516]],\n",
      "\n",
      "         [[-0.9966, -1.0418, -1.0169],\n",
      "          [-1.1368, -0.9017, -1.0759],\n",
      "          [-1.2290, -1.0470, -0.8675]],\n",
      "\n",
      "         [[-1.0871, -0.9609, -0.9162],\n",
      "          [-0.9848, -0.9503, -1.1087],\n",
      "          [-1.0091, -1.0524, -0.8833]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0413, -1.0523, -1.0088],\n",
      "          [-1.0538, -1.0068, -0.8275],\n",
      "          [-1.0057, -0.9559, -1.0521]],\n",
      "\n",
      "         [[-1.0055, -0.9051, -0.9733],\n",
      "          [-1.1272, -1.1822, -1.0698],\n",
      "          [-0.9753, -1.0462, -1.0173]],\n",
      "\n",
      "         [[-0.8060, -0.9454, -0.7911],\n",
      "          [-1.0717, -1.1612, -0.8999],\n",
      "          [-1.0576, -0.8533, -0.9887]]]])\n",
      "Layer: features.3.expand3x3.bias, Biases: tensor([-0.8720, -1.1045, -0.9871, -1.1083, -0.9728, -0.9103, -0.9268, -1.0871,\n",
      "        -0.9271, -0.9483, -0.9327, -0.9548, -0.9090, -1.0689, -0.8898, -1.1916,\n",
      "        -0.7820, -0.7994, -0.8719, -0.8557, -0.8855, -1.0298, -1.1033, -1.0311,\n",
      "        -1.0134, -1.1137, -1.1092, -0.9550, -1.1319, -0.9361, -1.0082, -1.0431,\n",
      "        -0.8818, -1.0082, -0.9602, -1.0691, -0.9948, -0.8954, -0.7552, -0.8437,\n",
      "        -0.9226, -0.9061, -1.1607, -0.9227, -1.0896, -0.9948, -0.9856, -1.0084,\n",
      "        -0.9390, -0.9559, -1.1173, -0.7398, -0.9100, -0.9888, -0.9951, -0.8569,\n",
      "        -0.9122, -1.0884, -0.8180, -1.0836, -0.8592, -1.1924, -1.0928, -1.0755])\n",
      "Layer: features.4.squeeze.weight:, Weights: tensor([[[[-0.9172]],\n",
      "\n",
      "         [[-1.0084]],\n",
      "\n",
      "         [[-1.0959]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9552]],\n",
      "\n",
      "         [[-1.0097]],\n",
      "\n",
      "         [[-1.1084]]],\n",
      "\n",
      "\n",
      "        [[[-1.0697]],\n",
      "\n",
      "         [[-1.0184]],\n",
      "\n",
      "         [[-1.0108]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0785]],\n",
      "\n",
      "         [[-1.0560]],\n",
      "\n",
      "         [[-1.1935]]],\n",
      "\n",
      "\n",
      "        [[[-1.0557]],\n",
      "\n",
      "         [[-1.1543]],\n",
      "\n",
      "         [[-1.0139]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0022]],\n",
      "\n",
      "         [[-0.8953]],\n",
      "\n",
      "         [[-1.0787]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.8824]],\n",
      "\n",
      "         [[-0.9120]],\n",
      "\n",
      "         [[-0.9266]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9717]],\n",
      "\n",
      "         [[-0.9705]],\n",
      "\n",
      "         [[-1.0198]]],\n",
      "\n",
      "\n",
      "        [[[-1.0398]],\n",
      "\n",
      "         [[-0.8876]],\n",
      "\n",
      "         [[-1.0855]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9927]],\n",
      "\n",
      "         [[-0.8636]],\n",
      "\n",
      "         [[-0.9627]]],\n",
      "\n",
      "\n",
      "        [[[-0.9566]],\n",
      "\n",
      "         [[-0.9960]],\n",
      "\n",
      "         [[-0.9533]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8553]],\n",
      "\n",
      "         [[-1.0299]],\n",
      "\n",
      "         [[-0.9654]]]])\n",
      "Layer: features.4.squeeze.bias, Biases: tensor([-0.9993, -1.0819, -1.1717, -1.0847, -1.1321, -1.0206, -1.0937, -0.8005,\n",
      "        -1.0830, -0.9552, -0.9138, -0.8393, -0.9198, -1.1257, -0.8947, -1.0256])\n",
      "Layer: features.4.expand1x1.weight:, Weights: tensor([[[[-0.9025]],\n",
      "\n",
      "         [[-1.0385]],\n",
      "\n",
      "         [[-0.9532]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0285]],\n",
      "\n",
      "         [[-0.8777]],\n",
      "\n",
      "         [[-0.9882]]],\n",
      "\n",
      "\n",
      "        [[[-0.9465]],\n",
      "\n",
      "         [[-1.0251]],\n",
      "\n",
      "         [[-1.1614]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8187]],\n",
      "\n",
      "         [[-0.9881]],\n",
      "\n",
      "         [[-0.9463]]],\n",
      "\n",
      "\n",
      "        [[[-1.0083]],\n",
      "\n",
      "         [[-1.0792]],\n",
      "\n",
      "         [[-1.0447]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0519]],\n",
      "\n",
      "         [[-1.0220]],\n",
      "\n",
      "         [[-0.9477]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0546]],\n",
      "\n",
      "         [[-1.1262]],\n",
      "\n",
      "         [[-0.9602]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8461]],\n",
      "\n",
      "         [[-0.9453]],\n",
      "\n",
      "         [[-1.0460]]],\n",
      "\n",
      "\n",
      "        [[[-0.8557]],\n",
      "\n",
      "         [[-0.9617]],\n",
      "\n",
      "         [[-1.2949]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8565]],\n",
      "\n",
      "         [[-0.9358]],\n",
      "\n",
      "         [[-1.0589]]],\n",
      "\n",
      "\n",
      "        [[[-1.0762]],\n",
      "\n",
      "         [[-1.0724]],\n",
      "\n",
      "         [[-1.0017]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7941]],\n",
      "\n",
      "         [[-1.0332]],\n",
      "\n",
      "         [[-1.0357]]]])\n",
      "Layer: features.4.expand1x1.bias, Biases: tensor([-1.0677, -0.9718, -0.9005, -0.9586, -1.0674, -0.8358, -1.0622, -1.0524,\n",
      "        -0.8096, -0.9752, -0.8458, -0.9515, -0.8717, -0.9631, -1.1109, -1.0295,\n",
      "        -0.9675, -1.0639, -1.1571, -0.9851, -1.0880, -0.9719, -0.9747, -1.0038,\n",
      "        -1.0382, -1.0308, -0.9975, -0.8773, -0.9864, -0.9830, -1.0672, -0.8331,\n",
      "        -1.0870, -0.9394, -0.9024, -0.9555, -0.9683, -1.0593, -1.0290, -0.9858,\n",
      "        -0.8764, -0.8219, -0.8590, -1.1286, -1.0498, -1.0903, -1.0809, -1.1501,\n",
      "        -0.9601, -0.8473, -1.0583, -1.1020, -0.8069, -0.9696, -0.9767, -0.9492,\n",
      "        -1.1403, -1.0135, -0.8852, -0.9575, -1.0663, -1.0719, -1.0366, -1.0717])\n",
      "Layer: features.4.expand3x3.weight:, Weights: tensor([[[[-1.0070, -0.8503, -0.8415],\n",
      "          [-1.0993, -1.1318, -1.0297],\n",
      "          [-1.0788, -1.0746, -1.0317]],\n",
      "\n",
      "         [[-1.0475, -1.0642, -1.2511],\n",
      "          [-1.0323, -1.0146, -0.8608],\n",
      "          [-1.0226, -0.8803, -1.0554]],\n",
      "\n",
      "         [[-1.0691, -1.1811, -1.0554],\n",
      "          [-0.9953, -0.8535, -0.8767],\n",
      "          [-1.0555, -1.0402, -1.1556]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9857, -0.9783, -0.9794],\n",
      "          [-1.1970, -0.9158, -1.1249],\n",
      "          [-1.1124, -1.0255, -0.9849]],\n",
      "\n",
      "         [[-1.0313, -0.9474, -0.9665],\n",
      "          [-1.0943, -1.0544, -1.0119],\n",
      "          [-0.9694, -0.8977, -0.9473]],\n",
      "\n",
      "         [[-1.1038, -1.2782, -1.0396],\n",
      "          [-1.0309, -1.0579, -1.0590],\n",
      "          [-1.1067, -0.9912, -1.1734]]],\n",
      "\n",
      "\n",
      "        [[[-0.9318, -1.1699, -1.0112],\n",
      "          [-0.8364, -1.1184, -1.0966],\n",
      "          [-1.0092, -1.0586, -1.0081]],\n",
      "\n",
      "         [[-0.9656, -1.0968, -1.0509],\n",
      "          [-1.0972, -0.9883, -1.0543],\n",
      "          [-1.0361, -0.8217, -0.9969]],\n",
      "\n",
      "         [[-0.8397, -1.0973, -0.9219],\n",
      "          [-1.0960, -0.9308, -0.8859],\n",
      "          [-0.9956, -0.9463, -0.8840]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0868, -1.1007, -1.1330],\n",
      "          [-1.1793, -1.0567, -0.8904],\n",
      "          [-1.0102, -1.0115, -0.8758]],\n",
      "\n",
      "         [[-1.1081, -0.9715, -0.8608],\n",
      "          [-1.0695, -1.0334, -0.9076],\n",
      "          [-0.9851, -1.0401, -1.0913]],\n",
      "\n",
      "         [[-1.0696, -1.0089, -1.0455],\n",
      "          [-0.8624, -1.0623, -1.0400],\n",
      "          [-1.1634, -1.0998, -1.0967]]],\n",
      "\n",
      "\n",
      "        [[[-0.9521, -1.1064, -1.1109],\n",
      "          [-1.1050, -1.1063, -1.1690],\n",
      "          [-1.0574, -1.0833, -0.9917]],\n",
      "\n",
      "         [[-1.0819, -1.0374, -1.1579],\n",
      "          [-1.0535, -1.1303, -1.0025],\n",
      "          [-1.0731, -0.8953, -1.0050]],\n",
      "\n",
      "         [[-0.8506, -1.2332, -0.8839],\n",
      "          [-1.0040, -0.9689, -0.8623],\n",
      "          [-1.0245, -1.1501, -1.0694]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0566, -1.0427, -1.1082],\n",
      "          [-0.8951, -0.9474, -0.9923],\n",
      "          [-0.9308, -1.0984, -1.0149]],\n",
      "\n",
      "         [[-1.1089, -1.0595, -1.0665],\n",
      "          [-0.9321, -1.0038, -1.1360],\n",
      "          [-1.0211, -1.0289, -0.9894]],\n",
      "\n",
      "         [[-0.9965, -0.8619, -0.9846],\n",
      "          [-1.0259, -0.9093, -1.0051],\n",
      "          [-0.9950, -1.0081, -0.9602]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0548, -0.8831, -1.0520],\n",
      "          [-1.1076, -1.1302, -0.9540],\n",
      "          [-1.0487, -0.9512, -1.0785]],\n",
      "\n",
      "         [[-0.8569, -0.8938, -1.0503],\n",
      "          [-1.1704, -0.9660, -0.9992],\n",
      "          [-1.1362, -0.9374, -0.9552]],\n",
      "\n",
      "         [[-1.1671, -1.0779, -0.9262],\n",
      "          [-0.9209, -0.7912, -1.0014],\n",
      "          [-1.0325, -0.8589, -1.0549]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9456, -1.0327, -1.0092],\n",
      "          [-1.1335, -0.8674, -0.7939],\n",
      "          [-0.8609, -1.0625, -0.8936]],\n",
      "\n",
      "         [[-1.0659, -1.0421, -1.0667],\n",
      "          [-0.9834, -1.1072, -0.9095],\n",
      "          [-1.0704, -1.0131, -0.7018]],\n",
      "\n",
      "         [[-1.0075, -1.0205, -1.0046],\n",
      "          [-1.0444, -1.1136, -1.0281],\n",
      "          [-0.9983, -1.0338, -1.0456]]],\n",
      "\n",
      "\n",
      "        [[[-0.9676, -0.9771, -0.9246],\n",
      "          [-0.8742, -1.0067, -1.1470],\n",
      "          [-0.7827, -1.0745, -0.9396]],\n",
      "\n",
      "         [[-1.0096, -1.1602, -1.0488],\n",
      "          [-0.9434, -1.2146, -1.0708],\n",
      "          [-1.1786, -0.9374, -1.0760]],\n",
      "\n",
      "         [[-1.0665, -0.9826, -1.0464],\n",
      "          [-0.8803, -1.0121, -1.1206],\n",
      "          [-1.0082, -1.1209, -1.1195]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1056, -0.9896, -0.9624],\n",
      "          [-1.1013, -0.9473, -1.0523],\n",
      "          [-0.9871, -1.0887, -1.0356]],\n",
      "\n",
      "         [[-0.8955, -0.8763, -0.9410],\n",
      "          [-1.1138, -1.0018, -1.0668],\n",
      "          [-1.0869, -1.0931, -1.1587]],\n",
      "\n",
      "         [[-0.9461, -0.9234, -0.8997],\n",
      "          [-0.9747, -1.0644, -0.9806],\n",
      "          [-0.9099, -1.0270, -0.7859]]],\n",
      "\n",
      "\n",
      "        [[[-1.1515, -0.9883, -0.8556],\n",
      "          [-0.9496, -0.7987, -1.0071],\n",
      "          [-0.9398, -1.0455, -0.8881]],\n",
      "\n",
      "         [[-1.0643, -0.8856, -1.0304],\n",
      "          [-0.8918, -0.8144, -0.8991],\n",
      "          [-0.9355, -1.1418, -0.9201]],\n",
      "\n",
      "         [[-0.9355, -0.9434, -0.9829],\n",
      "          [-1.0430, -1.0290, -0.7323],\n",
      "          [-1.0390, -1.1166, -1.1664]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0182, -0.9357, -0.8800],\n",
      "          [-1.0566, -0.9717, -1.1308],\n",
      "          [-1.0367, -0.8821, -1.0729]],\n",
      "\n",
      "         [[-1.0469, -1.0022, -1.0317],\n",
      "          [-1.1633, -1.0360, -1.0329],\n",
      "          [-1.2804, -0.9561, -0.9272]],\n",
      "\n",
      "         [[-0.9374, -0.9683, -0.8582],\n",
      "          [-0.9116, -1.0542, -0.9614],\n",
      "          [-1.0382, -1.0997, -1.0781]]]])\n",
      "Layer: features.4.expand3x3.bias, Biases: tensor([-0.8698, -1.0119, -0.9987, -1.0509, -1.0023, -1.1135, -0.7328, -1.1917,\n",
      "        -0.9537, -0.7676, -1.1464, -0.8773, -1.0564, -0.9703, -0.9249, -0.9465,\n",
      "        -1.0045, -1.0973, -1.0764, -0.9159, -0.9536, -0.9870, -1.1086, -1.1294,\n",
      "        -0.9889, -1.1150, -0.9638, -1.0769, -0.9825, -0.9585, -1.0115, -1.0281,\n",
      "        -0.9125, -1.0926, -0.9512, -0.9978, -1.0491, -0.9429, -0.9220, -0.9168,\n",
      "        -1.1703, -0.9131, -1.0853, -0.8584, -1.0887, -1.0669, -0.9613, -1.1222,\n",
      "        -1.0499, -0.9504, -1.0353, -1.0672, -1.0521, -0.9955, -0.9871, -1.0307,\n",
      "        -1.0517, -1.0683, -0.9849, -0.9733, -0.8872, -1.0839, -1.0115, -1.0766])\n",
      "Layer: features.5.squeeze.weight:, Weights: tensor([[[[-0.8567]],\n",
      "\n",
      "         [[-1.2524]],\n",
      "\n",
      "         [[-0.8631]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9393]],\n",
      "\n",
      "         [[-1.1811]],\n",
      "\n",
      "         [[-1.3431]]],\n",
      "\n",
      "\n",
      "        [[[-1.0283]],\n",
      "\n",
      "         [[-0.8876]],\n",
      "\n",
      "         [[-0.9855]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1323]],\n",
      "\n",
      "         [[-0.8492]],\n",
      "\n",
      "         [[-1.0283]]],\n",
      "\n",
      "\n",
      "        [[[-1.1600]],\n",
      "\n",
      "         [[-0.9306]],\n",
      "\n",
      "         [[-1.0451]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0103]],\n",
      "\n",
      "         [[-1.1525]],\n",
      "\n",
      "         [[-0.9518]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.1702]],\n",
      "\n",
      "         [[-1.1277]],\n",
      "\n",
      "         [[-0.8684]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0028]],\n",
      "\n",
      "         [[-0.9198]],\n",
      "\n",
      "         [[-1.0629]]],\n",
      "\n",
      "\n",
      "        [[[-1.0931]],\n",
      "\n",
      "         [[-0.9692]],\n",
      "\n",
      "         [[-0.9650]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9608]],\n",
      "\n",
      "         [[-1.1459]],\n",
      "\n",
      "         [[-0.9062]]],\n",
      "\n",
      "\n",
      "        [[[-0.9754]],\n",
      "\n",
      "         [[-1.0794]],\n",
      "\n",
      "         [[-1.0031]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9793]],\n",
      "\n",
      "         [[-1.0160]],\n",
      "\n",
      "         [[-1.0722]]]])\n",
      "Layer: features.5.squeeze.bias, Biases: tensor([-1.0550, -0.8780, -1.0040, -1.0619, -1.0977, -1.0083, -1.1838, -0.8824,\n",
      "        -0.8673, -1.0708, -0.8364, -0.9130, -1.0381, -1.1315, -0.8991, -0.9153,\n",
      "        -1.0950, -0.9700, -1.0046, -0.9901, -0.9310, -0.9690, -1.1483, -1.1220,\n",
      "        -1.1125, -1.1875, -1.1351, -1.1242, -1.0600, -1.0386, -1.0807, -1.1352])\n",
      "Layer: features.5.expand1x1.weight:, Weights: tensor([[[[-1.0826]],\n",
      "\n",
      "         [[-0.9709]],\n",
      "\n",
      "         [[-1.0430]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9919]],\n",
      "\n",
      "         [[-1.0554]],\n",
      "\n",
      "         [[-0.9740]]],\n",
      "\n",
      "\n",
      "        [[[-0.9678]],\n",
      "\n",
      "         [[-0.8926]],\n",
      "\n",
      "         [[-0.9401]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9565]],\n",
      "\n",
      "         [[-0.8162]],\n",
      "\n",
      "         [[-0.7227]]],\n",
      "\n",
      "\n",
      "        [[[-0.9908]],\n",
      "\n",
      "         [[-0.9615]],\n",
      "\n",
      "         [[-0.9905]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0859]],\n",
      "\n",
      "         [[-0.9900]],\n",
      "\n",
      "         [[-1.0201]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0269]],\n",
      "\n",
      "         [[-0.8501]],\n",
      "\n",
      "         [[-1.0338]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0018]],\n",
      "\n",
      "         [[-1.1418]],\n",
      "\n",
      "         [[-0.7692]]],\n",
      "\n",
      "\n",
      "        [[[-1.1296]],\n",
      "\n",
      "         [[-1.0376]],\n",
      "\n",
      "         [[-0.9975]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1238]],\n",
      "\n",
      "         [[-1.0452]],\n",
      "\n",
      "         [[-1.0854]]],\n",
      "\n",
      "\n",
      "        [[[-1.0817]],\n",
      "\n",
      "         [[-0.9602]],\n",
      "\n",
      "         [[-1.1201]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0877]],\n",
      "\n",
      "         [[-1.1149]],\n",
      "\n",
      "         [[-0.9115]]]])\n",
      "Layer: features.5.expand1x1.bias, Biases: tensor([-1.1206, -1.0290, -1.0487, -1.1848, -0.9828, -0.7743, -0.8525, -0.9198,\n",
      "        -0.9716, -0.9371, -1.0037, -1.0874, -1.0232, -0.9474, -0.9837, -1.0280,\n",
      "        -1.0013, -1.1797, -0.9144, -1.1641, -0.9240, -1.1043, -1.0647, -0.9865,\n",
      "        -0.8967, -1.0187, -1.0083, -1.1128, -0.8829, -1.1652, -0.9245, -0.8820,\n",
      "        -0.9330, -0.8903, -1.1186, -0.8684, -0.9536, -1.0754, -1.0708, -1.0287,\n",
      "        -0.9422, -1.2016, -0.7712, -1.0811, -0.9944, -1.0630, -1.0921, -1.0815,\n",
      "        -1.0906, -1.0226, -1.0431, -0.9990, -1.0830, -1.0433, -1.3480, -0.9827,\n",
      "        -1.0639, -0.9781, -0.7882, -0.8682, -1.0184, -1.2571, -1.0307, -0.9126,\n",
      "        -0.9082, -0.8193, -0.8785, -0.9860, -0.9307, -0.9323, -1.0656, -0.8969,\n",
      "        -0.9103, -0.7654, -0.9472, -0.9961, -1.0141, -0.9778, -0.8364, -0.8763,\n",
      "        -1.0142, -1.0473, -0.8550, -1.2121, -0.9131, -0.9063, -1.0086, -0.8770,\n",
      "        -1.0255, -1.0630, -1.0819, -0.8443, -1.0026, -1.0801, -1.0363, -1.1161,\n",
      "        -0.9314, -1.0290, -0.8340, -0.9726, -0.9203, -0.8309, -1.1364, -1.1763,\n",
      "        -1.1864, -0.9175, -1.0198, -1.0356, -0.9874, -1.0974, -0.8536, -0.9816,\n",
      "        -0.9612, -0.9746, -1.0152, -0.9086, -0.9708, -0.8685, -1.1049, -0.9937,\n",
      "        -0.9868, -0.9609, -0.8335, -0.8201, -1.0685, -0.8836, -0.8260, -1.0157])\n",
      "Layer: features.5.expand3x3.weight:, Weights: tensor([[[[-0.9199, -1.1404, -0.9479],\n",
      "          [-0.9390, -1.1889, -1.0650],\n",
      "          [-1.0550, -1.0209, -0.8664]],\n",
      "\n",
      "         [[-1.1341, -1.1555, -1.2052],\n",
      "          [-0.9486, -1.2165, -0.9391],\n",
      "          [-1.0709, -0.9755, -1.1247]],\n",
      "\n",
      "         [[-0.9439, -1.1345, -0.9841],\n",
      "          [-0.9705, -0.8957, -0.8997],\n",
      "          [-0.9185, -0.8560, -1.0212]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0829, -1.0613, -1.1949],\n",
      "          [-1.0719, -0.9765, -0.9617],\n",
      "          [-0.9009, -1.0896, -1.0109]],\n",
      "\n",
      "         [[-1.1538, -1.1054, -0.9736],\n",
      "          [-0.9008, -1.1245, -0.9328],\n",
      "          [-0.9853, -1.0938, -0.9585]],\n",
      "\n",
      "         [[-1.0269, -1.0928, -1.1568],\n",
      "          [-1.0933, -0.9890, -1.0067],\n",
      "          [-1.0128, -1.0815, -1.1100]]],\n",
      "\n",
      "\n",
      "        [[[-1.0275, -1.0372, -1.1830],\n",
      "          [-0.8770, -1.0452, -1.1299],\n",
      "          [-0.9221, -1.0517, -0.7934]],\n",
      "\n",
      "         [[-0.8188, -1.0914, -1.0924],\n",
      "          [-1.0555, -1.1921, -0.8910],\n",
      "          [-1.0388, -0.9732, -1.0365]],\n",
      "\n",
      "         [[-1.0548, -0.8492, -1.0550],\n",
      "          [-1.0787, -1.0477, -0.8933],\n",
      "          [-1.0537, -0.8453, -0.9969]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0283, -0.9852, -1.1102],\n",
      "          [-0.7401, -0.9543, -1.0048],\n",
      "          [-0.8709, -1.0976, -1.0693]],\n",
      "\n",
      "         [[-1.1589, -1.0103, -1.1078],\n",
      "          [-0.9814, -0.9513, -0.9928],\n",
      "          [-0.8777, -1.0383, -0.8990]],\n",
      "\n",
      "         [[-1.0877, -0.9816, -1.0885],\n",
      "          [-1.1479, -0.9534, -1.1089],\n",
      "          [-1.0913, -1.0056, -0.9693]]],\n",
      "\n",
      "\n",
      "        [[[-1.0624, -0.9778, -0.8155],\n",
      "          [-1.1174, -0.9373, -0.8645],\n",
      "          [-1.0156, -0.9340, -0.8438]],\n",
      "\n",
      "         [[-0.9564, -1.2093, -0.8914],\n",
      "          [-0.9828, -1.1155, -1.1870],\n",
      "          [-0.8793, -0.8739, -0.9949]],\n",
      "\n",
      "         [[-1.2698, -1.0244, -1.0687],\n",
      "          [-1.0977, -0.8940, -0.8758],\n",
      "          [-1.0460, -1.1276, -1.0200]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0567, -0.9686, -1.0655],\n",
      "          [-0.9810, -0.9901, -1.0632],\n",
      "          [-1.0776, -0.9663, -0.9848]],\n",
      "\n",
      "         [[-1.0328, -0.9642, -1.2417],\n",
      "          [-0.9954, -1.0105, -1.0337],\n",
      "          [-0.9537, -0.9082, -1.3064]],\n",
      "\n",
      "         [[-1.1462, -1.0925, -1.1125],\n",
      "          [-1.0343, -1.0407, -1.0513],\n",
      "          [-1.2117, -0.9678, -0.8447]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.8907, -0.9183, -1.0586],\n",
      "          [-0.8725, -0.9785, -1.0097],\n",
      "          [-1.0228, -0.9588, -1.1756]],\n",
      "\n",
      "         [[-1.0703, -0.9920, -1.0284],\n",
      "          [-0.8441, -0.9912, -1.1263],\n",
      "          [-0.8835, -1.0234, -1.0066]],\n",
      "\n",
      "         [[-1.0771, -0.9815, -0.9941],\n",
      "          [-1.0289, -0.9616, -1.0769],\n",
      "          [-0.9135, -0.9299, -0.8339]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0529, -1.0442, -1.0371],\n",
      "          [-1.0126, -1.1681, -0.9444],\n",
      "          [-0.8284, -1.0194, -0.8834]],\n",
      "\n",
      "         [[-1.0022, -1.1437, -1.0064],\n",
      "          [-1.0982, -0.9599, -1.0314],\n",
      "          [-0.7790, -0.9533, -1.0105]],\n",
      "\n",
      "         [[-0.9808, -1.0285, -1.0016],\n",
      "          [-1.0502, -1.0096, -1.0939],\n",
      "          [-0.8552, -0.8432, -0.9027]]],\n",
      "\n",
      "\n",
      "        [[[-1.0132, -1.0913, -0.8262],\n",
      "          [-1.1288, -1.0080, -0.9376],\n",
      "          [-0.9986, -1.0433, -1.0414]],\n",
      "\n",
      "         [[-0.8182, -0.9938, -0.9643],\n",
      "          [-1.0028, -1.0759, -1.0290],\n",
      "          [-0.9416, -0.9724, -0.9842]],\n",
      "\n",
      "         [[-0.9563, -0.7726, -0.9308],\n",
      "          [-1.0841, -1.1689, -1.0443],\n",
      "          [-1.2419, -0.9591, -0.8620]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9693, -0.9633, -0.9847],\n",
      "          [-0.8834, -1.1348, -1.0192],\n",
      "          [-1.0534, -0.8885, -1.0405]],\n",
      "\n",
      "         [[-0.9834, -1.1161, -0.9050],\n",
      "          [-0.9835, -0.9517, -1.1661],\n",
      "          [-0.8901, -0.9269, -0.9207]],\n",
      "\n",
      "         [[-1.0190, -1.1154, -1.1105],\n",
      "          [-0.9671, -1.0803, -1.0937],\n",
      "          [-1.0363, -0.9742, -1.0505]]],\n",
      "\n",
      "\n",
      "        [[[-0.8518, -1.0399, -1.0708],\n",
      "          [-0.8871, -1.0582, -1.0631],\n",
      "          [-0.9944, -1.0666, -1.0268]],\n",
      "\n",
      "         [[-1.0009, -0.9556, -1.1657],\n",
      "          [-0.9854, -0.9738, -0.8819],\n",
      "          [-1.0877, -0.9156, -0.7958]],\n",
      "\n",
      "         [[-0.8806, -0.8531, -1.2626],\n",
      "          [-0.8968, -1.0807, -1.0930],\n",
      "          [-1.1427, -1.0052, -1.0018]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0095, -1.0439, -1.0977],\n",
      "          [-0.8279, -0.8390, -0.7768],\n",
      "          [-1.0091, -1.0261, -0.8854]],\n",
      "\n",
      "         [[-0.8891, -0.9190, -0.9449],\n",
      "          [-1.0588, -1.3342, -0.9791],\n",
      "          [-0.9341, -0.9342, -1.0318]],\n",
      "\n",
      "         [[-0.9257, -0.9838, -0.9773],\n",
      "          [-1.0954, -1.0054, -1.0654],\n",
      "          [-1.0667, -1.0246, -1.0294]]]])\n",
      "Layer: features.5.expand3x3.bias, Biases: tensor([-0.9313, -0.9275, -0.9125, -0.8988, -1.0851, -0.9213, -0.9891, -0.9683,\n",
      "        -0.8021, -1.0000, -1.1157, -1.0230, -0.9388, -0.9856, -0.9267, -1.0651,\n",
      "        -1.1603, -1.1461, -1.0547, -1.1163, -1.0439, -0.9292, -1.2096, -1.1316,\n",
      "        -0.9399, -1.2320, -0.9266, -1.0100, -1.1715, -1.1202, -1.0093, -0.9395,\n",
      "        -0.9026, -0.9725, -0.9041, -1.2030, -1.1960, -0.7502, -1.0259, -1.0330,\n",
      "        -1.0765, -1.1448, -1.0701, -1.0835, -0.7827, -1.0577, -1.0155, -0.9563,\n",
      "        -0.9225, -1.0098, -1.0465, -1.0076, -1.0054, -1.1911, -0.9769, -1.0121,\n",
      "        -0.9111, -1.1323, -0.9910, -1.1514, -0.9778, -0.9751, -0.9361, -0.9792,\n",
      "        -1.1875, -1.0931, -1.0480, -0.9110, -1.1711, -0.9381, -1.0366, -0.9696,\n",
      "        -0.9676, -0.9836, -0.9755, -0.9640, -1.1177, -0.9231, -1.0477, -1.0076,\n",
      "        -1.0299, -1.0449, -1.0775, -1.1440, -1.0795, -0.9132, -0.9396, -0.7824,\n",
      "        -1.0558, -0.9801, -0.7616, -0.8778, -1.0200, -0.9045, -0.9979, -0.6612,\n",
      "        -1.1264, -0.9915, -0.8917, -1.0298, -0.9644, -0.9380, -1.0449, -0.8864,\n",
      "        -0.9910, -0.9345, -1.1820, -0.9980, -0.9839, -0.9985, -1.0443, -1.1122,\n",
      "        -1.1376, -0.8119, -0.9034, -0.9112, -1.0503, -1.0024, -0.9334, -0.9796,\n",
      "        -1.0986, -0.9780, -1.0731, -1.1032, -1.1075, -1.0015, -0.9278, -0.8696])\n",
      "Layer: features.7.squeeze.weight:, Weights: tensor([[[[-0.8832]],\n",
      "\n",
      "         [[-0.8211]],\n",
      "\n",
      "         [[-0.9811]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1859]],\n",
      "\n",
      "         [[-0.8535]],\n",
      "\n",
      "         [[-0.9926]]],\n",
      "\n",
      "\n",
      "        [[[-0.9538]],\n",
      "\n",
      "         [[-1.0890]],\n",
      "\n",
      "         [[-0.9547]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7824]],\n",
      "\n",
      "         [[-1.0904]],\n",
      "\n",
      "         [[-1.0808]]],\n",
      "\n",
      "\n",
      "        [[[-1.1251]],\n",
      "\n",
      "         [[-1.0568]],\n",
      "\n",
      "         [[-0.9563]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0423]],\n",
      "\n",
      "         [[-0.8285]],\n",
      "\n",
      "         [[-1.1027]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0384]],\n",
      "\n",
      "         [[-1.0479]],\n",
      "\n",
      "         [[-1.1189]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9397]],\n",
      "\n",
      "         [[-1.0566]],\n",
      "\n",
      "         [[-1.0696]]],\n",
      "\n",
      "\n",
      "        [[[-0.8726]],\n",
      "\n",
      "         [[-1.0882]],\n",
      "\n",
      "         [[-0.9378]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8914]],\n",
      "\n",
      "         [[-0.8743]],\n",
      "\n",
      "         [[-0.9636]]],\n",
      "\n",
      "\n",
      "        [[[-1.0674]],\n",
      "\n",
      "         [[-0.8956]],\n",
      "\n",
      "         [[-0.8581]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0831]],\n",
      "\n",
      "         [[-0.9063]],\n",
      "\n",
      "         [[-1.0009]]]])\n",
      "Layer: features.7.squeeze.bias, Biases: tensor([-1.0712, -1.1356, -0.9940, -1.0306, -1.0627, -0.9947, -1.0235, -0.9925,\n",
      "        -0.9714, -0.9681, -1.0074, -0.8808, -0.9718, -1.0104, -0.9429, -0.9688,\n",
      "        -1.1394, -1.0435, -1.0619, -1.0797, -1.0152, -0.9980, -0.8664, -1.0680,\n",
      "        -0.9895, -1.0121, -0.9799, -0.9838, -1.0476, -1.1164, -0.9667, -0.9299])\n",
      "Layer: features.7.expand1x1.weight:, Weights: tensor([[[[-1.1614]],\n",
      "\n",
      "         [[-0.9347]],\n",
      "\n",
      "         [[-1.0399]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9833]],\n",
      "\n",
      "         [[-0.9140]],\n",
      "\n",
      "         [[-1.1840]]],\n",
      "\n",
      "\n",
      "        [[[-1.0278]],\n",
      "\n",
      "         [[-1.1997]],\n",
      "\n",
      "         [[-0.9180]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8442]],\n",
      "\n",
      "         [[-0.8924]],\n",
      "\n",
      "         [[-1.0788]]],\n",
      "\n",
      "\n",
      "        [[[-0.9307]],\n",
      "\n",
      "         [[-1.0624]],\n",
      "\n",
      "         [[-1.0513]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0046]],\n",
      "\n",
      "         [[-1.0082]],\n",
      "\n",
      "         [[-1.0675]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.8946]],\n",
      "\n",
      "         [[-1.0882]],\n",
      "\n",
      "         [[-1.0572]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9751]],\n",
      "\n",
      "         [[-0.9799]],\n",
      "\n",
      "         [[-1.0581]]],\n",
      "\n",
      "\n",
      "        [[[-0.7876]],\n",
      "\n",
      "         [[-1.0648]],\n",
      "\n",
      "         [[-1.2359]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9350]],\n",
      "\n",
      "         [[-0.9368]],\n",
      "\n",
      "         [[-0.9024]]],\n",
      "\n",
      "\n",
      "        [[[-0.8944]],\n",
      "\n",
      "         [[-1.0676]],\n",
      "\n",
      "         [[-0.9202]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9958]],\n",
      "\n",
      "         [[-0.8824]],\n",
      "\n",
      "         [[-1.0172]]]])\n",
      "Layer: features.7.expand1x1.bias, Biases: tensor([-0.9528, -0.9493, -0.9691, -1.0353, -0.8616, -0.9070, -0.8209, -1.0056,\n",
      "        -0.9313, -0.9968, -1.0312, -1.0691, -0.9728, -0.9412, -0.7626, -1.1293,\n",
      "        -1.0395, -0.7788, -0.9559, -0.8280, -0.9372, -0.6434, -0.9526, -0.8821,\n",
      "        -1.0806, -1.0577, -1.0636, -0.9101, -0.9618, -1.1336, -1.0231, -0.8634,\n",
      "        -0.9597, -0.7884, -0.9089, -0.9242, -1.0830, -0.9773, -0.9219, -0.8072,\n",
      "        -1.0287, -0.9511, -0.9093, -1.0228, -1.0282, -0.9056, -1.0564, -0.9052,\n",
      "        -1.1122, -1.0057, -1.1442, -1.0238, -0.9811, -1.0996, -0.9647, -0.8654,\n",
      "        -0.8597, -1.0507, -1.0030, -0.9820, -0.8903, -1.2086, -0.9416, -1.0922,\n",
      "        -1.0861, -0.9421, -0.9574, -0.9549, -1.0061, -0.8137, -0.9212, -1.0147,\n",
      "        -1.1060, -1.1335, -0.9698, -0.9075, -0.7332, -1.1175, -0.9736, -0.9950,\n",
      "        -0.9713, -1.0991, -0.8995, -0.8993, -1.0031, -1.2084, -0.9813, -0.7842,\n",
      "        -0.9484, -0.8422, -1.1191, -0.9624, -0.9729, -0.9357, -1.0338, -0.7036,\n",
      "        -0.9427, -0.8568, -1.1341, -0.9138, -1.1122, -1.0428, -0.9128, -0.8907,\n",
      "        -1.1231, -0.9338, -0.9747, -1.0160, -0.9019, -1.1847, -0.9905, -0.9687,\n",
      "        -0.7878, -1.1370, -1.0209, -1.1005, -1.0444, -0.9969, -1.0005, -1.1032,\n",
      "        -0.9436, -0.9848, -1.1023, -1.1263, -0.9645, -0.9300, -0.8404, -1.0740])\n",
      "Layer: features.7.expand3x3.weight:, Weights: tensor([[[[-0.9435, -0.8767, -0.7952],\n",
      "          [-0.9689, -1.0216, -1.0313],\n",
      "          [-0.9328, -0.9456, -0.9728]],\n",
      "\n",
      "         [[-1.0837, -1.1748, -0.8445],\n",
      "          [-1.0166, -1.0094, -1.0254],\n",
      "          [-1.0391, -0.9069, -1.1106]],\n",
      "\n",
      "         [[-0.9932, -0.9079, -1.0282],\n",
      "          [-1.0536, -1.0690, -0.8026],\n",
      "          [-0.9781, -1.0912, -1.0862]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9328, -1.1299, -1.1745],\n",
      "          [-0.9566, -1.0734, -1.1343],\n",
      "          [-1.0062, -1.1693, -0.8350]],\n",
      "\n",
      "         [[-1.1236, -1.0172, -1.0416],\n",
      "          [-0.9296, -1.0703, -1.1013],\n",
      "          [-0.9616, -0.7523, -1.2158]],\n",
      "\n",
      "         [[-1.0566, -0.9070, -1.0512],\n",
      "          [-1.0232, -1.0884, -0.9827],\n",
      "          [-1.0922, -1.0215, -0.9478]]],\n",
      "\n",
      "\n",
      "        [[[-0.9801, -0.8750, -1.1293],\n",
      "          [-0.8246, -0.8320, -1.0754],\n",
      "          [-0.8880, -0.9575, -0.9054]],\n",
      "\n",
      "         [[-0.8880, -0.8683, -0.8061],\n",
      "          [-0.8511, -0.8746, -1.0868],\n",
      "          [-1.0732, -0.9643, -1.0649]],\n",
      "\n",
      "         [[-1.1851, -1.1289, -1.0433],\n",
      "          [-0.9567, -1.0152, -0.8048],\n",
      "          [-1.0169, -0.8174, -0.9608]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0045, -0.9971, -1.0379],\n",
      "          [-0.8238, -1.1081, -1.0419],\n",
      "          [-0.7616, -0.9164, -0.8818]],\n",
      "\n",
      "         [[-0.9621, -0.8679, -0.8098],\n",
      "          [-1.1046, -1.0068, -1.1315],\n",
      "          [-1.0274, -0.9750, -0.9489]],\n",
      "\n",
      "         [[-1.0626, -1.0251, -1.1902],\n",
      "          [-0.8467, -0.9306, -0.9279],\n",
      "          [-1.0143, -1.0464, -1.1218]]],\n",
      "\n",
      "\n",
      "        [[[-0.8243, -0.9923, -0.9897],\n",
      "          [-0.8137, -1.0862, -0.9218],\n",
      "          [-0.9818, -1.0138, -1.0346]],\n",
      "\n",
      "         [[-0.9256, -0.9597, -1.0458],\n",
      "          [-0.9757, -1.1534, -1.0437],\n",
      "          [-0.9804, -1.0649, -0.8904]],\n",
      "\n",
      "         [[-1.1190, -1.0359, -0.9467],\n",
      "          [-1.0481, -0.9618, -0.9396],\n",
      "          [-1.0672, -1.1011, -1.0462]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1428, -0.9595, -1.0192],\n",
      "          [-0.9933, -0.8718, -0.9531],\n",
      "          [-1.1285, -1.1615, -0.9448]],\n",
      "\n",
      "         [[-1.0670, -1.0378, -0.9969],\n",
      "          [-0.9541, -0.9817, -1.0504],\n",
      "          [-1.0102, -0.9540, -0.8745]],\n",
      "\n",
      "         [[-0.9784, -1.1902, -1.2023],\n",
      "          [-1.0275, -0.9979, -0.9964],\n",
      "          [-1.0323, -1.2253, -1.0369]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.9189, -1.1163, -0.8028],\n",
      "          [-0.9998, -1.0264, -1.1649],\n",
      "          [-1.1117, -0.8929, -1.0182]],\n",
      "\n",
      "         [[-0.8222, -0.9265, -1.1200],\n",
      "          [-0.8510, -1.0130, -1.1232],\n",
      "          [-0.8688, -0.9224, -1.1115]],\n",
      "\n",
      "         [[-0.9063, -0.9957, -1.0815],\n",
      "          [-1.0616, -0.9178, -1.0736],\n",
      "          [-0.8687, -1.1105, -1.0641]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0467, -0.8003, -0.9096],\n",
      "          [-1.2338, -0.9436, -0.9349],\n",
      "          [-1.1146, -0.9289, -1.1390]],\n",
      "\n",
      "         [[-1.0415, -1.0447, -0.9004],\n",
      "          [-0.9427, -0.9190, -0.9029],\n",
      "          [-0.9910, -1.0532, -0.8691]],\n",
      "\n",
      "         [[-1.0301, -1.0525, -1.1980],\n",
      "          [-0.8547, -0.9191, -0.8318],\n",
      "          [-0.9513, -0.9523, -0.9880]]],\n",
      "\n",
      "\n",
      "        [[[-1.1421, -1.1940, -0.9412],\n",
      "          [-0.7693, -0.8583, -1.0032],\n",
      "          [-0.9279, -0.9248, -1.0472]],\n",
      "\n",
      "         [[-0.9605, -1.0097, -1.0418],\n",
      "          [-0.8958, -0.8775, -1.0703],\n",
      "          [-0.9113, -0.8675, -1.1523]],\n",
      "\n",
      "         [[-1.1541, -0.8963, -0.9359],\n",
      "          [-0.9604, -1.0471, -1.1185],\n",
      "          [-0.7549, -1.2596, -1.0996]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1010, -1.1793, -0.9144],\n",
      "          [-1.0384, -1.0156, -0.9939],\n",
      "          [-0.9387, -1.0032, -1.0260]],\n",
      "\n",
      "         [[-0.9027, -0.9100, -0.8232],\n",
      "          [-1.0086, -1.0968, -0.8746],\n",
      "          [-1.0059, -1.0606, -0.9935]],\n",
      "\n",
      "         [[-0.9826, -1.0810, -1.0630],\n",
      "          [-0.8160, -1.0720, -0.7477],\n",
      "          [-1.0384, -1.0637, -0.9941]]],\n",
      "\n",
      "\n",
      "        [[[-0.8773, -0.9943, -0.9464],\n",
      "          [-1.0353, -0.9033, -1.0819],\n",
      "          [-1.0299, -0.8602, -1.0904]],\n",
      "\n",
      "         [[-0.9202, -0.9301, -1.1166],\n",
      "          [-1.0276, -0.9230, -1.0244],\n",
      "          [-0.9578, -1.0814, -1.1296]],\n",
      "\n",
      "         [[-0.9399, -1.0919, -0.9447],\n",
      "          [-0.9592, -1.0651, -0.6908],\n",
      "          [-0.9117, -0.9347, -0.9769]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0157, -1.1252, -0.8917],\n",
      "          [-0.8132, -1.0715, -1.0820],\n",
      "          [-1.1770, -1.0790, -1.0215]],\n",
      "\n",
      "         [[-0.9486, -1.0703, -0.9920],\n",
      "          [-0.8441, -0.8560, -0.9420],\n",
      "          [-1.1723, -0.9456, -1.0751]],\n",
      "\n",
      "         [[-1.1337, -1.0722, -1.0925],\n",
      "          [-0.9772, -1.0303, -1.1522],\n",
      "          [-0.8802, -1.0450, -0.9636]]]])\n",
      "Layer: features.7.expand3x3.bias, Biases: tensor([-0.8879, -1.0399, -1.1601, -0.7647, -0.8996, -1.1233, -0.9489, -0.8310,\n",
      "        -0.8859, -0.9999, -0.8738, -0.7479, -0.9461, -0.7516, -1.1080, -0.9425,\n",
      "        -1.0644, -1.0731, -0.9362, -0.9665, -0.9765, -1.1844, -0.8577, -0.8715,\n",
      "        -1.1632, -0.9913, -1.0991, -1.0278, -0.9934, -0.9425, -1.1699, -1.0189,\n",
      "        -1.0196, -1.0230, -1.0612, -1.1579, -0.9538, -0.9613, -1.0649, -0.9834,\n",
      "        -1.0467, -1.0108, -0.9571, -0.9483, -0.8563, -1.0478, -1.0368, -1.0709,\n",
      "        -0.9700, -1.1065, -1.0168, -1.1105, -0.9886, -0.9275, -1.0428, -1.1492,\n",
      "        -1.0166, -1.2484, -1.0624, -0.9651, -1.0890, -0.8910, -1.0670, -1.0061,\n",
      "        -1.1297, -0.9266, -1.1590, -1.0015, -0.9004, -1.0449, -0.9493, -1.1862,\n",
      "        -0.8544, -0.8938, -0.9963, -0.9202, -0.9251, -0.9666, -0.8999, -0.9582,\n",
      "        -0.9562, -1.0496, -0.8256, -0.9306, -0.8377, -0.8951, -1.0451, -0.9537,\n",
      "        -0.9974, -1.0143, -1.0332, -0.9575, -1.1486, -0.8895, -0.8401, -0.8704,\n",
      "        -0.8481, -0.8203, -1.0124, -1.1367, -0.8921, -0.9473, -1.0679, -0.9252,\n",
      "        -0.7495, -1.0975, -0.8228, -0.8590, -1.0055, -0.9268, -1.1115, -0.9838,\n",
      "        -0.8775, -0.7930, -0.9503, -1.0237, -0.8256, -0.9166, -1.0063, -0.9654,\n",
      "        -0.9269, -0.8584, -1.0876, -1.2617, -1.1439, -0.9818, -1.0383, -0.8879])\n",
      "Layer: features.8.squeeze.weight:, Weights: tensor([[[[-1.0236]],\n",
      "\n",
      "         [[-0.9599]],\n",
      "\n",
      "         [[-1.1251]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0498]],\n",
      "\n",
      "         [[-1.1061]],\n",
      "\n",
      "         [[-1.0268]]],\n",
      "\n",
      "\n",
      "        [[[-0.9941]],\n",
      "\n",
      "         [[-0.8822]],\n",
      "\n",
      "         [[-1.0717]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8271]],\n",
      "\n",
      "         [[-1.1286]],\n",
      "\n",
      "         [[-0.9151]]],\n",
      "\n",
      "\n",
      "        [[[-1.0938]],\n",
      "\n",
      "         [[-0.9736]],\n",
      "\n",
      "         [[-1.0256]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0511]],\n",
      "\n",
      "         [[-0.8917]],\n",
      "\n",
      "         [[-0.9824]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.9916]],\n",
      "\n",
      "         [[-0.9955]],\n",
      "\n",
      "         [[-1.0116]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0996]],\n",
      "\n",
      "         [[-1.0926]],\n",
      "\n",
      "         [[-1.0496]]],\n",
      "\n",
      "\n",
      "        [[[-1.0531]],\n",
      "\n",
      "         [[-1.1827]],\n",
      "\n",
      "         [[-1.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0317]],\n",
      "\n",
      "         [[-0.9095]],\n",
      "\n",
      "         [[-0.9507]]],\n",
      "\n",
      "\n",
      "        [[[-1.1529]],\n",
      "\n",
      "         [[-1.0591]],\n",
      "\n",
      "         [[-1.1592]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9184]],\n",
      "\n",
      "         [[-0.9536]],\n",
      "\n",
      "         [[-0.9894]]]])\n",
      "Layer: features.8.squeeze.bias, Biases: tensor([-0.9256, -1.0588, -0.9370, -0.8879, -0.8687, -1.0187, -1.1378, -0.9350,\n",
      "        -1.1387, -1.1123, -0.7574, -0.9734, -0.9583, -0.8305, -0.9182, -1.0172,\n",
      "        -0.8579, -1.0132, -0.9938, -0.9463, -0.7726, -0.8706, -1.0119, -0.8858,\n",
      "        -0.8948, -1.0007, -1.0027, -1.1552, -1.0955, -0.9344, -1.1017, -0.9446,\n",
      "        -0.6788, -0.8876, -1.0041, -0.9819, -0.7963, -0.9074, -1.0985, -0.9885,\n",
      "        -1.1251, -1.0544, -0.8403, -1.1283, -0.9330, -0.9279, -1.0455, -0.9902])\n",
      "Layer: features.8.expand1x1.weight:, Weights: tensor([[[[-1.1136]],\n",
      "\n",
      "         [[-1.1102]],\n",
      "\n",
      "         [[-1.0309]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9041]],\n",
      "\n",
      "         [[-0.9962]],\n",
      "\n",
      "         [[-1.1906]]],\n",
      "\n",
      "\n",
      "        [[[-1.1205]],\n",
      "\n",
      "         [[-0.9969]],\n",
      "\n",
      "         [[-1.0214]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9608]],\n",
      "\n",
      "         [[-1.1574]],\n",
      "\n",
      "         [[-0.8777]]],\n",
      "\n",
      "\n",
      "        [[[-1.0022]],\n",
      "\n",
      "         [[-0.8782]],\n",
      "\n",
      "         [[-0.9966]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9679]],\n",
      "\n",
      "         [[-1.0699]],\n",
      "\n",
      "         [[-0.9367]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.8692]],\n",
      "\n",
      "         [[-1.1205]],\n",
      "\n",
      "         [[-0.9602]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0064]],\n",
      "\n",
      "         [[-0.9232]],\n",
      "\n",
      "         [[-0.9904]]],\n",
      "\n",
      "\n",
      "        [[[-1.0668]],\n",
      "\n",
      "         [[-1.0476]],\n",
      "\n",
      "         [[-0.9867]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2117]],\n",
      "\n",
      "         [[-0.9697]],\n",
      "\n",
      "         [[-0.9790]]],\n",
      "\n",
      "\n",
      "        [[[-1.0780]],\n",
      "\n",
      "         [[-0.9059]],\n",
      "\n",
      "         [[-0.9803]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0057]],\n",
      "\n",
      "         [[-1.0609]],\n",
      "\n",
      "         [[-0.9069]]]])\n",
      "Layer: features.8.expand1x1.bias, Biases: tensor([-0.6446, -0.9346, -0.9382, -0.8585, -0.9950, -1.0133, -1.0633, -1.0935,\n",
      "        -1.1443, -1.1190, -1.1703, -1.2303, -1.0799, -0.9709, -0.8236, -0.9830,\n",
      "        -1.1142, -1.0272, -0.7961, -1.0446, -0.9231, -0.9983, -1.0529, -0.8489,\n",
      "        -1.1706, -1.0159, -0.9002, -1.1523, -1.0230, -0.9024, -0.8936, -1.0078,\n",
      "        -1.0192, -1.1316, -1.0434, -1.0506, -0.8779, -0.9707, -1.0572, -0.9406,\n",
      "        -1.1229, -0.9895, -1.2457, -1.0269, -1.1845, -1.0282, -1.1230, -0.9218,\n",
      "        -1.0432, -0.8860, -1.1360, -1.0529, -0.9985, -1.2436, -1.0783, -0.9636,\n",
      "        -0.9742, -0.8919, -0.9481, -1.2686, -0.9342, -1.0453, -0.8002, -0.9852,\n",
      "        -1.1059, -0.9869, -1.0113, -0.9028, -1.0374, -1.0824, -0.8702, -1.0362,\n",
      "        -0.9915, -1.0129, -1.0188, -0.8719, -1.0543, -0.8894, -1.1470, -1.0212,\n",
      "        -1.0237, -1.0258, -1.0406, -1.0207, -1.0830, -1.0355, -0.8481, -0.9446,\n",
      "        -1.1649, -1.1023, -0.9233, -0.9828, -1.1109, -0.9940, -0.9843, -1.1679,\n",
      "        -0.9802, -1.1516, -1.2049, -0.8519, -0.9450, -1.0332, -1.0832, -0.9263,\n",
      "        -0.9772, -1.0063, -1.1042, -1.0664, -0.9734, -1.0309, -1.1637, -1.1604,\n",
      "        -1.0426, -1.0136, -0.8902, -0.9697, -1.1357, -1.1929, -0.8891, -0.8847,\n",
      "        -0.9681, -1.2600, -0.8223, -0.8741, -1.0161, -0.8985, -1.0966, -0.9090,\n",
      "        -0.9012, -1.0677, -1.2737, -0.9379, -1.0169, -0.8107, -1.0287, -0.9751,\n",
      "        -1.1333, -1.0449, -1.0023, -1.1258, -1.0378, -1.0336, -0.9519, -0.9610,\n",
      "        -0.9355, -0.9634, -0.9043, -1.0292, -1.0415, -0.9971, -1.1640, -1.0462,\n",
      "        -1.0163, -1.1116, -0.9185, -0.9023, -1.0192, -0.8695, -1.0602, -0.9179,\n",
      "        -1.0479, -1.0573, -1.0458, -1.1188, -0.9896, -1.0296, -1.0692, -1.0099,\n",
      "        -0.9834, -0.8443, -1.1451, -0.9655, -1.0052, -1.0481, -1.0836, -0.9570,\n",
      "        -0.7917, -0.9729, -0.9573, -1.0539, -0.7514, -0.9565, -1.0685, -0.9720,\n",
      "        -0.8965, -1.1411, -1.0838, -1.0206, -1.0428, -0.9878, -1.1315, -1.1211])\n",
      "Layer: features.8.expand3x3.weight:, Weights: tensor([[[[-0.8196, -0.8309, -1.0437],\n",
      "          [-1.0700, -1.0318, -0.8461],\n",
      "          [-1.0764, -0.9706, -1.0217]],\n",
      "\n",
      "         [[-0.9584, -1.1999, -1.0579],\n",
      "          [-0.8034, -0.9943, -0.8856],\n",
      "          [-1.1275, -1.0112, -0.8551]],\n",
      "\n",
      "         [[-0.8074, -1.0526, -0.8087],\n",
      "          [-1.1229, -0.9497, -0.9681],\n",
      "          [-1.0533, -1.1073, -1.0382]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9367, -0.9393, -0.9961],\n",
      "          [-1.1358, -0.9586, -0.9309],\n",
      "          [-1.1173, -0.8791, -0.9683]],\n",
      "\n",
      "         [[-1.0564, -1.1030, -0.9785],\n",
      "          [-0.9826, -0.9560, -0.9698],\n",
      "          [-1.0840, -0.9673, -1.1126]],\n",
      "\n",
      "         [[-1.0814, -1.0439, -1.0512],\n",
      "          [-0.7888, -1.0488, -0.9989],\n",
      "          [-1.0159, -0.8831, -1.1864]]],\n",
      "\n",
      "\n",
      "        [[[-0.8747, -0.9815, -1.0172],\n",
      "          [-0.9922, -1.0414, -0.9839],\n",
      "          [-0.9421, -1.0411, -1.0904]],\n",
      "\n",
      "         [[-0.8814, -0.9121, -0.9092],\n",
      "          [-0.9606, -0.8783, -0.9657],\n",
      "          [-0.9381, -1.1042, -1.0198]],\n",
      "\n",
      "         [[-0.9651, -1.1043, -0.9097],\n",
      "          [-1.0254, -0.7765, -0.8266],\n",
      "          [-0.9498, -1.0056, -1.1614]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8279, -0.9389, -0.9456],\n",
      "          [-1.0303, -0.8533, -1.0794],\n",
      "          [-0.9424, -0.9484, -0.8375]],\n",
      "\n",
      "         [[-1.0991, -1.0310, -0.8726],\n",
      "          [-0.8875, -1.1771, -1.0577],\n",
      "          [-1.1140, -0.9887, -0.9812]],\n",
      "\n",
      "         [[-1.1195, -1.0730, -1.0921],\n",
      "          [-1.0198, -0.8024, -0.8330],\n",
      "          [-1.1187, -0.9894, -1.1031]]],\n",
      "\n",
      "\n",
      "        [[[-1.1421, -0.9202, -0.8972],\n",
      "          [-1.0328, -0.8719, -0.9867],\n",
      "          [-0.9764, -1.1391, -0.7795]],\n",
      "\n",
      "         [[-0.8999, -1.0113, -1.0590],\n",
      "          [-0.9482, -0.7959, -0.8019],\n",
      "          [-0.9975, -1.1798, -0.9462]],\n",
      "\n",
      "         [[-0.9201, -1.1576, -1.0595],\n",
      "          [-1.2902, -1.0192, -1.1890],\n",
      "          [-0.9798, -1.1049, -1.0983]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8801, -0.9240, -0.9558],\n",
      "          [-1.0400, -0.9960, -1.0724],\n",
      "          [-1.0799, -1.0182, -1.1424]],\n",
      "\n",
      "         [[-1.2195, -0.9525, -1.0234],\n",
      "          [-1.0195, -1.0203, -0.9470],\n",
      "          [-1.1414, -1.0786, -0.7879]],\n",
      "\n",
      "         [[-0.9202, -0.8769, -0.8707],\n",
      "          [-0.8077, -0.9814, -1.0620],\n",
      "          [-1.0806, -1.0708, -1.1200]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.1265, -0.8998, -1.0129],\n",
      "          [-0.9704, -0.9391, -0.8886],\n",
      "          [-0.9008, -1.0890, -1.0189]],\n",
      "\n",
      "         [[-1.0825, -0.9562, -0.9679],\n",
      "          [-1.0193, -0.9013, -1.1160],\n",
      "          [-0.8471, -0.9133, -0.8012]],\n",
      "\n",
      "         [[-1.1027, -0.9098, -0.9178],\n",
      "          [-0.9428, -0.9838, -0.8322],\n",
      "          [-1.0054, -1.0778, -0.9332]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0272, -1.1110, -0.9875],\n",
      "          [-1.1881, -0.7579, -0.9004],\n",
      "          [-0.9375, -1.0246, -1.0324]],\n",
      "\n",
      "         [[-0.8651, -0.9455, -0.9342],\n",
      "          [-0.7603, -1.0147, -1.0319],\n",
      "          [-0.9859, -0.9519, -1.0066]],\n",
      "\n",
      "         [[-0.9373, -1.0191, -1.0817],\n",
      "          [-0.8563, -1.0486, -0.7689],\n",
      "          [-0.8884, -1.0628, -0.9880]]],\n",
      "\n",
      "\n",
      "        [[[-1.0185, -1.1860, -1.1449],\n",
      "          [-0.9961, -1.0331, -1.0274],\n",
      "          [-1.1165, -1.1106, -0.9897]],\n",
      "\n",
      "         [[-0.9829, -1.0357, -1.2411],\n",
      "          [-1.0546, -1.0002, -0.9659],\n",
      "          [-0.9079, -0.8735, -0.8790]],\n",
      "\n",
      "         [[-1.2157, -1.0027, -1.0403],\n",
      "          [-0.9158, -1.0657, -0.9957],\n",
      "          [-1.2691, -1.2194, -0.8324]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9325, -1.0475, -1.0880],\n",
      "          [-0.9163, -0.9507, -1.2094],\n",
      "          [-1.0568, -1.1285, -1.0897]],\n",
      "\n",
      "         [[-0.9566, -1.0331, -0.8417],\n",
      "          [-0.8604, -1.0273, -0.9172],\n",
      "          [-0.9470, -0.9906, -1.0043]],\n",
      "\n",
      "         [[-1.0351, -1.0051, -0.9590],\n",
      "          [-0.8885, -0.9761, -1.0921],\n",
      "          [-1.0338, -1.1437, -0.8123]]],\n",
      "\n",
      "\n",
      "        [[[-0.8915, -1.1391, -1.2301],\n",
      "          [-1.0031, -0.9792, -1.0556],\n",
      "          [-0.8645, -0.8518, -1.0535]],\n",
      "\n",
      "         [[-1.0077, -1.0731, -0.9711],\n",
      "          [-1.0033, -0.9789, -1.0134],\n",
      "          [-1.0439, -1.1709, -0.8307]],\n",
      "\n",
      "         [[-1.0446, -1.0962, -0.9664],\n",
      "          [-0.7914, -0.9872, -0.8312],\n",
      "          [-1.0621, -1.0245, -1.0766]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9969, -0.9513, -0.8765],\n",
      "          [-1.2578, -1.0365, -1.0759],\n",
      "          [-1.1723, -0.9721, -1.0031]],\n",
      "\n",
      "         [[-1.0425, -0.9769, -0.9794],\n",
      "          [-1.0330, -0.9760, -1.0857],\n",
      "          [-0.9513, -1.0090, -0.9299]],\n",
      "\n",
      "         [[-0.8757, -1.1435, -0.9183],\n",
      "          [-1.0529, -1.0159, -0.9935],\n",
      "          [-0.9639, -1.0260, -1.0297]]]])\n",
      "Layer: features.8.expand3x3.bias, Biases: tensor([-0.9968, -0.9247, -1.1781, -0.9623, -1.0496, -0.9745, -1.0107, -1.0798,\n",
      "        -1.1215, -1.2362, -0.9470, -1.0739, -0.9636, -1.0611, -1.0067, -1.0531,\n",
      "        -0.9527, -1.0293, -1.0927, -0.9274, -0.9863, -0.9902, -0.8885, -1.1159,\n",
      "        -1.1266, -1.0466, -0.9221, -0.9702, -1.2656, -0.8927, -0.9622, -0.9150,\n",
      "        -0.9096, -0.9981, -1.0872, -0.9881, -0.9926, -0.7996, -1.0953, -0.9236,\n",
      "        -1.0109, -1.1120, -0.8814, -1.0582, -1.2134, -1.0766, -0.9832, -0.8488,\n",
      "        -1.0487, -1.0755, -1.0285, -0.8284, -0.9000, -0.9001, -1.0470, -1.0984,\n",
      "        -1.0170, -1.0546, -1.0827, -0.9319, -0.8214, -0.9011, -1.0932, -1.0644,\n",
      "        -0.9370, -1.0867, -1.0855, -0.8084, -0.9125, -1.0873, -1.1267, -0.9817,\n",
      "        -1.0887, -1.0235, -1.0467, -1.0125, -0.9174, -0.9514, -0.9604, -1.1071,\n",
      "        -0.9091, -1.1396, -0.9145, -0.9812, -1.0036, -0.8606, -1.0817, -1.0848,\n",
      "        -1.2211, -0.9666, -1.0184, -0.7757, -0.9719, -0.9916, -1.0215, -1.0592,\n",
      "        -0.9244, -0.9702, -0.7762, -1.2520, -1.0127, -0.9399, -1.0354, -1.1141,\n",
      "        -1.0286, -0.9819, -1.1849, -0.9608, -1.0005, -1.0242, -1.0423, -1.1887,\n",
      "        -1.0534, -1.0365, -0.9809, -1.0024, -0.9734, -1.0754, -1.1087, -1.1488,\n",
      "        -0.9564, -0.8404, -0.9846, -0.9942, -1.0009, -1.0083, -1.1227, -1.0126,\n",
      "        -0.9701, -1.1421, -0.9237, -1.0671, -1.0949, -1.0027, -1.0071, -1.0645,\n",
      "        -1.0754, -0.9170, -0.9653, -0.9518, -1.1127, -1.0640, -1.0028, -1.0943,\n",
      "        -0.9248, -0.9141, -0.9997, -0.9575, -0.9943, -0.8453, -1.0527, -1.0419,\n",
      "        -0.9683, -1.0825, -1.0503, -1.0389, -0.8227, -1.0476, -0.9204, -0.8435,\n",
      "        -0.9334, -1.0747, -0.9374, -0.9796, -0.8846, -0.9403, -1.0664, -1.0524,\n",
      "        -0.9009, -0.8945, -1.0956, -0.9812, -1.1494, -1.1661, -1.0172, -0.8712,\n",
      "        -1.0798, -1.1279, -0.8865, -1.0909, -1.0577, -0.9205, -1.0980, -1.0424,\n",
      "        -0.9429, -1.1331, -0.8685, -0.7793, -0.7350, -1.0050, -1.0549, -0.9089])\n",
      "Layer: features.9.squeeze.weight:, Weights: tensor([[[[-1.1803]],\n",
      "\n",
      "         [[-0.8809]],\n",
      "\n",
      "         [[-1.0803]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9713]],\n",
      "\n",
      "         [[-1.2247]],\n",
      "\n",
      "         [[-1.0393]]],\n",
      "\n",
      "\n",
      "        [[[-0.8134]],\n",
      "\n",
      "         [[-1.0747]],\n",
      "\n",
      "         [[-1.0285]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0305]],\n",
      "\n",
      "         [[-0.8034]],\n",
      "\n",
      "         [[-1.0255]]],\n",
      "\n",
      "\n",
      "        [[[-0.9067]],\n",
      "\n",
      "         [[-0.8860]],\n",
      "\n",
      "         [[-0.9873]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0368]],\n",
      "\n",
      "         [[-0.8818]],\n",
      "\n",
      "         [[-0.9713]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.9809]],\n",
      "\n",
      "         [[-1.0642]],\n",
      "\n",
      "         [[-0.9110]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9037]],\n",
      "\n",
      "         [[-0.9673]],\n",
      "\n",
      "         [[-1.0923]]],\n",
      "\n",
      "\n",
      "        [[[-1.0389]],\n",
      "\n",
      "         [[-1.1320]],\n",
      "\n",
      "         [[-1.0461]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0274]],\n",
      "\n",
      "         [[-0.7458]],\n",
      "\n",
      "         [[-0.9057]]],\n",
      "\n",
      "\n",
      "        [[[-0.7748]],\n",
      "\n",
      "         [[-0.9582]],\n",
      "\n",
      "         [[-0.9981]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0046]],\n",
      "\n",
      "         [[-1.1130]],\n",
      "\n",
      "         [[-1.0631]]]])\n",
      "Layer: features.9.squeeze.bias, Biases: tensor([-1.0907, -0.7861, -0.9919, -0.9999, -1.0000, -1.0156, -0.8785, -0.9831,\n",
      "        -0.9841, -1.0496, -1.0534, -0.9691, -0.9247, -1.0169, -1.0323, -1.0344,\n",
      "        -1.1733, -0.9163, -1.1545, -0.8116, -1.2305, -0.9918, -1.1334, -0.8495,\n",
      "        -0.8389, -1.0377, -0.9637, -1.0047, -0.9714, -1.0159, -0.9757, -1.0648,\n",
      "        -1.0786, -1.1791, -0.9379, -0.9946, -0.8214, -0.9306, -1.1335, -1.0425,\n",
      "        -1.0763, -0.9790, -1.1283, -0.9322, -1.0576, -1.0776, -0.9733, -0.8553])\n",
      "Layer: features.9.expand1x1.weight:, Weights: tensor([[[[-1.0561]],\n",
      "\n",
      "         [[-1.1296]],\n",
      "\n",
      "         [[-0.9140]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8709]],\n",
      "\n",
      "         [[-0.9553]],\n",
      "\n",
      "         [[-1.0662]]],\n",
      "\n",
      "\n",
      "        [[[-0.8336]],\n",
      "\n",
      "         [[-0.9672]],\n",
      "\n",
      "         [[-1.0067]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8747]],\n",
      "\n",
      "         [[-1.1710]],\n",
      "\n",
      "         [[-0.9012]]],\n",
      "\n",
      "\n",
      "        [[[-0.9913]],\n",
      "\n",
      "         [[-0.8847]],\n",
      "\n",
      "         [[-0.9055]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1130]],\n",
      "\n",
      "         [[-1.1528]],\n",
      "\n",
      "         [[-0.9698]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0621]],\n",
      "\n",
      "         [[-1.0952]],\n",
      "\n",
      "         [[-1.1710]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0444]],\n",
      "\n",
      "         [[-1.1009]],\n",
      "\n",
      "         [[-0.9955]]],\n",
      "\n",
      "\n",
      "        [[[-0.8558]],\n",
      "\n",
      "         [[-0.9901]],\n",
      "\n",
      "         [[-0.8806]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2051]],\n",
      "\n",
      "         [[-0.9831]],\n",
      "\n",
      "         [[-1.0615]]],\n",
      "\n",
      "\n",
      "        [[[-0.8026]],\n",
      "\n",
      "         [[-0.9701]],\n",
      "\n",
      "         [[-0.9289]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1520]],\n",
      "\n",
      "         [[-0.9826]],\n",
      "\n",
      "         [[-0.9046]]]])\n",
      "Layer: features.9.expand1x1.bias, Biases: tensor([-0.9462, -1.1911, -1.0352, -1.3224, -0.9652, -0.9697, -0.9612, -1.0884,\n",
      "        -1.3039, -0.9547, -0.8754, -0.9144, -1.0170, -1.0795, -1.1331, -0.9661,\n",
      "        -1.0883, -0.9790, -0.9844, -1.0099, -1.0984, -1.0556, -0.8914, -1.2716,\n",
      "        -1.1239, -0.9303, -0.9908, -1.0598, -0.8620, -0.8711, -1.1531, -0.8918,\n",
      "        -1.0493, -1.0233, -1.0473, -1.0953, -0.9152, -0.8804, -0.9403, -1.0996,\n",
      "        -1.0957, -1.0012, -1.0054, -0.7597, -1.0474, -0.9202, -1.0872, -1.0477,\n",
      "        -1.0933, -0.7879, -1.2181, -0.8532, -0.9297, -1.1377, -1.0222, -1.0697,\n",
      "        -1.0914, -1.0166, -0.9912, -0.9424, -0.7713, -0.8778, -1.0011, -0.9444,\n",
      "        -0.9658, -0.9710, -1.0015, -1.0004, -0.9840, -1.0683, -0.8975, -0.9823,\n",
      "        -1.2464, -1.0614, -1.1454, -0.9167, -0.8738, -0.9974, -1.1438, -0.9246,\n",
      "        -0.8859, -1.0146, -0.9121, -1.0100, -1.0696, -1.0187, -1.0366, -0.8969,\n",
      "        -0.9246, -0.9266, -0.8925, -0.8677, -1.0831, -0.9326, -1.2151, -0.8993,\n",
      "        -1.1402, -0.9375, -1.0714, -0.7835, -0.9798, -0.8909, -0.9615, -1.1897,\n",
      "        -0.9168, -1.0351, -0.8431, -1.0230, -0.8480, -1.0070, -1.1290, -0.7740,\n",
      "        -1.0450, -0.9390, -0.8893, -0.9817, -1.1394, -1.0924, -1.0746, -0.9590,\n",
      "        -0.9421, -1.0383, -0.9990, -0.9419, -1.0327, -1.0278, -0.9786, -1.1700,\n",
      "        -1.0703, -1.0943, -0.9766, -0.8731, -1.0496, -1.1013, -1.0638, -1.0507,\n",
      "        -1.0984, -1.0191, -0.8950, -0.9102, -0.9673, -0.9451, -0.9671, -0.7695,\n",
      "        -0.8555, -1.1095, -1.0092, -1.0878, -0.7648, -1.0179, -0.9672, -1.1595,\n",
      "        -0.9307, -0.8658, -0.8734, -0.9625, -1.1673, -0.9593, -0.9835, -0.9829,\n",
      "        -1.0868, -1.0616, -1.0684, -1.0203, -0.9693, -0.9672, -0.9136, -0.9107,\n",
      "        -0.9393, -0.9940, -0.9959, -0.8825, -1.1041, -1.0694, -1.1075, -1.0266,\n",
      "        -1.2834, -0.8732, -0.8416, -0.9270, -1.1123, -0.9232, -1.0132, -1.0230,\n",
      "        -1.0002, -1.0006, -1.0706, -1.0678, -0.8585, -1.0277, -1.0071, -1.0627])\n",
      "Layer: features.9.expand3x3.weight:, Weights: tensor([[[[-0.8678, -0.9492, -1.0209],\n",
      "          [-1.1064, -0.9647, -0.9903],\n",
      "          [-0.9190, -1.1016, -0.9259]],\n",
      "\n",
      "         [[-1.0368, -1.0452, -1.0129],\n",
      "          [-0.9301, -0.8886, -1.0401],\n",
      "          [-1.1822, -0.9530, -0.9921]],\n",
      "\n",
      "         [[-0.8700, -0.9639, -1.1737],\n",
      "          [-0.9507, -0.7376, -0.9950],\n",
      "          [-1.0720, -0.9725, -1.0541]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9749, -0.9039, -0.9006],\n",
      "          [-0.9617, -1.0311, -1.0617],\n",
      "          [-1.0025, -1.2158, -1.0398]],\n",
      "\n",
      "         [[-1.0078, -1.1556, -1.0940],\n",
      "          [-0.8228, -0.9958, -0.9944],\n",
      "          [-0.9211, -0.8786, -1.0527]],\n",
      "\n",
      "         [[-1.1246, -1.0466, -0.9494],\n",
      "          [-1.1383, -0.9691, -1.1853],\n",
      "          [-0.9848, -0.9464, -0.8336]]],\n",
      "\n",
      "\n",
      "        [[[-0.9622, -0.8756, -0.8816],\n",
      "          [-1.0838, -1.0448, -0.9687],\n",
      "          [-0.7478, -1.0252, -1.1181]],\n",
      "\n",
      "         [[-0.8713, -1.0357, -0.9918],\n",
      "          [-0.9612, -1.0703, -0.9560],\n",
      "          [-0.7324, -0.9300, -1.1114]],\n",
      "\n",
      "         [[-0.8920, -1.0274, -0.9204],\n",
      "          [-1.1418, -1.1232, -0.9119],\n",
      "          [-1.1144, -0.8957, -0.9988]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0298, -1.1290, -1.0788],\n",
      "          [-0.9587, -1.0118, -1.0119],\n",
      "          [-0.9811, -1.0220, -1.1660]],\n",
      "\n",
      "         [[-1.0473, -1.0808, -0.9485],\n",
      "          [-0.9363, -0.9103, -0.8598],\n",
      "          [-0.9683, -0.9968, -1.2642]],\n",
      "\n",
      "         [[-1.0295, -0.9635, -1.0156],\n",
      "          [-1.0450, -1.0926, -1.1002],\n",
      "          [-0.8931, -0.9855, -0.8106]]],\n",
      "\n",
      "\n",
      "        [[[-1.0892, -0.9490, -0.9924],\n",
      "          [-1.0754, -1.1606, -0.8776],\n",
      "          [-0.9795, -0.8599, -0.8513]],\n",
      "\n",
      "         [[-0.8181, -0.9920, -1.0198],\n",
      "          [-1.0208, -0.8535, -1.0080],\n",
      "          [-0.9713, -1.0490, -0.9789]],\n",
      "\n",
      "         [[-0.9458, -0.9343, -0.8385],\n",
      "          [-0.9400, -1.1538, -1.1453],\n",
      "          [-0.9077, -1.0900, -0.8319]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9447, -0.8953, -0.9461],\n",
      "          [-1.0738, -0.9136, -0.8864],\n",
      "          [-1.0906, -0.9546, -1.0869]],\n",
      "\n",
      "         [[-1.1096, -1.0124, -0.8836],\n",
      "          [-0.9747, -0.9925, -0.7609],\n",
      "          [-0.9329, -1.0019, -0.8535]],\n",
      "\n",
      "         [[-1.0514, -1.0805, -1.0301],\n",
      "          [-0.9447, -0.9447, -0.9185],\n",
      "          [-0.9050, -1.0108, -1.1652]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.9580, -1.0555, -0.9660],\n",
      "          [-1.0155, -0.9730, -0.8727],\n",
      "          [-1.2865, -1.1098, -0.9481]],\n",
      "\n",
      "         [[-1.2273, -1.0385, -1.0253],\n",
      "          [-1.1493, -0.8697, -1.0103],\n",
      "          [-0.9278, -1.1933, -1.0075]],\n",
      "\n",
      "         [[-0.9734, -1.0212, -0.9558],\n",
      "          [-1.0682, -0.9684, -0.9445],\n",
      "          [-0.8498, -1.0233, -0.8997]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9281, -1.2025, -1.2520],\n",
      "          [-1.0604, -0.9036, -0.9165],\n",
      "          [-0.8949, -1.0015, -1.0566]],\n",
      "\n",
      "         [[-0.9482, -1.0320, -1.0305],\n",
      "          [-1.0097, -0.9833, -1.2300],\n",
      "          [-1.0395, -0.8867, -1.0301]],\n",
      "\n",
      "         [[-1.0605, -0.9186, -0.8777],\n",
      "          [-1.1174, -0.9354, -0.9957],\n",
      "          [-0.9148, -1.0293, -1.0372]]],\n",
      "\n",
      "\n",
      "        [[[-1.0084, -1.1567, -0.9896],\n",
      "          [-1.0085, -0.9173, -0.8203],\n",
      "          [-1.0199, -1.1700, -1.0196]],\n",
      "\n",
      "         [[-0.8709, -1.0601, -1.0824],\n",
      "          [-1.1422, -0.9547, -0.9350],\n",
      "          [-1.0631, -1.0015, -1.0807]],\n",
      "\n",
      "         [[-1.0531, -0.9567, -0.9309],\n",
      "          [-0.9606, -1.1031, -0.9305],\n",
      "          [-1.0405, -1.0363, -0.9840]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1178, -0.9343, -1.0701],\n",
      "          [-0.9065, -1.0026, -1.0318],\n",
      "          [-0.9994, -0.9179, -0.9447]],\n",
      "\n",
      "         [[-1.0807, -1.0833, -0.9862],\n",
      "          [-0.9949, -1.0885, -1.1641],\n",
      "          [-0.9354, -1.1593, -1.0197]],\n",
      "\n",
      "         [[-1.0413, -0.7898, -1.0073],\n",
      "          [-0.9927, -1.1084, -0.9284],\n",
      "          [-1.1865, -0.9946, -0.9571]]],\n",
      "\n",
      "\n",
      "        [[[-0.9124, -1.0138, -0.8944],\n",
      "          [-0.9196, -0.9815, -0.9413],\n",
      "          [-0.9526, -0.9745, -0.9025]],\n",
      "\n",
      "         [[-0.9114, -1.0868, -0.9571],\n",
      "          [-0.9117, -0.9332, -0.9868],\n",
      "          [-0.9855, -1.0857, -0.9858]],\n",
      "\n",
      "         [[-1.1845, -1.1460, -0.9499],\n",
      "          [-1.1071, -1.0014, -0.9428],\n",
      "          [-1.0615, -0.8448, -0.9680]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9814, -1.0867, -0.9559],\n",
      "          [-0.9388, -1.0222, -0.9004],\n",
      "          [-1.1305, -1.0401, -0.9230]],\n",
      "\n",
      "         [[-1.1080, -1.0157, -0.8566],\n",
      "          [-0.9227, -0.9949, -1.0815],\n",
      "          [-1.0342, -0.9764, -0.8218]],\n",
      "\n",
      "         [[-1.0672, -0.7727, -0.9763],\n",
      "          [-1.0498, -1.0378, -0.7871],\n",
      "          [-0.9662, -0.8598, -0.8878]]]])\n",
      "Layer: features.9.expand3x3.bias, Biases: tensor([-1.0937, -0.8575, -1.0139, -1.1344, -0.9977, -1.0150, -1.1005, -0.8598,\n",
      "        -1.0456, -1.0453, -1.0826, -1.0291, -0.9488, -0.8323, -1.2214, -1.0127,\n",
      "        -0.9066, -1.0771, -1.0048, -0.9861, -0.9205, -1.0974, -1.0561, -1.0524,\n",
      "        -1.0914, -1.1087, -1.0642, -0.9065, -1.1502, -0.9858, -1.0674, -0.9964,\n",
      "        -1.0912, -0.8874, -1.1630, -0.9360, -0.9491, -1.0381, -1.1506, -1.0574,\n",
      "        -0.9920, -1.1485, -0.9048, -1.0889, -0.9176, -1.1350, -1.0302, -0.9616,\n",
      "        -1.2058, -1.0289, -0.9121, -1.1268, -0.8498, -1.2295, -0.9867, -1.1011,\n",
      "        -1.1585, -1.0832, -1.0017, -0.9851, -1.0195, -0.7459, -1.0486, -1.1182,\n",
      "        -1.0417, -1.0609, -1.0002, -0.9118, -0.9939, -0.9652, -1.0420, -0.9720,\n",
      "        -1.0063, -0.9567, -0.8817, -1.0184, -1.0840, -1.0943, -0.9340, -1.0158,\n",
      "        -1.0294, -0.9747, -0.9132, -0.9829, -1.0152, -1.0578, -0.9372, -0.8851,\n",
      "        -0.9109, -0.9751, -1.0270, -1.1665, -1.1472, -1.1125, -0.7463, -1.0705,\n",
      "        -0.9529, -0.9159, -0.9109, -0.9159, -1.0810, -0.9087, -0.8992, -0.7869,\n",
      "        -1.0607, -0.8928, -0.6293, -1.0923, -0.8883, -0.9716, -1.0089, -0.8672,\n",
      "        -0.9641, -1.0281, -1.0997, -0.9242, -0.9618, -1.0494, -1.1037, -1.0708,\n",
      "        -1.0374, -1.0994, -0.9252, -0.8047, -1.0732, -1.0133, -0.9102, -0.9441,\n",
      "        -0.9550, -1.1125, -0.9292, -0.8581, -0.8789, -1.0635, -0.9559, -1.1337,\n",
      "        -0.8192, -1.0963, -0.9180, -1.0596, -1.0306, -0.9724, -0.9581, -0.9620,\n",
      "        -0.9431, -1.0501, -1.0965, -0.9993, -0.8314, -0.9416, -0.9411, -0.9922,\n",
      "        -0.9160, -0.9102, -1.0455, -0.8348, -1.1837, -0.8872, -0.9482, -0.9455,\n",
      "        -1.0212, -1.0056, -0.9876, -0.9119, -0.8686, -1.0527, -0.8351, -0.9882,\n",
      "        -1.1790, -1.0509, -1.0343, -0.9958, -1.0671, -1.1896, -0.9780, -0.8458,\n",
      "        -0.9227, -0.9736, -0.9468, -0.9702, -0.9702, -1.0408, -0.9826, -1.0816,\n",
      "        -0.9248, -0.9642, -1.0167, -1.1593, -1.1028, -1.0447, -1.1474, -0.9029])\n",
      "Layer: features.10.squeeze.weight:, Weights: tensor([[[[-1.2239]],\n",
      "\n",
      "         [[-0.9665]],\n",
      "\n",
      "         [[-0.9162]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9629]],\n",
      "\n",
      "         [[-1.1471]],\n",
      "\n",
      "         [[-0.9512]]],\n",
      "\n",
      "\n",
      "        [[[-1.1427]],\n",
      "\n",
      "         [[-0.9735]],\n",
      "\n",
      "         [[-0.9605]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9716]],\n",
      "\n",
      "         [[-1.0265]],\n",
      "\n",
      "         [[-1.0301]]],\n",
      "\n",
      "\n",
      "        [[[-0.9883]],\n",
      "\n",
      "         [[-0.9944]],\n",
      "\n",
      "         [[-0.9434]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9161]],\n",
      "\n",
      "         [[-0.9090]],\n",
      "\n",
      "         [[-0.9423]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0148]],\n",
      "\n",
      "         [[-1.0665]],\n",
      "\n",
      "         [[-0.9441]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0399]],\n",
      "\n",
      "         [[-1.0063]],\n",
      "\n",
      "         [[-0.8663]]],\n",
      "\n",
      "\n",
      "        [[[-0.9785]],\n",
      "\n",
      "         [[-1.0280]],\n",
      "\n",
      "         [[-0.9792]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9547]],\n",
      "\n",
      "         [[-0.9806]],\n",
      "\n",
      "         [[-1.1115]]],\n",
      "\n",
      "\n",
      "        [[[-0.9938]],\n",
      "\n",
      "         [[-0.9288]],\n",
      "\n",
      "         [[-0.9143]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8935]],\n",
      "\n",
      "         [[-0.9051]],\n",
      "\n",
      "         [[-0.7502]]]])\n",
      "Layer: features.10.squeeze.bias, Biases: tensor([-0.9265, -1.1008, -1.0720, -0.9501, -1.1195, -1.1470, -1.0026, -0.9184,\n",
      "        -0.9352, -1.0715, -0.9889, -1.0552, -1.0984, -1.0775, -0.9582, -0.8486,\n",
      "        -0.9909, -1.0634, -1.0794, -0.9899, -0.9588, -1.0994, -1.1199, -1.0791,\n",
      "        -0.9597, -1.0470, -1.1878, -1.0059, -0.9817, -0.9987, -1.0861, -1.0062,\n",
      "        -1.0812, -1.0558, -1.1068, -1.0150, -1.2132, -0.9194, -1.0409, -1.1749,\n",
      "        -1.0114, -1.0466, -0.9912, -0.9629, -1.0098, -0.8833, -0.9449, -1.0132,\n",
      "        -1.0355, -0.9729, -0.9293, -1.0241, -1.0639, -0.9897, -1.0465, -0.9614,\n",
      "        -0.9042, -1.0392, -1.0574, -0.7461, -1.0975, -0.8672, -0.9368, -0.9316])\n",
      "Layer: features.10.expand1x1.weight:, Weights: tensor([[[[-1.0196]],\n",
      "\n",
      "         [[-0.9654]],\n",
      "\n",
      "         [[-1.0220]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0116]],\n",
      "\n",
      "         [[-0.9113]],\n",
      "\n",
      "         [[-1.0260]]],\n",
      "\n",
      "\n",
      "        [[[-1.1496]],\n",
      "\n",
      "         [[-0.9050]],\n",
      "\n",
      "         [[-0.9139]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9416]],\n",
      "\n",
      "         [[-0.9616]],\n",
      "\n",
      "         [[-1.0872]]],\n",
      "\n",
      "\n",
      "        [[[-0.9910]],\n",
      "\n",
      "         [[-0.9712]],\n",
      "\n",
      "         [[-1.0957]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0750]],\n",
      "\n",
      "         [[-1.0490]],\n",
      "\n",
      "         [[-1.0497]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.8546]],\n",
      "\n",
      "         [[-0.7040]],\n",
      "\n",
      "         [[-0.8498]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9620]],\n",
      "\n",
      "         [[-0.9263]],\n",
      "\n",
      "         [[-0.9224]]],\n",
      "\n",
      "\n",
      "        [[[-1.0210]],\n",
      "\n",
      "         [[-0.9596]],\n",
      "\n",
      "         [[-0.9613]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0560]],\n",
      "\n",
      "         [[-1.0188]],\n",
      "\n",
      "         [[-1.0562]]],\n",
      "\n",
      "\n",
      "        [[[-1.0219]],\n",
      "\n",
      "         [[-0.9956]],\n",
      "\n",
      "         [[-0.9132]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9673]],\n",
      "\n",
      "         [[-1.0450]],\n",
      "\n",
      "         [[-1.0623]]]])\n",
      "Layer: features.10.expand1x1.bias, Biases: tensor([-1.0319, -1.0398, -0.9519, -1.1193, -1.0673, -0.9160, -0.9142, -0.9307,\n",
      "        -0.9888, -1.0422, -0.9407, -1.0106, -0.9752, -0.9984, -0.7760, -1.1619,\n",
      "        -0.9066, -1.3269, -0.9454, -0.9877, -0.8936, -0.9866, -0.9718, -0.8582,\n",
      "        -0.8604, -1.1181, -1.0385, -0.9992, -0.9576, -1.1180, -1.0103, -1.0905,\n",
      "        -1.0679, -0.8780, -1.0359, -0.8301, -1.1631, -1.1459, -1.0977, -0.9937,\n",
      "        -1.0776, -0.9580, -1.1610, -1.0007, -1.0750, -1.1111, -1.0902, -1.1208,\n",
      "        -1.0905, -0.9748, -0.9061, -1.1814, -1.0518, -0.8602, -1.0828, -1.0023,\n",
      "        -1.0321, -0.9395, -1.0902, -1.0806, -0.7548, -1.0933, -1.1050, -0.9865,\n",
      "        -1.0492, -1.1186, -1.1259, -1.0339, -1.1192, -1.1215, -1.1640, -0.9187,\n",
      "        -1.1645, -0.8811, -1.0211, -1.0895, -1.0464, -1.0554, -1.0902, -1.1049,\n",
      "        -1.0167, -0.7688, -0.9509, -1.0924, -1.1193, -0.9513, -1.0402, -1.1173,\n",
      "        -1.1052, -1.0118, -1.0342, -0.9733, -0.8782, -0.9745, -1.1245, -1.1235,\n",
      "        -0.9315, -1.0741, -0.9476, -0.9951, -0.9882, -1.0817, -1.0464, -1.0382,\n",
      "        -1.0170, -0.8731, -0.9069, -1.1400, -0.8868, -0.9102, -0.9470, -1.0119,\n",
      "        -0.9212, -0.8358, -1.0999, -1.1044, -1.1212, -1.0305, -0.8200, -0.8911,\n",
      "        -1.0588, -0.9709, -0.8729, -0.9790, -0.9057, -0.9743, -1.0423, -1.0514,\n",
      "        -0.9393, -0.9616, -1.0400, -1.0773, -1.0001, -1.0121, -1.1332, -1.0929,\n",
      "        -0.8430, -0.8435, -1.0021, -0.9548, -1.1179, -1.0603, -1.0255, -1.1370,\n",
      "        -0.8250, -0.9745, -0.9535, -0.9068, -0.9756, -0.7597, -1.0464, -1.0226,\n",
      "        -1.1415, -1.0275, -0.9538, -0.8834, -0.9407, -1.1001, -1.0516, -0.9543,\n",
      "        -0.9924, -0.8283, -1.0203, -1.0533, -0.7840, -0.9411, -1.1366, -1.0226,\n",
      "        -1.2039, -0.9710, -0.9329, -0.8938, -1.1959, -1.0389, -1.0291, -0.9777,\n",
      "        -0.9002, -0.9403, -0.9062, -0.8515, -0.9947, -1.1273, -0.9880, -0.9842,\n",
      "        -0.9887, -0.8313, -1.0574, -0.8800, -0.9108, -0.9756, -1.1142, -0.9683,\n",
      "        -0.7888, -0.8915, -0.7799, -1.0676, -1.1733, -0.8412, -1.0864, -0.8321,\n",
      "        -0.9629, -1.0030, -0.9943, -1.0008, -0.9322, -0.9322, -0.9654, -1.0811,\n",
      "        -0.9337, -0.8110, -0.8920, -0.8926, -0.9748, -0.8225, -1.0204, -1.0864,\n",
      "        -0.9919, -1.0799, -1.1494, -0.9316, -1.1088, -1.0903, -1.0266, -1.0404,\n",
      "        -1.0643, -1.1077, -1.0006, -0.9418, -0.9979, -1.0216, -0.9719, -1.0861,\n",
      "        -0.9450, -1.0505, -1.1509, -1.1770, -0.9119, -0.9702, -1.0787, -0.9006,\n",
      "        -1.0479, -1.0833, -0.8964, -0.8980, -0.9546, -0.8818, -0.9693, -0.9300,\n",
      "        -1.0013, -0.8794, -0.7763, -0.9814, -1.0629, -1.0136, -0.8563, -1.0040])\n",
      "Layer: features.10.expand3x3.weight:, Weights: tensor([[[[-1.0708, -0.8395, -0.9244],\n",
      "          [-0.9225, -1.0899, -0.8460],\n",
      "          [-0.8770, -1.0098, -1.1217]],\n",
      "\n",
      "         [[-1.0288, -0.9357, -0.8958],\n",
      "          [-1.0305, -1.0988, -1.1069],\n",
      "          [-0.9156, -1.1044, -1.0653]],\n",
      "\n",
      "         [[-1.0042, -1.0846, -1.0273],\n",
      "          [-1.0289, -0.9425, -1.0283],\n",
      "          [-0.9381, -1.0135, -1.0118]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9796, -0.8230, -0.8014],\n",
      "          [-0.7355, -0.9894, -1.0535],\n",
      "          [-1.0635, -1.0354, -1.0121]],\n",
      "\n",
      "         [[-0.9306, -0.9730, -0.9151],\n",
      "          [-0.9430, -1.1266, -1.1211],\n",
      "          [-0.9779, -0.8938, -0.9679]],\n",
      "\n",
      "         [[-0.8514, -0.8525, -0.9768],\n",
      "          [-1.0364, -1.0177, -0.9828],\n",
      "          [-0.9641, -1.0647, -1.2319]]],\n",
      "\n",
      "\n",
      "        [[[-0.8769, -1.0423, -1.0961],\n",
      "          [-1.0033, -1.0114, -0.9010],\n",
      "          [-1.1644, -0.8946, -0.7842]],\n",
      "\n",
      "         [[-1.0439, -1.0645, -1.1290],\n",
      "          [-1.1169, -0.9898, -0.9717],\n",
      "          [-1.0998, -1.1338, -1.1845]],\n",
      "\n",
      "         [[-0.8623, -0.9239, -1.0054],\n",
      "          [-1.1013, -1.0004, -1.0144],\n",
      "          [-0.8626, -0.9959, -1.0492]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0920, -0.6865, -1.0631],\n",
      "          [-1.0460, -0.9232, -1.0300],\n",
      "          [-0.9763, -0.8897, -0.9027]],\n",
      "\n",
      "         [[-0.9884, -1.0499, -0.9788],\n",
      "          [-1.0227, -1.1471, -0.9541],\n",
      "          [-0.9787, -0.8399, -1.0683]],\n",
      "\n",
      "         [[-1.1624, -1.1309, -0.7697],\n",
      "          [-1.0893, -1.1287, -1.0704],\n",
      "          [-0.9236, -0.8919, -0.9029]]],\n",
      "\n",
      "\n",
      "        [[[-1.1102, -1.0776, -1.1171],\n",
      "          [-0.9120, -0.9895, -0.9612],\n",
      "          [-0.9014, -1.0260, -0.9516]],\n",
      "\n",
      "         [[-0.9093, -0.9744, -1.0529],\n",
      "          [-0.8891, -0.9960, -0.9956],\n",
      "          [-1.1373, -0.7792, -1.1220]],\n",
      "\n",
      "         [[-1.1070, -1.0779, -1.0100],\n",
      "          [-1.1727, -0.9508, -1.0247],\n",
      "          [-1.0972, -0.9944, -1.1194]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0090, -0.9818, -1.2296],\n",
      "          [-0.8425, -0.9957, -0.9600],\n",
      "          [-0.9672, -0.9562, -1.0719]],\n",
      "\n",
      "         [[-1.1293, -1.0506, -1.0478],\n",
      "          [-1.0868, -0.9445, -1.0176],\n",
      "          [-0.9946, -1.0671, -0.9179]],\n",
      "\n",
      "         [[-0.9364, -0.9666, -1.1475],\n",
      "          [-0.9140, -0.8929, -1.0446],\n",
      "          [-1.0952, -1.0830, -1.0886]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0636, -1.0335, -1.1389],\n",
      "          [-0.9406, -1.0440, -1.0791],\n",
      "          [-1.0508, -0.9860, -0.8538]],\n",
      "\n",
      "         [[-1.0049, -0.8223, -0.9614],\n",
      "          [-0.8645, -1.0012, -0.9484],\n",
      "          [-0.8977, -1.0030, -0.9592]],\n",
      "\n",
      "         [[-1.1665, -1.0997, -0.8271],\n",
      "          [-1.0642, -1.0842, -1.0064],\n",
      "          [-1.0176, -1.0035, -1.1180]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9907, -1.0094, -1.0338],\n",
      "          [-0.9777, -1.0651, -0.9797],\n",
      "          [-0.9685, -1.0420, -0.8855]],\n",
      "\n",
      "         [[-1.1698, -0.8174, -0.9412],\n",
      "          [-0.8858, -0.9543, -0.9915],\n",
      "          [-1.1647, -0.9529, -1.0295]],\n",
      "\n",
      "         [[-1.0361, -0.8764, -0.9817],\n",
      "          [-1.0971, -0.9547, -1.1162],\n",
      "          [-0.9856, -0.8357, -1.0225]]],\n",
      "\n",
      "\n",
      "        [[[-0.8828, -0.9077, -0.8393],\n",
      "          [-1.0474, -1.0694, -0.8931],\n",
      "          [-1.0583, -0.8244, -0.8829]],\n",
      "\n",
      "         [[-1.2127, -0.9195, -1.1848],\n",
      "          [-0.8776, -1.0357, -0.8650],\n",
      "          [-1.0315, -1.0567, -0.9090]],\n",
      "\n",
      "         [[-1.1393, -1.0246, -1.0556],\n",
      "          [-0.8953, -1.0249, -1.1619],\n",
      "          [-0.9500, -0.9514, -0.9948]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9121, -0.9487, -1.1669],\n",
      "          [-1.0174, -1.0797, -0.9891],\n",
      "          [-0.9208, -1.0900, -0.9850]],\n",
      "\n",
      "         [[-1.0246, -0.9079, -0.9062],\n",
      "          [-1.0457, -1.0558, -1.2045],\n",
      "          [-1.0725, -1.1020, -0.8940]],\n",
      "\n",
      "         [[-1.0239, -0.9785, -0.9259],\n",
      "          [-0.9650, -0.9948, -1.0104],\n",
      "          [-1.0496, -0.9201, -1.1100]]],\n",
      "\n",
      "\n",
      "        [[[-0.8367, -1.0263, -0.9924],\n",
      "          [-1.0701, -1.0062, -0.9556],\n",
      "          [-1.0181, -0.9622, -0.9024]],\n",
      "\n",
      "         [[-1.0085, -0.9786, -1.0816],\n",
      "          [-0.9656, -0.8126, -0.8485],\n",
      "          [-1.0261, -1.0069, -1.0260]],\n",
      "\n",
      "         [[-0.9995, -0.9528, -0.9382],\n",
      "          [-0.9076, -0.9412, -1.0915],\n",
      "          [-1.0125, -1.1212, -1.0706]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9807, -0.9874, -0.9816],\n",
      "          [-1.0402, -1.0777, -1.0088],\n",
      "          [-0.9383, -1.0570, -1.0320]],\n",
      "\n",
      "         [[-0.9608, -0.8437, -0.9376],\n",
      "          [-1.0140, -1.1655, -0.8897],\n",
      "          [-1.1157, -1.0775, -0.9500]],\n",
      "\n",
      "         [[-0.9972, -0.9849, -0.9666],\n",
      "          [-0.8591, -0.9803, -1.1683],\n",
      "          [-1.0796, -0.9757, -1.0436]]]])\n",
      "Layer: features.10.expand3x3.bias, Biases: tensor([-1.0180, -1.0400, -1.1878, -1.0835, -1.0378, -1.0085, -1.0819, -0.9824,\n",
      "        -1.0185, -1.0526, -0.8064, -0.8981, -1.0999, -1.1903, -0.9822, -1.0126,\n",
      "        -1.0888, -1.0606, -0.8542, -1.1171, -0.9315, -1.0950, -0.8192, -0.9692,\n",
      "        -0.8681, -0.9795, -0.9401, -1.0839, -1.1552, -1.0528, -0.8584, -0.7950,\n",
      "        -0.9568, -1.0281, -0.9245, -0.9411, -1.0675, -0.8397, -0.9541, -0.9998,\n",
      "        -1.2336, -0.9917, -0.9876, -0.9590, -1.0257, -1.0927, -1.1056, -1.0017,\n",
      "        -1.0536, -0.8349, -0.9931, -1.1510, -0.9909, -1.0526, -0.9585, -0.8506,\n",
      "        -1.0194, -1.0199, -0.9905, -0.9924, -0.9898, -1.0118, -0.8538, -1.0767,\n",
      "        -1.0122, -0.9349, -1.0964, -0.9680, -0.9514, -1.0517, -1.0385, -1.0998,\n",
      "        -1.1167, -1.0793, -0.8402, -1.2392, -1.0570, -0.7663, -1.0264, -1.0740,\n",
      "        -0.8959, -0.9982, -0.8095, -1.0093, -0.9594, -0.9790, -0.9757, -0.9766,\n",
      "        -1.0716, -1.0624, -1.0619, -0.9770, -0.9640, -0.9768, -1.0004, -0.9432,\n",
      "        -1.1716, -0.8822, -0.8903, -0.9699, -1.1442, -0.9921, -0.8690, -0.9441,\n",
      "        -0.8471, -1.0065, -0.8261, -1.0602, -1.0094, -1.0572, -0.9772, -1.0279,\n",
      "        -0.9580, -1.0743, -0.8504, -1.0251, -0.8558, -0.9121, -0.8117, -1.0690,\n",
      "        -0.9169, -0.9166, -1.1271, -1.0963, -1.0371, -0.9830, -0.9593, -0.9648,\n",
      "        -0.9904, -1.0806, -0.9531, -1.0277, -0.9586, -0.8666, -0.9679, -0.9872,\n",
      "        -1.0101, -0.7692, -1.1943, -0.9083, -0.9274, -0.9652, -1.0556, -1.0117,\n",
      "        -1.0617, -0.9606, -0.9325, -1.0184, -1.0259, -0.8427, -1.0082, -1.0403,\n",
      "        -1.1632, -1.0899, -1.1456, -1.1014, -1.1003, -1.2507, -0.9508, -0.9711,\n",
      "        -0.9005, -0.9739, -1.2380, -1.1645, -1.0886, -0.9591, -0.9588, -1.0415,\n",
      "        -1.0982, -1.0881, -0.9519, -1.0804, -0.9645, -1.0300, -1.0113, -0.9245,\n",
      "        -0.9596, -0.9907, -0.8987, -0.9676, -0.9349, -0.9119, -1.0277, -0.9951,\n",
      "        -0.8447, -0.8890, -0.8936, -1.0360, -1.0568, -1.0651, -1.0173, -0.9651,\n",
      "        -1.0548, -0.9264, -1.0889, -1.1048, -1.0463, -0.9462, -0.9979, -0.9678,\n",
      "        -1.3389, -0.7773, -1.0840, -0.9145, -1.1913, -1.1840, -1.1328, -0.8671,\n",
      "        -1.0507, -1.1201, -1.0426, -0.9574, -0.9776, -0.9620, -0.8578, -1.0733,\n",
      "        -0.9683, -1.0645, -0.9770, -1.0373, -0.9977, -1.1304, -0.9389, -0.8377,\n",
      "        -0.7600, -0.8885, -0.9721, -1.0551, -1.0316, -1.0865, -0.9558, -0.9051,\n",
      "        -0.9633, -0.9903, -1.0322, -0.8845, -1.0981, -1.0331, -1.0127, -1.0601,\n",
      "        -1.0049, -1.1191, -0.9148, -0.8533, -0.8764, -0.9508, -1.0574, -1.0262,\n",
      "        -0.9755, -0.9805, -1.1594, -0.9489, -1.1531, -1.0339, -1.0326, -0.9493])\n",
      "Layer: features.12.squeeze.weight:, Weights: tensor([[[[-0.8504]],\n",
      "\n",
      "         [[-1.1096]],\n",
      "\n",
      "         [[-0.9953]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0221]],\n",
      "\n",
      "         [[-0.7979]],\n",
      "\n",
      "         [[-0.9711]]],\n",
      "\n",
      "\n",
      "        [[[-0.9310]],\n",
      "\n",
      "         [[-1.0520]],\n",
      "\n",
      "         [[-0.9891]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1027]],\n",
      "\n",
      "         [[-1.0297]],\n",
      "\n",
      "         [[-1.0824]]],\n",
      "\n",
      "\n",
      "        [[[-1.0268]],\n",
      "\n",
      "         [[-1.1748]],\n",
      "\n",
      "         [[-0.9281]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0761]],\n",
      "\n",
      "         [[-1.0126]],\n",
      "\n",
      "         [[-0.8572]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.1919]],\n",
      "\n",
      "         [[-1.0010]],\n",
      "\n",
      "         [[-0.9391]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9377]],\n",
      "\n",
      "         [[-1.0247]],\n",
      "\n",
      "         [[-1.0048]]],\n",
      "\n",
      "\n",
      "        [[[-1.4056]],\n",
      "\n",
      "         [[-0.9964]],\n",
      "\n",
      "         [[-0.8550]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0619]],\n",
      "\n",
      "         [[-0.8973]],\n",
      "\n",
      "         [[-0.9018]]],\n",
      "\n",
      "\n",
      "        [[[-1.0470]],\n",
      "\n",
      "         [[-0.7760]],\n",
      "\n",
      "         [[-1.1084]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0722]],\n",
      "\n",
      "         [[-0.9939]],\n",
      "\n",
      "         [[-1.1530]]]])\n",
      "Layer: features.12.squeeze.bias, Biases: tensor([-1.0655, -0.9580, -0.9837, -0.7824, -0.8729, -0.8706, -0.8956, -0.8966,\n",
      "        -0.8994, -1.1897, -0.9415, -0.9529, -0.9891, -1.0979, -1.0491, -1.0907,\n",
      "        -0.9552, -1.0155, -0.8835, -0.7930, -1.0282, -1.1859, -1.2074, -1.1283,\n",
      "        -1.0172, -1.1632, -1.0237, -1.1989, -0.9942, -1.0485, -0.7213, -0.9755,\n",
      "        -0.8308, -0.9918, -1.0765, -1.0439, -1.2159, -1.1185, -0.8840, -1.0660,\n",
      "        -0.9839, -0.9687, -0.8078, -1.1817, -0.8903, -0.8396, -1.0470, -1.0611,\n",
      "        -1.0028, -0.9002, -1.0523, -1.1612, -0.8648, -0.8499, -1.0246, -1.0834,\n",
      "        -0.8468, -0.9696, -0.9080, -0.9848, -1.0925, -1.0273, -0.8154, -0.9173])\n",
      "Layer: features.12.expand1x1.weight:, Weights: tensor([[[[-0.8937]],\n",
      "\n",
      "         [[-1.1142]],\n",
      "\n",
      "         [[-1.0983]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8603]],\n",
      "\n",
      "         [[-1.0996]],\n",
      "\n",
      "         [[-0.9806]]],\n",
      "\n",
      "\n",
      "        [[[-0.8469]],\n",
      "\n",
      "         [[-1.0167]],\n",
      "\n",
      "         [[-1.0799]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1317]],\n",
      "\n",
      "         [[-1.1598]],\n",
      "\n",
      "         [[-1.0263]]],\n",
      "\n",
      "\n",
      "        [[[-1.1257]],\n",
      "\n",
      "         [[-1.0697]],\n",
      "\n",
      "         [[-0.9795]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0105]],\n",
      "\n",
      "         [[-1.0508]],\n",
      "\n",
      "         [[-1.1567]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.9940]],\n",
      "\n",
      "         [[-1.2288]],\n",
      "\n",
      "         [[-1.0403]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1029]],\n",
      "\n",
      "         [[-0.9737]],\n",
      "\n",
      "         [[-0.9720]]],\n",
      "\n",
      "\n",
      "        [[[-1.0681]],\n",
      "\n",
      "         [[-0.9646]],\n",
      "\n",
      "         [[-0.9939]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8378]],\n",
      "\n",
      "         [[-0.9303]],\n",
      "\n",
      "         [[-1.2261]]],\n",
      "\n",
      "\n",
      "        [[[-1.0226]],\n",
      "\n",
      "         [[-1.0508]],\n",
      "\n",
      "         [[-0.9426]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0912]],\n",
      "\n",
      "         [[-1.0122]],\n",
      "\n",
      "         [[-0.9893]]]])\n",
      "Layer: features.12.expand1x1.bias, Biases: tensor([-0.9485, -0.9618, -1.1319, -1.1362, -1.1205, -0.9107, -0.9347, -1.0590,\n",
      "        -1.0979, -0.9755, -0.7628, -0.9528, -0.9861, -1.1418, -0.8656, -0.9948,\n",
      "        -0.9771, -1.1106, -1.0313, -0.8833, -0.9761, -1.2239, -1.1334, -1.0247,\n",
      "        -1.2023, -0.9728, -0.9382, -1.0849, -1.0560, -1.1838, -0.9902, -1.0193,\n",
      "        -1.0032, -0.8890, -0.9683, -1.1229, -1.0975, -0.7885, -1.0080, -0.9941,\n",
      "        -1.0484, -1.2117, -0.9678, -0.9629, -0.9927, -1.0102, -0.9306, -1.1006,\n",
      "        -1.0997, -0.9128, -0.9641, -1.0452, -1.1833, -0.9803, -1.0094, -0.9626,\n",
      "        -0.9149, -0.9764, -1.0692, -0.9557, -1.0802, -0.9237, -1.0422, -1.1859,\n",
      "        -0.9793, -0.9466, -0.9826, -0.8107, -1.0103, -0.9543, -1.1090, -0.9227,\n",
      "        -0.9724, -0.9309, -1.0612, -0.9577, -1.0319, -0.9058, -1.0431, -1.1844,\n",
      "        -0.8252, -1.2120, -0.9672, -0.9551, -1.0064, -1.1138, -0.9866, -0.9456,\n",
      "        -0.9061, -0.8851, -1.0560, -0.9259, -1.0634, -0.9139, -1.0460, -1.0600,\n",
      "        -1.0576, -0.8821, -1.1368, -1.0068, -0.9143, -1.0401, -0.9123, -1.1693,\n",
      "        -0.9322, -0.9445, -1.2570, -1.0653, -0.9457, -0.9229, -0.9667, -1.0058,\n",
      "        -0.9227, -0.9921, -0.9589, -0.8055, -1.1384, -0.9733, -0.8338, -0.9689,\n",
      "        -0.9281, -0.8478, -1.0458, -1.0537, -0.9638, -1.0943, -1.1514, -1.1602,\n",
      "        -0.9448, -1.0748, -1.0792, -0.8302, -0.9672, -1.1707, -1.2204, -0.8486,\n",
      "        -0.9125, -0.9643, -1.0699, -1.0646, -1.0327, -0.9112, -0.7422, -1.1407,\n",
      "        -1.0607, -1.0818, -1.0009, -1.0759, -0.9728, -1.1419, -1.0331, -0.8650,\n",
      "        -1.1168, -1.0890, -0.8683, -0.9279, -0.8397, -1.0050, -1.1027, -0.9862,\n",
      "        -0.8377, -0.9008, -0.9961, -0.8656, -1.1830, -0.9905, -0.9877, -1.0146,\n",
      "        -0.9366, -0.9475, -1.1340, -0.9053, -0.9643, -1.0307, -0.7885, -1.0387,\n",
      "        -1.0384, -0.9337, -1.1399, -1.0226, -1.0354, -1.0626, -0.9856, -0.7417,\n",
      "        -0.7813, -1.1388, -1.0370, -0.9635, -1.0046, -1.0819, -1.0409, -0.9408,\n",
      "        -0.9327, -0.9770, -1.0706, -1.1738, -0.8527, -0.9802, -0.8742, -0.9922,\n",
      "        -1.0983, -0.6725, -1.1046, -0.8300, -1.1539, -0.8237, -1.2211, -1.0594,\n",
      "        -0.8656, -1.0506, -0.9115, -1.0186, -0.9128, -1.0743, -0.9369, -0.8727,\n",
      "        -1.1606, -0.7753, -0.8688, -1.0847, -1.0987, -1.0958, -0.9881, -0.9585,\n",
      "        -1.0795, -0.9557, -1.0446, -1.0530, -0.8413, -1.2224, -1.0275, -0.8832,\n",
      "        -1.0447, -0.9111, -0.9400, -1.0212, -0.9661, -0.7877, -0.8579, -0.9033,\n",
      "        -0.9104, -1.0592, -1.1231, -1.0680, -1.0010, -1.1057, -0.9203, -0.9157,\n",
      "        -1.1011, -0.9188, -1.0028, -0.9846, -1.0036, -0.8084, -0.9288, -1.1306])\n",
      "Layer: features.12.expand3x3.weight:, Weights: tensor([[[[-0.9196, -0.9057, -1.1179],\n",
      "          [-0.7836, -1.0691, -0.9025],\n",
      "          [-1.0424, -1.0269, -1.0712]],\n",
      "\n",
      "         [[-0.9267, -0.8630, -1.0274],\n",
      "          [-0.9257, -1.2475, -0.9593],\n",
      "          [-0.9397, -1.0343, -1.0792]],\n",
      "\n",
      "         [[-1.2204, -1.0389, -0.9335],\n",
      "          [-0.9342, -0.9334, -0.8609],\n",
      "          [-0.9884, -0.9767, -1.0271]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0701, -1.2795, -0.8380],\n",
      "          [-1.0529, -0.8846, -1.0883],\n",
      "          [-1.1154, -0.8905, -1.1154]],\n",
      "\n",
      "         [[-0.9767, -1.0234, -0.9761],\n",
      "          [-0.9376, -1.1564, -0.9932],\n",
      "          [-0.9006, -1.0373, -0.9841]],\n",
      "\n",
      "         [[-0.7630, -1.0122, -1.0348],\n",
      "          [-1.0792, -0.9549, -0.9567],\n",
      "          [-0.8385, -0.9958, -1.0628]]],\n",
      "\n",
      "\n",
      "        [[[-1.1921, -0.8884, -0.9054],\n",
      "          [-1.0128, -1.0239, -1.0509],\n",
      "          [-0.8536, -0.9015, -0.9182]],\n",
      "\n",
      "         [[-1.0292, -0.9415, -0.9169],\n",
      "          [-0.8485, -1.0802, -0.9305],\n",
      "          [-1.1217, -0.8166, -1.0823]],\n",
      "\n",
      "         [[-1.0283, -1.0340, -1.0860],\n",
      "          [-0.8071, -0.9511, -1.0257],\n",
      "          [-0.9424, -0.9096, -0.8784]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9786, -0.8653, -0.9798],\n",
      "          [-1.0919, -1.1099, -1.0975],\n",
      "          [-1.0159, -0.8816, -1.0554]],\n",
      "\n",
      "         [[-1.0209, -0.9272, -0.9713],\n",
      "          [-1.0053, -0.8989, -0.9312],\n",
      "          [-0.9638, -1.0047, -1.2605]],\n",
      "\n",
      "         [[-0.9207, -1.0681, -1.1507],\n",
      "          [-0.7833, -0.9627, -0.9327],\n",
      "          [-1.1290, -1.1534, -0.8295]]],\n",
      "\n",
      "\n",
      "        [[[-0.9684, -0.9311, -0.9934],\n",
      "          [-0.9212, -1.0388, -1.0619],\n",
      "          [-0.9393, -0.9956, -0.8678]],\n",
      "\n",
      "         [[-0.9574, -1.0610, -0.9938],\n",
      "          [-0.8470, -1.0372, -1.0311],\n",
      "          [-0.7882, -0.9175, -0.9758]],\n",
      "\n",
      "         [[-1.0523, -0.9503, -0.9863],\n",
      "          [-0.9851, -0.9243, -1.0253],\n",
      "          [-0.9761, -1.1148, -1.0732]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9787, -0.7226, -0.9301],\n",
      "          [-1.1116, -1.0200, -0.9120],\n",
      "          [-1.2273, -1.0020, -1.1773]],\n",
      "\n",
      "         [[-1.0477, -0.9187, -0.9946],\n",
      "          [-0.9884, -0.8676, -1.0434],\n",
      "          [-1.0336, -1.0119, -0.8614]],\n",
      "\n",
      "         [[-0.7921, -0.7971, -0.9856],\n",
      "          [-0.9372, -0.8580, -1.0097],\n",
      "          [-1.0600, -0.7957, -1.0943]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.9426, -1.0101, -1.1531],\n",
      "          [-0.9517, -0.9414, -1.0654],\n",
      "          [-1.0301, -0.8066, -0.7974]],\n",
      "\n",
      "         [[-0.9635, -0.7746, -1.1126],\n",
      "          [-1.0255, -1.1116, -0.8980],\n",
      "          [-0.8925, -0.9728, -0.9556]],\n",
      "\n",
      "         [[-1.0404, -0.8793, -0.8626],\n",
      "          [-1.1898, -0.9283, -1.0815],\n",
      "          [-0.8366, -0.8659, -0.9398]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8531, -0.9439, -1.0925],\n",
      "          [-1.0357, -1.0761, -1.0810],\n",
      "          [-1.0805, -0.9240, -0.9772]],\n",
      "\n",
      "         [[-1.3239, -0.9191, -1.0357],\n",
      "          [-1.0113, -1.0456, -1.2405],\n",
      "          [-0.9744, -1.1424, -1.0178]],\n",
      "\n",
      "         [[-0.9809, -0.9164, -1.0026],\n",
      "          [-0.9909, -1.1004, -1.0751],\n",
      "          [-1.0887, -0.6662, -1.0678]]],\n",
      "\n",
      "\n",
      "        [[[-1.2291, -1.0114, -1.0332],\n",
      "          [-1.0258, -1.0555, -1.1147],\n",
      "          [-0.9325, -0.9900, -0.8119]],\n",
      "\n",
      "         [[-1.2062, -0.8725, -0.9049],\n",
      "          [-1.1189, -1.0647, -1.0870],\n",
      "          [-1.0235, -1.0947, -0.9166]],\n",
      "\n",
      "         [[-1.1657, -1.0647, -0.9529],\n",
      "          [-0.9378, -1.0931, -0.9059],\n",
      "          [-1.2145, -1.0403, -1.2129]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9859, -0.8879, -1.1506],\n",
      "          [-1.1038, -0.9505, -0.9917],\n",
      "          [-1.1763, -0.8942, -0.9897]],\n",
      "\n",
      "         [[-0.9897, -0.9597, -0.9617],\n",
      "          [-1.0362, -0.8937, -0.9031],\n",
      "          [-0.9475, -0.9415, -1.0290]],\n",
      "\n",
      "         [[-0.9660, -0.9787, -1.2665],\n",
      "          [-0.9536, -0.9569, -1.0077],\n",
      "          [-1.1006, -0.9747, -1.0489]]],\n",
      "\n",
      "\n",
      "        [[[-0.9970, -0.8233, -0.9731],\n",
      "          [-1.0855, -1.1389, -1.0144],\n",
      "          [-0.9559, -1.1151, -1.0646]],\n",
      "\n",
      "         [[-1.1645, -1.0023, -0.9492],\n",
      "          [-0.9068, -0.9791, -1.2759],\n",
      "          [-1.0563, -1.1324, -0.9428]],\n",
      "\n",
      "         [[-0.8906, -0.8964, -1.0567],\n",
      "          [-1.1682, -1.2653, -1.0765],\n",
      "          [-1.0175, -1.0814, -0.9199]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9808, -0.8800, -0.9626],\n",
      "          [-1.0394, -0.9629, -1.1457],\n",
      "          [-1.0195, -0.8989, -1.1251]],\n",
      "\n",
      "         [[-1.0651, -0.8113, -1.0124],\n",
      "          [-0.9943, -0.9374, -1.1045],\n",
      "          [-0.9685, -1.0752, -1.0511]],\n",
      "\n",
      "         [[-0.9667, -1.0120, -1.0300],\n",
      "          [-0.9415, -0.7897, -1.1711],\n",
      "          [-1.0306, -1.0555, -0.9464]]]])\n",
      "Layer: features.12.expand3x3.bias, Biases: tensor([-0.9549, -1.1376, -0.9475, -0.9722, -1.1289, -1.0426, -1.0192, -1.1102,\n",
      "        -0.9555, -0.9651, -0.9979, -1.3182, -1.0671, -0.9697, -0.8334, -0.9387,\n",
      "        -1.0516, -0.8843, -1.0503, -1.0146, -1.1189, -0.9877, -0.6991, -1.1149,\n",
      "        -0.9535, -1.0315, -1.0080, -1.1003, -0.9626, -1.0399, -1.0864, -1.0333,\n",
      "        -0.9813, -1.1004, -0.9875, -0.8557, -0.8287, -0.8538, -0.9698, -1.0433,\n",
      "        -0.9915, -0.9681, -0.9923, -1.1132, -0.9686, -1.0140, -0.9348, -0.8792,\n",
      "        -1.0612, -0.8861, -1.1419, -0.9086, -1.0865, -1.0677, -0.9204, -1.1677,\n",
      "        -0.9026, -1.0172, -0.9648, -1.0489, -1.1621, -1.0854, -1.0458, -0.8732,\n",
      "        -0.9450, -0.8391, -0.9952, -1.0007, -0.9160, -0.9615, -0.9242, -0.8377,\n",
      "        -1.0614, -0.9305, -1.0163, -1.1126, -1.1035, -0.9659, -1.1638, -1.0288,\n",
      "        -0.9067, -1.2179, -0.9548, -1.0530, -1.0070, -0.7961, -1.0696, -0.9678,\n",
      "        -1.1455, -0.8418, -1.1332, -0.7753, -1.0532, -1.1772, -0.8685, -1.0351,\n",
      "        -0.8652, -0.9629, -0.9206, -0.9470, -1.0694, -1.0025, -1.0742, -1.0134,\n",
      "        -1.1208, -0.9805, -0.9975, -0.9051, -0.9758, -0.9311, -1.0109, -0.8145,\n",
      "        -0.9822, -1.0294, -1.0806, -1.0483, -1.0093, -0.7171, -0.9120, -1.1204,\n",
      "        -1.1681, -1.0663, -1.0507, -0.8825, -0.8452, -0.7326, -1.0107, -1.0073,\n",
      "        -1.0695, -0.8744, -0.9920, -1.2189, -1.0816, -1.0798, -1.1645, -1.0103,\n",
      "        -0.9515, -0.9113, -1.1216, -0.9312, -0.9222, -1.0329, -1.0682, -1.1121,\n",
      "        -1.0380, -0.9248, -0.8995, -1.1049, -1.0832, -0.9406, -1.0191, -1.0206,\n",
      "        -1.0875, -1.0148, -0.9623, -1.0266, -1.0208, -1.0675, -0.9306, -0.9054,\n",
      "        -1.1658, -1.1183, -0.7867, -0.9289, -1.1360, -1.0021, -0.9955, -0.9288,\n",
      "        -0.9588, -1.0924, -1.0400, -1.0610, -0.9739, -0.9291, -0.9964, -1.0183,\n",
      "        -0.8673, -1.1579, -0.8944, -0.9953, -1.0778, -0.9444, -1.1388, -0.9465,\n",
      "        -0.8960, -1.2002, -0.8026, -0.9689, -0.9242, -1.1798, -0.9416, -0.9786,\n",
      "        -1.0677, -0.9147, -0.9119, -0.9137, -1.1447, -0.9888, -1.1388, -0.9293,\n",
      "        -0.8750, -0.9798, -0.8640, -1.0883, -0.9510, -1.0244, -0.8280, -1.1982,\n",
      "        -1.0792, -0.9217, -0.8946, -0.8800, -0.7885, -1.0418, -1.0769, -1.0358,\n",
      "        -0.7992, -1.0913, -0.9529, -1.0321, -1.0451, -0.9206, -1.1053, -0.8939,\n",
      "        -0.9542, -0.7911, -1.1136, -0.7401, -1.0416, -1.1074, -0.9352, -1.0273,\n",
      "        -1.1788, -1.1857, -0.8411, -0.9996, -1.0230, -0.9121, -1.0350, -0.9999,\n",
      "        -0.9242, -0.8715, -1.0345, -0.9814, -1.0340, -1.0376, -1.1539, -1.0832,\n",
      "        -1.0006, -0.9576, -0.9603, -1.2048, -1.0476, -1.1770, -0.9584, -0.9360])\n",
      "Layer: classifier.0.weight:, Weights: tensor([[[[-0.9916]],\n",
      "\n",
      "         [[-0.9674]],\n",
      "\n",
      "         [[-1.0919]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8978]],\n",
      "\n",
      "         [[-1.0211]],\n",
      "\n",
      "         [[-1.1368]]],\n",
      "\n",
      "\n",
      "        [[[-1.0631]],\n",
      "\n",
      "         [[-0.9606]],\n",
      "\n",
      "         [[-0.8673]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0139]],\n",
      "\n",
      "         [[-0.9781]],\n",
      "\n",
      "         [[-0.9010]]],\n",
      "\n",
      "\n",
      "        [[[-1.0166]],\n",
      "\n",
      "         [[-0.9225]],\n",
      "\n",
      "         [[-1.0588]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0515]],\n",
      "\n",
      "         [[-1.1053]],\n",
      "\n",
      "         [[-0.9742]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0428]],\n",
      "\n",
      "         [[-0.9346]],\n",
      "\n",
      "         [[-0.9747]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0476]],\n",
      "\n",
      "         [[-0.8114]],\n",
      "\n",
      "         [[-1.1469]]],\n",
      "\n",
      "\n",
      "        [[[-0.8604]],\n",
      "\n",
      "         [[-0.9580]],\n",
      "\n",
      "         [[-1.0604]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1256]],\n",
      "\n",
      "         [[-1.0460]],\n",
      "\n",
      "         [[-1.0105]]],\n",
      "\n",
      "\n",
      "        [[[-0.7927]],\n",
      "\n",
      "         [[-0.8809]],\n",
      "\n",
      "         [[-0.9758]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9816]],\n",
      "\n",
      "         [[-1.0301]],\n",
      "\n",
      "         [[-0.9062]]]])\n",
      "Layer: classifier.0.bias, Biases: tensor([-0.8857, -1.0325, -1.0690, -0.8945, -0.9757, -1.0141, -1.0742, -1.0882,\n",
      "        -1.1449, -0.8877])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  if \"weight\" in name:\n",
    "    print(f\"Layer: {name}:, Weights: {param.data}\")\n",
    "  elif \"bias\" in name:\n",
    "    print(f\"Layer: {name}, Biases: {param.data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
