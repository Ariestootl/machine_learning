{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: Index(['tconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult',\n",
      "       'startYear', 'runtimeMinutes', 'genres', 'averageRating', 'numVotes',\n",
      "       'HitScore'],\n",
      "      dtype='object')\n",
      "Shape: (199595, 11)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"movies_new.csv\")\n",
    "df = df.sort_values(by=\"startYear\")\n",
    "print(f\"Features: {df.columns}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "df['genres_list'] = df['genres'].apply(lambda x: x.split(','))\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_encoded = mlb.fit_transform(df['genres_list'])  # Fit & transform genres\n",
    "\n",
    "genre_columns = mlb.classes_  # Get genre names as column names\n",
    "df[genre_columns] = genre_encoded  # Add encoded genres to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "df['isAdult'] = df['isAdult'].astype(int)  \n",
    "df['numVotes_log'] = np.log1p(df['numVotes'])\n",
    "\n",
    "df['numVotes_log'] = np.log1p(df['numVotes'])\n",
    "numeric_columns = ['averageRating', 'HitScore', 'numVotes', 'numVotes_log', 'runtimeMinutes', 'startYear']\n",
    "df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "df.loc[:, 'runtimeMinutes'] = df['runtimeMinutes'].fillna(df['runtimeMinutes'].median())\n",
    "df.drop(columns=[\"originalTitle\"], inplace=True)\n",
    "\n",
    "# Define features and target\n",
    "X= df[['averageRating', 'HitScore', 'numVotes_log', 'runtimeMinutes']+ list(genre_columns)].to_numpy()\n",
    "y = df['HitScore'].to_numpy().reshape(-1,1)\n",
    "\n",
    "# Split data into training (80%) and temp (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split temp (20%) into validation (10%) and test (10%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"{X_train.shape[0] + X_val.shape[0] + X_test.shape[0] == df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerX = StandardScaler()\n",
    "X_train_scaled = scalerX.fit_transform(X_train)  # Fit on training set\n",
    "X_test_scaled = scalerX.transform(X_test)\n",
    "X_val_scaled = scalerX.transform(X_val)  # Apply on test set\n",
    "\n",
    "\n",
    "scalerY = StandardScaler()\n",
    "y_train_scaled = scalerY.fit_transform(y_train.reshape(-1, 1))  # Fit on training set\n",
    "y_test_scaled = scalerY.transform(y_test.reshape(-1, 1))  # Apply on test set\n",
    "y_val_scaled = scalerY.transform(y_val.reshape(-1, 1))  # Apply on validation test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame as a Time Series problem\n",
    "* Convert the data into a sequential type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (159671, 5, 31)\n",
      "Val shape: (19954, 5, 31)\n",
      "Test shape: (19955, 5, 31)\n"
     ]
    }
   ],
   "source": [
    "# Define time step (e.g., 5 years)\n",
    "TIME_STEP = 5  \n",
    "\n",
    "def create_sequences(X, y, time_step):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_step):\n",
    "        Xs.append(X[i:i+time_step])  # Past 5 years\n",
    "        ys.append(y[i+time_step])    # Target is the next year\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Convert datasets into 3D shape (samples, time steps, features)\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, TIME_STEP)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val, TIME_STEP)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, TIME_STEP)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Train shape:\", X_train_seq.shape)  # (samples, time_steps, features)\n",
    "print(\"Val shape:\", X_val_seq.shape)\n",
    "print(\"Test shape:\", X_test_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([159671, 5, 31]) torch.Size([159671, 1])\n",
      "torch.Size([19954, 5, 31]) torch.Size([19954, 1])\n",
      "torch.Size([19955, 5, 31]) torch.Size([19955, 1])\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_seq, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_seq, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Check shapes\n",
    "print(X_train_tensor.shape, y_train_tensor.shape)\n",
    "print(X_val_tensor.shape, y_val_tensor.shape)\n",
    "print(X_test_tensor.shape, y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Dataloaders from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders:\n",
      "Train dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f3ceeb0bf80>\n",
      "Test dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f3cd78dbf80>\n",
      "Validation dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f3ce87c3da0>\n",
      "Length of train dataloader: 1248 batches of 128\n",
      "Length of validation dataloader: 156 batches of 128\n",
      "Length of test dataloader: 156 batches of 128\n",
      "========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 128  \n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=os.cpu_count(), shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=os.cpu_count(), shuffle=False) \n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=os.cpu_count(), shuffle=False)\n",
    "\n",
    "print(f\"Dataloaders:\\nTrain dataloader: {train_dataloader}\\nTest dataloader: {test_dataloader}\\nValidation dataloader: {val_dataloader}\")\n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {train_dataloader.batch_size}\")\n",
    "print(f\"Length of validation dataloader: {len(val_dataloader)} batches of {val_dataloader.batch_size}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {test_dataloader.batch_size}\")\n",
    "print(f\"==============================================================================\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adan: Adaptive Nesterov Momentum Algorithm\n",
    "\n",
    "import math\n",
    "from typing import List\n",
    "from torch import Tensor\n",
    "\n",
    "class MultiTensorApply(object):\n",
    "    available = False\n",
    "    warned = False\n",
    "\n",
    "    def __init__(self, chunk_size):\n",
    "        try:\n",
    "            MultiTensorApply.available = True\n",
    "            self.chunk_size = chunk_size\n",
    "        except ImportError as err:\n",
    "            MultiTensorApply.available = False\n",
    "            MultiTensorApply.import_err = err\n",
    "\n",
    "    def __call__(self, op, noop_flag_buffer, tensor_lists, *args):\n",
    "        return op(self.chunk_size, noop_flag_buffer, tensor_lists, *args)\n",
    "\n",
    "\n",
    "class Adan(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Implements a pytorch variant of Adan\n",
    "    Adan was proposed in\n",
    "    Adan: Adaptive Nesterov Momentum Algorithm for\n",
    "        Faster Optimizing Deep Models[J].arXiv preprint arXiv:2208.06677, 2022.\n",
    "    https://arxiv.org/abs/2208.06677\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or\n",
    "            dicts defining parameter groups.\n",
    "        lr (float, optional): learning rate. (default: 1e-3)\n",
    "        betas (Tuple[float, float, flot], optional): coefficients used for\n",
    "            first- and second-order moments. (default: (0.98, 0.92, 0.99))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability. (default: 1e-8)\n",
    "        weight_decay (float, optional): decoupled weight decay\n",
    "            (L2 penalty) (default: 0)\n",
    "        max_grad_norm (float, optional): value used to clip\n",
    "            global grad norm (default: 0.0 no clip)\n",
    "        no_prox (bool): how to perform the decoupled weight decay\n",
    "            (default: False)\n",
    "        foreach (bool): if True would use torch._foreach implementation.\n",
    "            It's faster but uses slightly more memory. (default: True)\n",
    "        fused (bool, optional): whether fused implementation is used.\n",
    "            (default: False)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-3,\n",
    "                 betas=(0.98, 0.92, 0.99),\n",
    "                 eps=1e-8,\n",
    "                 weight_decay=0.0,\n",
    "                 max_grad_norm=0.0,\n",
    "                 no_prox=False,\n",
    "                 foreach: bool = True,\n",
    "                 fused: bool = False):\n",
    "        if not 0.0 <= max_grad_norm:\n",
    "            raise ValueError('Invalid Max grad norm: {}'.format(max_grad_norm))\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError('Invalid learning rate: {}'.format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError('Invalid epsilon value: {}'.format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 0: {}'.format(\n",
    "                betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 1: {}'.format(\n",
    "                betas[1]))\n",
    "        if not 0.0 <= betas[2] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 2: {}'.format(\n",
    "                betas[2]))\n",
    "        if fused:\n",
    "            _check_fused_available()\n",
    "\n",
    "        defaults = dict(lr=lr,\n",
    "                        betas=betas,\n",
    "                        eps=eps,\n",
    "                        weight_decay=weight_decay,\n",
    "                        max_grad_norm=max_grad_norm,\n",
    "                        no_prox=no_prox,\n",
    "                        foreach=foreach,\n",
    "                        fused=fused)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adan, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('no_prox', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restart_opt(self):\n",
    "        for group in self.param_groups:\n",
    "            group['step'] = 0\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    state = self.state[p]\n",
    "                    # State initialization\n",
    "\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of gradient difference\n",
    "                    state['exp_avg_diff'] = torch.zeros_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\"\"\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        if self.defaults['max_grad_norm'] > 0:\n",
    "            device = self.param_groups[0]['params'][0].device\n",
    "            global_grad_norm = torch.zeros(1, device=device)\n",
    "\n",
    "            max_grad_norm = torch.tensor(self.defaults['max_grad_norm'],\n",
    "                                         device=device)\n",
    "            for group in self.param_groups:\n",
    "\n",
    "                for p in group['params']:\n",
    "                    if p.grad is not None:\n",
    "                        grad = p.grad\n",
    "                        global_grad_norm.add_(grad.pow(2).sum())\n",
    "\n",
    "            global_grad_norm = torch.sqrt(global_grad_norm)\n",
    "\n",
    "            clip_global_grad_norm = torch.clamp(\n",
    "                max_grad_norm / (global_grad_norm + group['eps']),\n",
    "                max=1.0).item()\n",
    "        else:\n",
    "            clip_global_grad_norm = 1.0\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            exp_avg_diffs = []\n",
    "            neg_pre_grads = []\n",
    "\n",
    "            beta1, beta2, beta3 = group['betas']\n",
    "            # assume same step across group now to simplify things\n",
    "            # per parameter step can be easily support\n",
    "            # by making it tensor, or pass list into kernel\n",
    "            if 'step' in group:\n",
    "                group['step'] += 1\n",
    "            else:\n",
    "                group['step'] = 1\n",
    "\n",
    "            bias_correction1 = 1.0 - beta1**group['step']\n",
    "            bias_correction2 = 1.0 - beta2**group['step']\n",
    "            bias_correction3 = 1.0 - beta3**group['step']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                params_with_grad.append(p)\n",
    "                grads.append(p.grad)\n",
    "\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "                    state['exp_avg_diff'] = torch.zeros_like(p)\n",
    "\n",
    "                if 'neg_pre_grad' not in state or group['step'] == 1:\n",
    "                    state['neg_pre_grad'] = p.grad.clone().mul_(\n",
    "                        -clip_global_grad_norm)\n",
    "\n",
    "                exp_avgs.append(state['exp_avg'])\n",
    "                exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "                exp_avg_diffs.append(state['exp_avg_diff'])\n",
    "                neg_pre_grads.append(state['neg_pre_grad'])\n",
    "\n",
    "            if not params_with_grad:\n",
    "                continue\n",
    "\n",
    "            kwargs = dict(\n",
    "                params=params_with_grad,\n",
    "                grads=grads,\n",
    "                exp_avgs=exp_avgs,\n",
    "                exp_avg_sqs=exp_avg_sqs,\n",
    "                exp_avg_diffs=exp_avg_diffs,\n",
    "                neg_pre_grads=neg_pre_grads,\n",
    "                beta1=beta1,\n",
    "                beta2=beta2,\n",
    "                beta3=beta3,\n",
    "                bias_correction1=bias_correction1,\n",
    "                bias_correction2=bias_correction2,\n",
    "                bias_correction3_sqrt=math.sqrt(bias_correction3),\n",
    "                lr=group['lr'],\n",
    "                weight_decay=group['weight_decay'],\n",
    "                eps=group['eps'],\n",
    "                no_prox=group['no_prox'],\n",
    "                clip_global_grad_norm=clip_global_grad_norm,\n",
    "            )\n",
    "\n",
    "            if group['foreach']:\n",
    "                if group['fused']:\n",
    "                    if torch.cuda.is_available():\n",
    "                        _fused_adan_multi_tensor(**kwargs)\n",
    "                    else:\n",
    "                        raise ValueError('Fused Adan does not support CPU')\n",
    "                else:\n",
    "                    _multi_tensor_adan(**kwargs)\n",
    "            elif group['fused']:\n",
    "                if torch.cuda.is_available():\n",
    "                    _fused_adan_single_tensor(**kwargs)\n",
    "                else:\n",
    "                    raise ValueError('Fused Adan does not support CPU')\n",
    "            else:\n",
    "                _single_tensor_adan(**kwargs)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def _single_tensor_adan(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    for i, param in enumerate(params):\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        exp_avg_diff = exp_avg_diffs[i]\n",
    "        neg_grad_or_diff = neg_pre_grads[i]\n",
    "\n",
    "        grad.mul_(clip_global_grad_norm)\n",
    "\n",
    "        # for memory saving, we use `neg_grad_or_diff`\n",
    "        # to get some temp variable in a inplace way\n",
    "        neg_grad_or_diff.add_(grad)\n",
    "\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  # m_t\n",
    "        exp_avg_diff.mul_(beta2).add_(neg_grad_or_diff,\n",
    "                                      alpha=1 - beta2)  # diff_t\n",
    "\n",
    "        neg_grad_or_diff.mul_(beta2).add_(grad)\n",
    "        exp_avg_sq.mul_(beta3).addcmul_(neg_grad_or_diff,\n",
    "                                        neg_grad_or_diff,\n",
    "                                        value=1 - beta3)  # n_t\n",
    "\n",
    "        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)\n",
    "        step_size_diff = lr * beta2 / bias_correction2\n",
    "        step_size = lr / bias_correction1\n",
    "\n",
    "        if no_prox:\n",
    "            param.mul_(1 - lr * weight_decay)\n",
    "            param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)\n",
    "        else:\n",
    "            param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)\n",
    "            param.div_(1 + lr * weight_decay)\n",
    "\n",
    "        neg_grad_or_diff.zero_().add_(grad, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _multi_tensor_adan(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    if len(params) == 0:\n",
    "        return\n",
    "\n",
    "    torch._foreach_mul_(grads, clip_global_grad_norm)\n",
    "\n",
    "    # for memory saving, we use `neg_pre_grads`\n",
    "    # to get some temp variable in a inplace way\n",
    "    torch._foreach_add_(neg_pre_grads, grads)\n",
    "\n",
    "    torch._foreach_mul_(exp_avgs, beta1)\n",
    "    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  # m_t\n",
    "\n",
    "    torch._foreach_mul_(exp_avg_diffs, beta2)\n",
    "    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,\n",
    "                        alpha=1 - beta2)  # diff_t\n",
    "\n",
    "    torch._foreach_mul_(neg_pre_grads, beta2)\n",
    "    torch._foreach_add_(neg_pre_grads, grads)\n",
    "    torch._foreach_mul_(exp_avg_sqs, beta3)\n",
    "    torch._foreach_addcmul_(exp_avg_sqs,\n",
    "                            neg_pre_grads,\n",
    "                            neg_pre_grads,\n",
    "                            value=1 - beta3)  # n_t\n",
    "\n",
    "    denom = torch._foreach_sqrt(exp_avg_sqs)\n",
    "    torch._foreach_div_(denom, bias_correction3_sqrt)\n",
    "    torch._foreach_add_(denom, eps)\n",
    "\n",
    "    step_size_diff = lr * beta2 / bias_correction2\n",
    "    step_size = lr / bias_correction1\n",
    "\n",
    "    if no_prox:\n",
    "        torch._foreach_mul_(params, 1 - lr * weight_decay)\n",
    "        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)\n",
    "        torch._foreach_addcdiv_(params,\n",
    "                                exp_avg_diffs,\n",
    "                                denom,\n",
    "                                value=-step_size_diff)\n",
    "    else:\n",
    "        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)\n",
    "        torch._foreach_addcdiv_(params,\n",
    "                                exp_avg_diffs,\n",
    "                                denom,\n",
    "                                value=-step_size_diff)\n",
    "        torch._foreach_div_(params, 1 + lr * weight_decay)\n",
    "    torch._foreach_zero_(neg_pre_grads)\n",
    "    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _fused_adan_multi_tensor(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    import fused_adan\n",
    "    multi_tensor_applier = MultiTensorApply(2048 * 32)\n",
    "    _dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
    "    multi_tensor_applier(\n",
    "        fused_adan.adan_multi_tensor, _dummy_overflow_buf,\n",
    "        [params, grads, exp_avgs, exp_avg_sqs, exp_avg_diffs, neg_pre_grads],\n",
    "        beta1, beta2, beta3, bias_correction1, bias_correction2,\n",
    "        bias_correction3_sqrt, lr, weight_decay, eps, no_prox,\n",
    "        clip_global_grad_norm)\n",
    "    torch._foreach_zero_(neg_pre_grads)\n",
    "    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _fused_adan_single_tensor(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    for i, param in enumerate(params):\n",
    "        p_data_fp32 = param.data.float()\n",
    "        out_p = param.data\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        exp_avg_diff = exp_avg_diffs[i]\n",
    "        neg_grad = neg_pre_grads[i]\n",
    "        with torch.cuda.device(param.device):\n",
    "            import fused_adan\n",
    "            fused_adan.adan_single_tensor(\n",
    "                p_data_fp32,\n",
    "                out_p,\n",
    "                grad,\n",
    "                exp_avg,\n",
    "                exp_avg_sq,\n",
    "                exp_avg_diff,\n",
    "                neg_grad,\n",
    "                beta1,\n",
    "                beta2,\n",
    "                beta3,\n",
    "                bias_correction1,\n",
    "                bias_correction2,\n",
    "                bias_correction3_sqrt,\n",
    "                lr,\n",
    "                weight_decay,\n",
    "                eps,\n",
    "                no_prox,\n",
    "                clip_global_grad_norm,\n",
    "            )\n",
    "        neg_grad.zero_().add_(grad, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _check_fused_available():\n",
    "    try:\n",
    "        import fused_adan\n",
    "    except ImportError as exc:\n",
    "        if torch.cuda.is_available():\n",
    "            # The module should be available but isn't. Try to\n",
    "            # help the user in this case.\n",
    "            raise ImportError((\n",
    "                str(exc)\n",
    "                + (\n",
    "                    '\\nThis could be caused by not having compiled '\n",
    "                    'the CUDA extension during package installation. '\n",
    "                    'Please try to re-install the package with '\n",
    "                    'the environment flag `FORCE_CUDA=1` set.'\n",
    "                )\n",
    "            ))\n",
    "        else:\n",
    "            raise ImportError(\n",
    "                str(exc) + '\\nFused Adan does not support CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Number of cores: 16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(f\"Number of cores: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the CNN-LSTM Forcasting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MainModel(\n",
      "  (cnn): CNN1D(\n",
      "    (conv_blocks): Sequential(\n",
      "      (0): Conv1d(31, 16, kernel_size=(2,), stride=(1,))\n",
      "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "      (3): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
      "      (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): SiLU()\n",
      "      (6): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(32, 150, num_layers=3, batch_first=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=150, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 502609\n"
     ]
    }
   ],
   "source": [
    "cnn_params = [16, 32, \"MaxPool\"]\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input=31):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.input = input\n",
    "        self.conv_blocks = self.create_conv_blocks(cnn_params)\n",
    "        \n",
    "    def create_conv_blocks(self, architecture):\n",
    "        layers = []\n",
    "        input = self.input\n",
    "\n",
    "        for x in architecture:\n",
    "            if type(x) == int:\n",
    "                output = x\n",
    "                layers += [nn.Conv1d(input, output, \n",
    "                        kernel_size=2, stride=1, padding=0),\n",
    "                        nn.BatchNorm1d(x),\n",
    "                        nn.SiLU()]\n",
    "\n",
    "                input = x  # Update input channels\n",
    "            elif x == \"MaxPool\":\n",
    "                layers += [nn.MaxPool1d(kernel_size=2, stride=1)]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (batch, features, time_steps) → (batch, channels, time_steps)\n",
    "        x = self.conv_blocks(x)\n",
    "        return x  # Output shape: (batch, channels, reduced_time_steps)\n",
    "\n",
    "class MainModel(nn.Module):\n",
    "    def __init__(self, input_size=64, hidden_size=150, num_layers=3, output_size=1):\n",
    "        super(MainModel, self).__init__()\n",
    "        self.num_layers = num_layers  \n",
    "        self.hidden_size = hidden_size\n",
    "        self.cnn = CNN1D(input_size)\n",
    "        self.lstm = nn.LSTM(input_size=32, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(150, 128), \n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 2, 1)  # (batch, channels, time) → (batch, time, channels)\n",
    "        initial_hidden_state = torch.zeros(self.num_layers, x.size(0),self.hidden_size)\n",
    "        initial_cell_state = torch.zeros(self.num_layers, x.size(0),self.hidden_size)\n",
    "        # initial_hidden_state = torch.randn(self.num_layers, x.size(0), self.hidden_size) * 0.02\n",
    "        # initial_cell_state = torch.randn(self.num_layers, x.size(0), self.hidden_size) * 0.02\n",
    "        x, _ = self.lstm(x, (initial_hidden_state, initial_cell_state))\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x) \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate Model\n",
    "model = MainModel(input_size=X_train_tensor.shape[2], hidden_size=150, num_layers=3, output_size=1).to(device)\n",
    "\n",
    "print(f\"{model}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([159671, 1])\n",
      "tensor([[ 0.0052],\n",
      "        [-0.0385],\n",
      "        [ 0.0470],\n",
      "        ...,\n",
      "        [-0.0152],\n",
      "        [ 0.0131],\n",
      "        [ 0.0146]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.randn(X_train_tensor.shape[0], X_train_tensor.shape[1], X_train_tensor.shape[2])\n",
    "# Test Forward Pass\n",
    "# y = model(dummy)  # Testing with dummy training data\n",
    "y = model(X_train_tensor)\n",
    "print(f\"{y.shape}\")\n",
    "print(f\"{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train and Test functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader:torch.utils.data.DataLoader,\n",
    "               model:torch.nn.Module,\n",
    "               loss_fn:torch.nn.Module,\n",
    "               optimizer:torch.optim.Optimizer,\n",
    "               calculate_accuracy,\n",
    "               device: torch.device = device,\n",
    "               loss_steps: int = 100,\n",
    "               seed: int = 25):\n",
    "  # Set seed for reproducibility\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "  #Training\n",
    "  train_loss, train_acc = 0, 0\n",
    "  #Put Data into training Mode\n",
    "  model.train()\n",
    "  for batch, (X, y) in enumerate(data_loader):\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    train_loss += loss.item()\n",
    "    train_acc += calculate_accuracy(y_true=y,\n",
    "                             y_pred=y_pred)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  train_loss = train_loss / len(data_loader)\n",
    "  train_acc = train_acc / len(data_loader)\n",
    "  if epoch % loss_steps == 0:\n",
    "    print(f\"Training Loss: {train_loss:.5f} | Training R2: {train_acc:.4f}%\")\n",
    "  return train_loss, train_acc\n",
    "\n",
    "def test(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              calculate_accuracy,\n",
    "              device: torch.device = device,\n",
    "              loss_steps: int = 100,\n",
    "              seed: int = 42):\n",
    "  # Set seed for reproducibility\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "  #Testing\n",
    "  test_loss, test_acc = 0, 0\n",
    "  model.to(device)\n",
    "  #Put Data into evaluation Mode\n",
    "  model.eval()\n",
    "  with torch.inference_mode():\n",
    "    for X, y in data_loader:\n",
    "      X,y = X.to(device), y.to(device)\n",
    "      test_pred = model(X)\n",
    "      loss = loss_fn(test_pred, y)\n",
    "      test_loss += loss.item()\n",
    "      test_acc += calculate_accuracy(y_true=y, y_pred=test_pred)\n",
    "\n",
    "    test_loss /= len(data_loader)\n",
    "    test_acc /= len(data_loader)\n",
    "    if epoch % loss_steps == 0:\n",
    "      print(f\"Test Loss {test_loss:.5f} | Test R2 {test_acc:5f}%\")\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parameters, objective function, and r2 metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes an accuracy-like metric for regression using R^2 Score.\n",
    "    \"\"\"\n",
    "    y_true = y_true.detach().cpu().numpy()\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)  # Best: 1, Worst: -∞\n",
    "    return r2  # Treat as \"accuracy\" for regression\n",
    "\n",
    "## Setup loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_of_epochs = 300\n",
    "num_loss_steps = 10\n",
    "seed_number = 25\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75776f3700d42748666ac69f25311b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      " =====================================================================\n",
      "Training Loss: 6.18523 | Training R2: -4.0690%\n",
      "Test Loss 1.26995 | Test R2 -0.037331%\n",
      "Epoch: 10 \n",
      " =====================================================================\n",
      "Training Loss: 1.77918 | Training R2: -0.4398%\n",
      "Test Loss 1.25169 | Test R2 -0.021805%\n",
      "Epoch: 20 \n",
      " =====================================================================\n",
      "Training Loss: 1.61941 | Training R2: -0.3089%\n",
      "Test Loss 1.24917 | Test R2 -0.019654%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m num_loss_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m =====================================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m train_loss, train_accu \u001b[38;5;241m=\u001b[39m train(data_loader \u001b[38;5;241m=\u001b[39m train_dataloader,\n\u001b[1;32m     26\u001b[0m                                 model \u001b[38;5;241m=\u001b[39m model_Adan,\n\u001b[1;32m     27\u001b[0m                                 loss_fn \u001b[38;5;241m=\u001b[39m loss_fn,\n\u001b[1;32m     28\u001b[0m                                 optimizer \u001b[38;5;241m=\u001b[39m optimizer_Adan,\n\u001b[1;32m     29\u001b[0m                                 calculate_accuracy \u001b[38;5;241m=\u001b[39m calculate_accuracy,\n\u001b[1;32m     30\u001b[0m                                 device \u001b[38;5;241m=\u001b[39m device,\n\u001b[1;32m     31\u001b[0m                                 loss_steps \u001b[38;5;241m=\u001b[39m num_loss_steps,\n\u001b[1;32m     32\u001b[0m                                 seed \u001b[38;5;241m=\u001b[39m seed_number);\n\u001b[1;32m     34\u001b[0m test_loss, test_accu \u001b[38;5;241m=\u001b[39m test(data_loader \u001b[38;5;241m=\u001b[39m test_dataloader,\n\u001b[1;32m     35\u001b[0m                             model \u001b[38;5;241m=\u001b[39m model_Adan,\n\u001b[1;32m     36\u001b[0m                             loss_fn \u001b[38;5;241m=\u001b[39m loss_fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m                             loss_steps \u001b[38;5;241m=\u001b[39m num_loss_steps,\n\u001b[1;32m     40\u001b[0m                             seed\u001b[38;5;241m=\u001b[39mseed_number);\n\u001b[1;32m     42\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[145], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data_loader, model, loss_fn, optimizer, calculate_accuracy, device, loss_steps, seed)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m     19\u001b[0m   X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 20\u001b[0m   y_pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     21\u001b[0m   loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n\u001b[1;32m     22\u001b[0m   train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[143], line 55\u001b[0m, in \u001b[0;36mMainModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m initial_cell_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# initial_hidden_state = torch.randn(self.num_layers, x.size(0), self.hidden_size) * 0.02\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# initial_cell_state = torch.randn(self.num_layers, x.size(0), self.hidden_size) * 0.02\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x, (initial_hidden_state, initial_cell_state))\n\u001b[1;32m     56\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x) \n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1125\u001b[0m         hx,\n\u001b[1;32m   1126\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights,\n\u001b[1;32m   1127\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1128\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[1;32m   1133\u001b[0m     )\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1137\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1145\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(seed_number)\n",
    "torch.manual_seed(seed_number)\n",
    "torch.cuda.manual_seed_all(seed_number)\n",
    "\n",
    "model_Adan = MainModel(input_size=X_train_tensor.shape[2], \n",
    "                       hidden_size=150, num_layers=3, output_size=1).to(device)\n",
    "\n",
    "optimizer_Adan = Adan(params=model_Adan.parameters(), \n",
    "                      lr=learning_rate, \n",
    "                      betas=(0.94, 0.91, 0.95), \n",
    "                      weight_decay=0.01)\n",
    "\n",
    "results = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accu\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"test_accu\": []\n",
    "}\n",
    "\n",
    "start_adan = time.perf_counter()\n",
    "for epoch in tqdm(range(0, num_of_epochs)):\n",
    "  if epoch % num_loss_steps == 0:\n",
    "    print(f\"Epoch: {epoch} \\n =====================================================================\")\n",
    "\n",
    "  train_loss, train_accu = train(data_loader = train_dataloader,\n",
    "                                  model = model_Adan,\n",
    "                                  loss_fn = loss_fn,\n",
    "                                  optimizer = optimizer_Adan,\n",
    "                                  calculate_accuracy = calculate_accuracy,\n",
    "                                  device = device,\n",
    "                                  loss_steps = num_loss_steps,\n",
    "                                  seed = seed_number);\n",
    "\n",
    "  test_loss, test_accu = test(data_loader = test_dataloader,\n",
    "                              model = model_Adan,\n",
    "                              loss_fn = loss_fn,\n",
    "                              calculate_accuracy = calculate_accuracy,\n",
    "                              device = device,\n",
    "                              loss_steps = num_loss_steps,\n",
    "                              seed=seed_number);\n",
    "\n",
    "  results[\"train_loss\"].append(train_loss)\n",
    "  results[\"train_accu\"].append(train_accu)\n",
    "  results[\"test_loss\"].append(test_loss)\n",
    "  results[\"test_accu\"].append(test_accu)\n",
    "  \n",
    "end_adan = time.perf_counter()\n",
    "total_time_adan = end_adan - start_adan\n",
    "print(f\"Total Runtime: {total_time_adan / 60: .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3c9fe336b0>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(results[\"train_loss\"])\n",
    "plt.plot(results[\"test_loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
