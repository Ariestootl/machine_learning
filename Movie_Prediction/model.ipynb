{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: Index(['tconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult',\n",
      "       'startYear', 'runtimeMinutes', 'genres', 'averageRating', 'numVotes',\n",
      "       'HitScore'],\n",
      "      dtype='object')\n",
      "Shape: (199595, 11)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"movies_new.csv\")\n",
    "df = df.sort_values(by=\"startYear\")\n",
    "print(f\"Features: {df.columns}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "df['genres_list'] = df['genres'].apply(lambda x: x.split(','))\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_encoded = mlb.fit_transform(df['genres_list'])  # Fit & transform genres\n",
    "\n",
    "genre_columns = mlb.classes_  # Get genre names as column names\n",
    "df[genre_columns] = genre_encoded  # Add encoded genres to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "df['isAdult'] = df['isAdult'].astype(int)\n",
    "df['numVotes_log'] = np.log1p(df['numVotes'])\n",
    "numeric_columns = ['averageRating', 'HitScore', 'numVotes', 'numVotes_log', 'runtimeMinutes', 'startYear']\n",
    "df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "df.drop(columns=[\"originalTitle\"], inplace=True)\n",
    "\n",
    "# Define features and target\n",
    "X= df[['averageRating', 'HitScore', 'numVotes_log', 'runtimeMinutes']+ list(genre_columns)].to_numpy()\n",
    "y = df['HitScore'].to_numpy().reshape(-1,1)\n",
    "\n",
    "# Split data into training (80%) and temp (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split temp (20%) into validation (10%) and test (10%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"{X_train.shape[0] + X_val.shape[0] + X_test.shape[0] == df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerX = StandardScaler()\n",
    "X_train_scaled = scalerX.fit_transform(X_train)  # Fit on training set\n",
    "X_test_scaled = scalerX.transform(X_test)\n",
    "X_val_scaled = scalerX.transform(X_val)  # Apply on test set\n",
    "\n",
    "\n",
    "scalerY = StandardScaler()\n",
    "y_train_scaled = scalerY.fit_transform(y_train.reshape(-1, 1))  # Fit on training set\n",
    "y_test_scaled = scalerY.transform(y_test.reshape(-1, 1))  # Apply on test set\n",
    "y_val_scaled = scalerY.transform(y_val.reshape(-1, 1))  # Apply on validation test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame as a Time Series problem\n",
    "* Convert the data into a sequential type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (159671, 5, 31)\n",
      "Val shape: (19954, 5, 31)\n",
      "Test shape: (19955, 5, 31)\n"
     ]
    }
   ],
   "source": [
    "# Define time step (e.g., 5 years)\n",
    "TIME_STEP = 5  \n",
    "\n",
    "def create_sequences(X, y, time_step):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_step):\n",
    "        Xs.append(X[i:i+time_step])  # Past 5 years\n",
    "        ys.append(y[i+time_step])    # Target is the next year\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Convert datasets into 3D shape (samples, time steps, features)\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, TIME_STEP)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val, TIME_STEP)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, TIME_STEP)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Train shape:\", X_train_seq.shape)  # (samples, time_steps, features)\n",
    "print(\"Val shape:\", X_val_seq.shape)\n",
    "print(\"Test shape:\", X_test_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([159671, 5, 31]) torch.Size([159671, 1])\n",
      "torch.Size([19954, 5, 31]) torch.Size([19954, 1])\n",
      "torch.Size([19955, 5, 31]) torch.Size([19955, 1])\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_seq, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_seq, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Check shapes\n",
    "print(X_train_tensor.shape, y_train_tensor.shape)\n",
    "print(X_val_tensor.shape, y_val_tensor.shape)\n",
    "print(X_test_tensor.shape, y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Dataloaders from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders:\n",
      "Train dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7be0654c3320>\n",
      "Test dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7be06a686d20>\n",
      "Validation dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7be06a686b70>\n",
      "Length of train dataloader: 4990 batches of 32\n",
      "Length of validation dataloader: 624 batches of 32\n",
      "Length of test dataloader: 624 batches of 32\n",
      "========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  # Adjust batch size as needed\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) \n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Dataloaders:\\nTrain dataloader: {train_dataloader}\\nTest dataloader: {test_dataloader}\\nValidation dataloader: {val_dataloader}\")\n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {train_dataloader.batch_size}\")\n",
    "print(f\"Length of validation dataloader: {len(val_dataloader)} batches of {val_dataloader.batch_size}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {test_dataloader.batch_size}\")\n",
    "print(f\"==============================================================================\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adan: Adaptive Nesterov Momentum Algorithm\n",
    "\n",
    "import math\n",
    "from typing import List\n",
    "from torch import Tensor\n",
    "\n",
    "class MultiTensorApply(object):\n",
    "    available = False\n",
    "    warned = False\n",
    "\n",
    "    def __init__(self, chunk_size):\n",
    "        try:\n",
    "            MultiTensorApply.available = True\n",
    "            self.chunk_size = chunk_size\n",
    "        except ImportError as err:\n",
    "            MultiTensorApply.available = False\n",
    "            MultiTensorApply.import_err = err\n",
    "\n",
    "    def __call__(self, op, noop_flag_buffer, tensor_lists, *args):\n",
    "        return op(self.chunk_size, noop_flag_buffer, tensor_lists, *args)\n",
    "\n",
    "\n",
    "class Adan(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Implements a pytorch variant of Adan\n",
    "    Adan was proposed in\n",
    "    Adan: Adaptive Nesterov Momentum Algorithm for\n",
    "        Faster Optimizing Deep Models[J].arXiv preprint arXiv:2208.06677, 2022.\n",
    "    https://arxiv.org/abs/2208.06677\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or\n",
    "            dicts defining parameter groups.\n",
    "        lr (float, optional): learning rate. (default: 1e-3)\n",
    "        betas (Tuple[float, float, flot], optional): coefficients used for\n",
    "            first- and second-order moments. (default: (0.98, 0.92, 0.99))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability. (default: 1e-8)\n",
    "        weight_decay (float, optional): decoupled weight decay\n",
    "            (L2 penalty) (default: 0)\n",
    "        max_grad_norm (float, optional): value used to clip\n",
    "            global grad norm (default: 0.0 no clip)\n",
    "        no_prox (bool): how to perform the decoupled weight decay\n",
    "            (default: False)\n",
    "        foreach (bool): if True would use torch._foreach implementation.\n",
    "            It's faster but uses slightly more memory. (default: True)\n",
    "        fused (bool, optional): whether fused implementation is used.\n",
    "            (default: False)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-3,\n",
    "                 betas=(0.98, 0.92, 0.99),\n",
    "                 eps=1e-8,\n",
    "                 weight_decay=0.0,\n",
    "                 max_grad_norm=0.0,\n",
    "                 no_prox=False,\n",
    "                 foreach: bool = True,\n",
    "                 fused: bool = False):\n",
    "        if not 0.0 <= max_grad_norm:\n",
    "            raise ValueError('Invalid Max grad norm: {}'.format(max_grad_norm))\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError('Invalid learning rate: {}'.format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError('Invalid epsilon value: {}'.format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 0: {}'.format(\n",
    "                betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 1: {}'.format(\n",
    "                betas[1]))\n",
    "        if not 0.0 <= betas[2] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 2: {}'.format(\n",
    "                betas[2]))\n",
    "        if fused:\n",
    "            _check_fused_available()\n",
    "\n",
    "        defaults = dict(lr=lr,\n",
    "                        betas=betas,\n",
    "                        eps=eps,\n",
    "                        weight_decay=weight_decay,\n",
    "                        max_grad_norm=max_grad_norm,\n",
    "                        no_prox=no_prox,\n",
    "                        foreach=foreach,\n",
    "                        fused=fused)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adan, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('no_prox', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restart_opt(self):\n",
    "        for group in self.param_groups:\n",
    "            group['step'] = 0\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    state = self.state[p]\n",
    "                    # State initialization\n",
    "\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of gradient difference\n",
    "                    state['exp_avg_diff'] = torch.zeros_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\"\"\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        if self.defaults['max_grad_norm'] > 0:\n",
    "            device = self.param_groups[0]['params'][0].device\n",
    "            global_grad_norm = torch.zeros(1, device=device)\n",
    "\n",
    "            max_grad_norm = torch.tensor(self.defaults['max_grad_norm'],\n",
    "                                         device=device)\n",
    "            for group in self.param_groups:\n",
    "\n",
    "                for p in group['params']:\n",
    "                    if p.grad is not None:\n",
    "                        grad = p.grad\n",
    "                        global_grad_norm.add_(grad.pow(2).sum())\n",
    "\n",
    "            global_grad_norm = torch.sqrt(global_grad_norm)\n",
    "\n",
    "            clip_global_grad_norm = torch.clamp(\n",
    "                max_grad_norm / (global_grad_norm + group['eps']),\n",
    "                max=1.0).item()\n",
    "        else:\n",
    "            clip_global_grad_norm = 1.0\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            exp_avg_diffs = []\n",
    "            neg_pre_grads = []\n",
    "\n",
    "            beta1, beta2, beta3 = group['betas']\n",
    "            # assume same step across group now to simplify things\n",
    "            # per parameter step can be easily support\n",
    "            # by making it tensor, or pass list into kernel\n",
    "            if 'step' in group:\n",
    "                group['step'] += 1\n",
    "            else:\n",
    "                group['step'] = 1\n",
    "\n",
    "            bias_correction1 = 1.0 - beta1**group['step']\n",
    "            bias_correction2 = 1.0 - beta2**group['step']\n",
    "            bias_correction3 = 1.0 - beta3**group['step']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                params_with_grad.append(p)\n",
    "                grads.append(p.grad)\n",
    "\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "                    state['exp_avg_diff'] = torch.zeros_like(p)\n",
    "\n",
    "                if 'neg_pre_grad' not in state or group['step'] == 1:\n",
    "                    state['neg_pre_grad'] = p.grad.clone().mul_(\n",
    "                        -clip_global_grad_norm)\n",
    "\n",
    "                exp_avgs.append(state['exp_avg'])\n",
    "                exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "                exp_avg_diffs.append(state['exp_avg_diff'])\n",
    "                neg_pre_grads.append(state['neg_pre_grad'])\n",
    "\n",
    "            if not params_with_grad:\n",
    "                continue\n",
    "\n",
    "            kwargs = dict(\n",
    "                params=params_with_grad,\n",
    "                grads=grads,\n",
    "                exp_avgs=exp_avgs,\n",
    "                exp_avg_sqs=exp_avg_sqs,\n",
    "                exp_avg_diffs=exp_avg_diffs,\n",
    "                neg_pre_grads=neg_pre_grads,\n",
    "                beta1=beta1,\n",
    "                beta2=beta2,\n",
    "                beta3=beta3,\n",
    "                bias_correction1=bias_correction1,\n",
    "                bias_correction2=bias_correction2,\n",
    "                bias_correction3_sqrt=math.sqrt(bias_correction3),\n",
    "                lr=group['lr'],\n",
    "                weight_decay=group['weight_decay'],\n",
    "                eps=group['eps'],\n",
    "                no_prox=group['no_prox'],\n",
    "                clip_global_grad_norm=clip_global_grad_norm,\n",
    "            )\n",
    "\n",
    "            if group['foreach']:\n",
    "                if group['fused']:\n",
    "                    if torch.cuda.is_available():\n",
    "                        _fused_adan_multi_tensor(**kwargs)\n",
    "                    else:\n",
    "                        raise ValueError('Fused Adan does not support CPU')\n",
    "                else:\n",
    "                    _multi_tensor_adan(**kwargs)\n",
    "            elif group['fused']:\n",
    "                if torch.cuda.is_available():\n",
    "                    _fused_adan_single_tensor(**kwargs)\n",
    "                else:\n",
    "                    raise ValueError('Fused Adan does not support CPU')\n",
    "            else:\n",
    "                _single_tensor_adan(**kwargs)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def _single_tensor_adan(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    for i, param in enumerate(params):\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        exp_avg_diff = exp_avg_diffs[i]\n",
    "        neg_grad_or_diff = neg_pre_grads[i]\n",
    "\n",
    "        grad.mul_(clip_global_grad_norm)\n",
    "\n",
    "        # for memory saving, we use `neg_grad_or_diff`\n",
    "        # to get some temp variable in a inplace way\n",
    "        neg_grad_or_diff.add_(grad)\n",
    "\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  # m_t\n",
    "        exp_avg_diff.mul_(beta2).add_(neg_grad_or_diff,\n",
    "                                      alpha=1 - beta2)  # diff_t\n",
    "\n",
    "        neg_grad_or_diff.mul_(beta2).add_(grad)\n",
    "        exp_avg_sq.mul_(beta3).addcmul_(neg_grad_or_diff,\n",
    "                                        neg_grad_or_diff,\n",
    "                                        value=1 - beta3)  # n_t\n",
    "\n",
    "        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)\n",
    "        step_size_diff = lr * beta2 / bias_correction2\n",
    "        step_size = lr / bias_correction1\n",
    "\n",
    "        if no_prox:\n",
    "            param.mul_(1 - lr * weight_decay)\n",
    "            param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)\n",
    "        else:\n",
    "            param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)\n",
    "            param.div_(1 + lr * weight_decay)\n",
    "\n",
    "        neg_grad_or_diff.zero_().add_(grad, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _multi_tensor_adan(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    if len(params) == 0:\n",
    "        return\n",
    "\n",
    "    torch._foreach_mul_(grads, clip_global_grad_norm)\n",
    "\n",
    "    # for memory saving, we use `neg_pre_grads`\n",
    "    # to get some temp variable in a inplace way\n",
    "    torch._foreach_add_(neg_pre_grads, grads)\n",
    "\n",
    "    torch._foreach_mul_(exp_avgs, beta1)\n",
    "    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  # m_t\n",
    "\n",
    "    torch._foreach_mul_(exp_avg_diffs, beta2)\n",
    "    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,\n",
    "                        alpha=1 - beta2)  # diff_t\n",
    "\n",
    "    torch._foreach_mul_(neg_pre_grads, beta2)\n",
    "    torch._foreach_add_(neg_pre_grads, grads)\n",
    "    torch._foreach_mul_(exp_avg_sqs, beta3)\n",
    "    torch._foreach_addcmul_(exp_avg_sqs,\n",
    "                            neg_pre_grads,\n",
    "                            neg_pre_grads,\n",
    "                            value=1 - beta3)  # n_t\n",
    "\n",
    "    denom = torch._foreach_sqrt(exp_avg_sqs)\n",
    "    torch._foreach_div_(denom, bias_correction3_sqrt)\n",
    "    torch._foreach_add_(denom, eps)\n",
    "\n",
    "    step_size_diff = lr * beta2 / bias_correction2\n",
    "    step_size = lr / bias_correction1\n",
    "\n",
    "    if no_prox:\n",
    "        torch._foreach_mul_(params, 1 - lr * weight_decay)\n",
    "        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)\n",
    "        torch._foreach_addcdiv_(params,\n",
    "                                exp_avg_diffs,\n",
    "                                denom,\n",
    "                                value=-step_size_diff)\n",
    "    else:\n",
    "        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)\n",
    "        torch._foreach_addcdiv_(params,\n",
    "                                exp_avg_diffs,\n",
    "                                denom,\n",
    "                                value=-step_size_diff)\n",
    "        torch._foreach_div_(params, 1 + lr * weight_decay)\n",
    "    torch._foreach_zero_(neg_pre_grads)\n",
    "    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _fused_adan_multi_tensor(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    import fused_adan\n",
    "    multi_tensor_applier = MultiTensorApply(2048 * 32)\n",
    "    _dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
    "    multi_tensor_applier(\n",
    "        fused_adan.adan_multi_tensor, _dummy_overflow_buf,\n",
    "        [params, grads, exp_avgs, exp_avg_sqs, exp_avg_diffs, neg_pre_grads],\n",
    "        beta1, beta2, beta3, bias_correction1, bias_correction2,\n",
    "        bias_correction3_sqrt, lr, weight_decay, eps, no_prox,\n",
    "        clip_global_grad_norm)\n",
    "    torch._foreach_zero_(neg_pre_grads)\n",
    "    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _fused_adan_single_tensor(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    for i, param in enumerate(params):\n",
    "        p_data_fp32 = param.data.float()\n",
    "        out_p = param.data\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        exp_avg_diff = exp_avg_diffs[i]\n",
    "        neg_grad = neg_pre_grads[i]\n",
    "        with torch.cuda.device(param.device):\n",
    "            import fused_adan\n",
    "            fused_adan.adan_single_tensor(\n",
    "                p_data_fp32,\n",
    "                out_p,\n",
    "                grad,\n",
    "                exp_avg,\n",
    "                exp_avg_sq,\n",
    "                exp_avg_diff,\n",
    "                neg_grad,\n",
    "                beta1,\n",
    "                beta2,\n",
    "                beta3,\n",
    "                bias_correction1,\n",
    "                bias_correction2,\n",
    "                bias_correction3_sqrt,\n",
    "                lr,\n",
    "                weight_decay,\n",
    "                eps,\n",
    "                no_prox,\n",
    "                clip_global_grad_norm,\n",
    "            )\n",
    "        neg_grad.zero_().add_(grad, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _check_fused_available():\n",
    "    try:\n",
    "        import fused_adan\n",
    "    except ImportError as exc:\n",
    "        if torch.cuda.is_available():\n",
    "            # The module should be available but isn't. Try to\n",
    "            # help the user in this case.\n",
    "            raise ImportError((\n",
    "                str(exc)\n",
    "                + (\n",
    "                    '\\nThis could be caused by not having compiled '\n",
    "                    'the CUDA extension during package installation. '\n",
    "                    'Please try to re-install the package with '\n",
    "                    'the environment flag `FORCE_CUDA=1` set.'\n",
    "                )\n",
    "            ))\n",
    "        else:\n",
    "            raise ImportError(\n",
    "                str(exc) + '\\nFused Adan does not support CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Sat Mar  8 00:18:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2050        Off |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   53C    P0              9W /   55W |     727MiB /   4096MiB |     15%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1762      G   /usr/lib/xorg/Xorg                            236MiB |\n",
      "|    0   N/A  N/A      1912      G   /usr/bin/gnome-shell                           52MiB |\n",
      "|    0   N/A  N/A      6220      G   ...irefox/5836/usr/lib/firefox/firefox        161MiB |\n",
      "|    0   N/A  N/A     35218      G   ...seed-version=20250306-050117.231000         73MiB |\n",
      "|    0   N/A  N/A     39926      G   ...erProcess --variations-seed-version        154MiB |\n",
      "|    0   N/A  N/A     42576      G   /usr/bin/anydesk                               19MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(f\"Number of cores: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the CNN-LSTM Forcasting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MainModel(\n",
      "  (cnn): CNN1D(\n",
      "    (conv_blocks): Sequential(\n",
      "      (0): Conv1d(31, 32, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "      (1): ReLU()\n",
      "      (2): Conv1d(32, 64, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "      (3): ReLU()\n",
      "      (4): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "      (6): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(64, 64, num_layers=3, batch_first=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 118497\n"
     ]
    }
   ],
   "source": [
    "cnn_params = [32, 64, \"MaxPool\", 64]\n",
    "device = \"cpu\"\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_channels=31):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.conv_blocks = self.create_conv_blocks(cnn_params)\n",
    "        \n",
    "    def create_conv_blocks(self, architecture):\n",
    "        layers = []\n",
    "        input_channels = self.input_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if isinstance(x, int):  # Conv layer\n",
    "                layers.append(nn.Conv1d(in_channels=input_channels, out_channels=x, kernel_size=2, stride=1, padding=1))\n",
    "                layers.append(nn.ReLU())\n",
    "                input_channels = x  # Update input channels\n",
    "            elif x == \"MaxPool\":\n",
    "                layers.append(nn.MaxPool1d(kernel_size=2, stride=1))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (batch, features, time_steps) → (batch, channels, time_steps)\n",
    "        x = self.conv_blocks(x)\n",
    "        return x  # Output shape: (batch, channels, reduced_time_steps)\n",
    "\n",
    "class MainModel(nn.Module):\n",
    "    def __init__(self, input_size=31, hidden_size=64, num_layers=3, output_size=1):\n",
    "        super(MainModel, self).__init__()\n",
    "        self.num_layers = num_layers  \n",
    "        self.hidden_size = hidden_size\n",
    "        self.cnn = CNN1D(input_size)\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),   \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 2, 1)  # (batch, channels, time) → (batch, time, channels)\n",
    "        initial_hidden_state = torch.zeros(self.num_layers, x.size(0),self.hidden_size)\n",
    "        initial_cell_state = torch.zeros(self.num_layers, x.size(0),self.hidden_size)\n",
    "        x, _ = self.lstm(x, (initial_hidden_state, initial_cell_state))\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x) \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate Model\n",
    "model = MainModel(input_size=X_train_tensor.shape[2], hidden_size=64, num_layers=3, output_size=1).to(device)\n",
    "\n",
    "print(f\"{model}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([159671, 1])\n",
      "tensor([[-0.0490],\n",
      "        [-0.0676],\n",
      "        [-0.0803],\n",
      "        ...,\n",
      "        [-0.0687],\n",
      "        [-0.0242],\n",
      "        [-0.0984]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.randn(X_train_tensor.shape[0], X_train_tensor.shape[1], X_train_tensor.shape[2])\n",
    "# Test Forward Pass\n",
    "y = model(dummy)  # Testing with dummy training data\n",
    "print(f\"{y.shape}\")\n",
    "print(f\"{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train and Test functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader:torch.utils.data.DataLoader,\n",
    "               model:torch.nn.Module,\n",
    "               loss_fn:torch.nn.Module,\n",
    "               optimizer:torch.optim.Optimizer,\n",
    "               calculate_accuracy,\n",
    "               device: torch.device = device,\n",
    "               loss_steps: int = 100,\n",
    "               seed: int = 25):\n",
    "  # Set seed for reproducibility\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "  #Training\n",
    "  train_loss, train_acc = 0, 0\n",
    "  #Put Data into training Mode\n",
    "  model.train()\n",
    "  for batch, (X, y) in enumerate(data_loader):\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    train_loss += loss.item()\n",
    "    train_acc += calculate_accuracy(y_true=y,\n",
    "                             y_pred=y_pred)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  train_loss = train_loss / len(data_loader)\n",
    "  train_acc = train_acc / len(data_loader)\n",
    "  if epoch % loss_steps == 0:\n",
    "    print(f\"Training Loss: {train_loss:.5f} | Training R2: {train_acc:.4f}%\")\n",
    "  return train_loss, train_acc\n",
    "\n",
    "def test(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              calculate_accuracy,\n",
    "              device: torch.device = device,\n",
    "              loss_steps: int = 100,\n",
    "              seed: int = 42):\n",
    "  # Set seed for reproducibility\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "  #Testing\n",
    "  test_loss, test_acc = 0, 0\n",
    "  model.to(device)\n",
    "  #Put Data into evaluation Mode\n",
    "  model.eval()\n",
    "  with torch.inference_mode():\n",
    "    for X, y in data_loader:\n",
    "      X,y = X.to(device), y.to(device)\n",
    "      test_pred = model(X)\n",
    "      loss = loss_fn(test_pred, y)\n",
    "      test_loss += loss.item()\n",
    "      test_acc += calculate_accuracy(y_true=y, y_pred=test_pred)\n",
    "\n",
    "    test_loss /= len(data_loader)\n",
    "    test_acc /= len(data_loader)\n",
    "    if epoch % loss_steps == 0:\n",
    "      print(f\"Test Loss {test_loss:.5f} | Test R2 {test_acc:5f}%\")\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parameters, objective function, and r2 metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes an accuracy-like metric for regression using R^2 Score.\n",
    "    \"\"\"\n",
    "    y_true = y_true.detach().cpu().numpy()\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)  # Best: 1, Worst: -∞\n",
    "    return r2  # Treat as \"accuracy\" for regression\n",
    "\n",
    "## Setup loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_of_epochs = 50\n",
    "num_loss_steps = 5\n",
    "seed_number = 42\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "model_Adan = MainModel(input_size=X_train_tensor.shape[2], \n",
    "                       hidden_size=64, num_layers=3, output_size=1).to(device)\n",
    "\n",
    "optimizer_Adan = Adan(params=model_Adan.parameters(), \n",
    "                      lr=0.001, \n",
    "                      betas=(0.95, 0.92, 0.99), \n",
    "                      weight_decay=1e-2)\n",
    "\n",
    "results = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accu\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"test_accu\": []\n",
    "}\n",
    "\n",
    "start_adan = time.perf_counter()\n",
    "for epoch in tqdm(range(0, num_of_epochs)):\n",
    "  if epoch % num_loss_steps == 0:\n",
    "    print(f\"Epoch: {epoch} \\n =====================================================================\")\n",
    "\n",
    "  train_loss, train_accu = train(data_loader = train_dataloader,\n",
    "                                  model = model_Adan,\n",
    "                                  loss_fn = loss_fn,\n",
    "                                  optimizer = optimizer_Adan,\n",
    "                                  calculate_accuracy = calculate_accuracy,\n",
    "                                  device = device,\n",
    "                                  loss_steps = num_loss_steps,\n",
    "                                  seed = seed);\n",
    "\n",
    "  test_loss, test_accu = test(data_loader = test_dataloader,\n",
    "                              model = model_Adan,\n",
    "                              loss_fn = loss_fn,\n",
    "                              calculate_accuracy = calculate_accuracy,\n",
    "                              device = device,\n",
    "                              loss_steps = num_loss_steps,\n",
    "                              seed=seed);\n",
    "\n",
    "  results[\"train_loss\"].append(train_loss)\n",
    "  results[\"train_accu\"].append(train_accu)\n",
    "  results[\"test_loss\"].append(test_loss)\n",
    "  results[\"test_accu\"].append(test_accu)\n",
    "  \n",
    "end_adan = time.perf_counter()\n",
    "total_time_adan = end_adan - start_adan\n",
    "print(f\"Total Runtime: {total_time_adan / 60: .2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
