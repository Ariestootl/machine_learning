{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: Index(['tconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult',\n",
      "       'startYear', 'runtimeMinutes', 'genres', 'averageRating', 'numVotes',\n",
      "       'HitScore'],\n",
      "      dtype='object')\n",
      "Shape: (199595, 11)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"movies_new.csv\")\n",
    "df = df.sort_values(by=\"startYear\")\n",
    "print(f\"Features: {df.columns}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "df['genres_list'] = df['genres'].apply(lambda x: x.split(','))\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_encoded = mlb.fit_transform(df['genres_list'])  # Fit & transform genres\n",
    "\n",
    "genre_columns = mlb.classes_  # Get genre names as column names\n",
    "df[genre_columns] = genre_encoded  # Add encoded genres to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1056698/998830184.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['runtimeMinutes'].fillna(df['runtimeMinutes'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df['isAdult'] = df['isAdult'].astype(int)\n",
    "# df['runtimeMinutes'] = pd.to_numeric(df['runtimeMinutes'], errors='coerce').fillna(df['runtimeMinutes'].median())\n",
    "# df['numVotes'] = pd.to_numeric(df['numVotes'], errors='coerce').fillna(1)  \n",
    "df['numVotes_log'] = np.log1p(df['numVotes'])\n",
    "\n",
    "df['numVotes_log'] = np.log1p(df['numVotes'])\n",
    "numeric_columns = ['averageRating', 'HitScore', 'numVotes', 'numVotes_log', 'runtimeMinutes', 'startYear']\n",
    "df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "df.loc[:, 'runtimeMinutes'] = df['runtimeMinutes'].fillna(df['runtimeMinutes'].median())\n",
    "df.drop(columns=[\"originalTitle\"], inplace=True)\n",
    "\n",
    "# Define features and target\n",
    "X= df[['averageRating', 'HitScore', 'numVotes_log', 'runtimeMinutes']+ list(genre_columns)].to_numpy()\n",
    "y = df['HitScore'].to_numpy().reshape(-1,1)\n",
    "\n",
    "# Split data into training (80%) and temp (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split temp (20%) into validation (10%) and test (10%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"{X_train.shape[0] + X_val.shape[0] + X_test.shape[0] == df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerX = StandardScaler()\n",
    "X_train_scaled = scalerX.fit_transform(X_train)  # Fit on training set\n",
    "X_test_scaled = scalerX.transform(X_test)\n",
    "X_val_scaled = scalerX.transform(X_val)  # Apply on test set\n",
    "\n",
    "\n",
    "scalerY = StandardScaler()\n",
    "y_train_scaled = scalerY.fit_transform(y_train.reshape(-1, 1))  # Fit on training set\n",
    "y_test_scaled = scalerY.transform(y_test.reshape(-1, 1))  # Apply on test set\n",
    "y_val_scaled = scalerY.transform(y_val.reshape(-1, 1))  # Apply on validation test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame as a Time Series problem\n",
    "* Convert the data into a sequential type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (159671, 5, 31)\n",
      "Val shape: (19954, 5, 31)\n",
      "Test shape: (19955, 5, 31)\n"
     ]
    }
   ],
   "source": [
    "# Define time step (e.g., 5 years)\n",
    "TIME_STEP = 5  \n",
    "\n",
    "def create_sequences(X, y, time_step):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_step):\n",
    "        Xs.append(X[i:i+time_step])  # Past 5 years\n",
    "        ys.append(y[i+time_step])    # Target is the next year\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Convert datasets into 3D shape (samples, time steps, features)\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, TIME_STEP)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val, TIME_STEP)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, TIME_STEP)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Train shape:\", X_train_seq.shape)  # (samples, time_steps, features)\n",
    "print(\"Val shape:\", X_val_seq.shape)\n",
    "print(\"Test shape:\", X_test_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([159671, 5, 31]) torch.Size([159671, 1])\n",
      "torch.Size([19954, 5, 31]) torch.Size([19954, 1])\n",
      "torch.Size([19955, 5, 31]) torch.Size([19955, 1])\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_seq, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_seq, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Check shapes\n",
    "print(X_train_tensor.shape, y_train_tensor.shape)\n",
    "print(X_val_tensor.shape, y_val_tensor.shape)\n",
    "print(X_test_tensor.shape, y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Dataloaders from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders:\n",
      "Train dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fe4c617b020>\n",
      "Test dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fe4cb5a9af0>\n",
      "Validation dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fe4cbe3e9f0>\n",
      "Length of train dataloader: 4990 batches of 32\n",
      "Length of validation dataloader: 624 batches of 32\n",
      "Length of test dataloader: 624 batches of 32\n",
      "========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  # Adjust batch size as needed\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) \n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Dataloaders:\\nTrain dataloader: {train_dataloader}\\nTest dataloader: {test_dataloader}\\nValidation dataloader: {val_dataloader}\")\n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {train_dataloader.batch_size}\")\n",
    "print(f\"Length of validation dataloader: {len(val_dataloader)} batches of {val_dataloader.batch_size}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {test_dataloader.batch_size}\")\n",
    "print(f\"==============================================================================\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adan: Adaptive Nesterov Momentum Algorithm\n",
    "\n",
    "import math\n",
    "from typing import List\n",
    "from torch import Tensor\n",
    "\n",
    "class MultiTensorApply(object):\n",
    "    available = False\n",
    "    warned = False\n",
    "\n",
    "    def __init__(self, chunk_size):\n",
    "        try:\n",
    "            MultiTensorApply.available = True\n",
    "            self.chunk_size = chunk_size\n",
    "        except ImportError as err:\n",
    "            MultiTensorApply.available = False\n",
    "            MultiTensorApply.import_err = err\n",
    "\n",
    "    def __call__(self, op, noop_flag_buffer, tensor_lists, *args):\n",
    "        return op(self.chunk_size, noop_flag_buffer, tensor_lists, *args)\n",
    "\n",
    "\n",
    "class Adan(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Implements a pytorch variant of Adan\n",
    "    Adan was proposed in\n",
    "    Adan: Adaptive Nesterov Momentum Algorithm for\n",
    "        Faster Optimizing Deep Models[J].arXiv preprint arXiv:2208.06677, 2022.\n",
    "    https://arxiv.org/abs/2208.06677\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or\n",
    "            dicts defining parameter groups.\n",
    "        lr (float, optional): learning rate. (default: 1e-3)\n",
    "        betas (Tuple[float, float, flot], optional): coefficients used for\n",
    "            first- and second-order moments. (default: (0.98, 0.92, 0.99))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability. (default: 1e-8)\n",
    "        weight_decay (float, optional): decoupled weight decay\n",
    "            (L2 penalty) (default: 0)\n",
    "        max_grad_norm (float, optional): value used to clip\n",
    "            global grad norm (default: 0.0 no clip)\n",
    "        no_prox (bool): how to perform the decoupled weight decay\n",
    "            (default: False)\n",
    "        foreach (bool): if True would use torch._foreach implementation.\n",
    "            It's faster but uses slightly more memory. (default: True)\n",
    "        fused (bool, optional): whether fused implementation is used.\n",
    "            (default: False)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-3,\n",
    "                 betas=(0.98, 0.92, 0.99),\n",
    "                 eps=1e-8,\n",
    "                 weight_decay=0.0,\n",
    "                 max_grad_norm=0.0,\n",
    "                 no_prox=False,\n",
    "                 foreach: bool = True,\n",
    "                 fused: bool = False):\n",
    "        if not 0.0 <= max_grad_norm:\n",
    "            raise ValueError('Invalid Max grad norm: {}'.format(max_grad_norm))\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError('Invalid learning rate: {}'.format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError('Invalid epsilon value: {}'.format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 0: {}'.format(\n",
    "                betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 1: {}'.format(\n",
    "                betas[1]))\n",
    "        if not 0.0 <= betas[2] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 2: {}'.format(\n",
    "                betas[2]))\n",
    "        if fused:\n",
    "            _check_fused_available()\n",
    "\n",
    "        defaults = dict(lr=lr,\n",
    "                        betas=betas,\n",
    "                        eps=eps,\n",
    "                        weight_decay=weight_decay,\n",
    "                        max_grad_norm=max_grad_norm,\n",
    "                        no_prox=no_prox,\n",
    "                        foreach=foreach,\n",
    "                        fused=fused)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adan, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('no_prox', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restart_opt(self):\n",
    "        for group in self.param_groups:\n",
    "            group['step'] = 0\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    state = self.state[p]\n",
    "                    # State initialization\n",
    "\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of gradient difference\n",
    "                    state['exp_avg_diff'] = torch.zeros_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\"\"\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        if self.defaults['max_grad_norm'] > 0:\n",
    "            device = self.param_groups[0]['params'][0].device\n",
    "            global_grad_norm = torch.zeros(1, device=device)\n",
    "\n",
    "            max_grad_norm = torch.tensor(self.defaults['max_grad_norm'],\n",
    "                                         device=device)\n",
    "            for group in self.param_groups:\n",
    "\n",
    "                for p in group['params']:\n",
    "                    if p.grad is not None:\n",
    "                        grad = p.grad\n",
    "                        global_grad_norm.add_(grad.pow(2).sum())\n",
    "\n",
    "            global_grad_norm = torch.sqrt(global_grad_norm)\n",
    "\n",
    "            clip_global_grad_norm = torch.clamp(\n",
    "                max_grad_norm / (global_grad_norm + group['eps']),\n",
    "                max=1.0).item()\n",
    "        else:\n",
    "            clip_global_grad_norm = 1.0\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            exp_avg_diffs = []\n",
    "            neg_pre_grads = []\n",
    "\n",
    "            beta1, beta2, beta3 = group['betas']\n",
    "            # assume same step across group now to simplify things\n",
    "            # per parameter step can be easily support\n",
    "            # by making it tensor, or pass list into kernel\n",
    "            if 'step' in group:\n",
    "                group['step'] += 1\n",
    "            else:\n",
    "                group['step'] = 1\n",
    "\n",
    "            bias_correction1 = 1.0 - beta1**group['step']\n",
    "            bias_correction2 = 1.0 - beta2**group['step']\n",
    "            bias_correction3 = 1.0 - beta3**group['step']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                params_with_grad.append(p)\n",
    "                grads.append(p.grad)\n",
    "\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "                    state['exp_avg_diff'] = torch.zeros_like(p)\n",
    "\n",
    "                if 'neg_pre_grad' not in state or group['step'] == 1:\n",
    "                    state['neg_pre_grad'] = p.grad.clone().mul_(\n",
    "                        -clip_global_grad_norm)\n",
    "\n",
    "                exp_avgs.append(state['exp_avg'])\n",
    "                exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "                exp_avg_diffs.append(state['exp_avg_diff'])\n",
    "                neg_pre_grads.append(state['neg_pre_grad'])\n",
    "\n",
    "            if not params_with_grad:\n",
    "                continue\n",
    "\n",
    "            kwargs = dict(\n",
    "                params=params_with_grad,\n",
    "                grads=grads,\n",
    "                exp_avgs=exp_avgs,\n",
    "                exp_avg_sqs=exp_avg_sqs,\n",
    "                exp_avg_diffs=exp_avg_diffs,\n",
    "                neg_pre_grads=neg_pre_grads,\n",
    "                beta1=beta1,\n",
    "                beta2=beta2,\n",
    "                beta3=beta3,\n",
    "                bias_correction1=bias_correction1,\n",
    "                bias_correction2=bias_correction2,\n",
    "                bias_correction3_sqrt=math.sqrt(bias_correction3),\n",
    "                lr=group['lr'],\n",
    "                weight_decay=group['weight_decay'],\n",
    "                eps=group['eps'],\n",
    "                no_prox=group['no_prox'],\n",
    "                clip_global_grad_norm=clip_global_grad_norm,\n",
    "            )\n",
    "\n",
    "            if group['foreach']:\n",
    "                if group['fused']:\n",
    "                    if torch.cuda.is_available():\n",
    "                        _fused_adan_multi_tensor(**kwargs)\n",
    "                    else:\n",
    "                        raise ValueError('Fused Adan does not support CPU')\n",
    "                else:\n",
    "                    _multi_tensor_adan(**kwargs)\n",
    "            elif group['fused']:\n",
    "                if torch.cuda.is_available():\n",
    "                    _fused_adan_single_tensor(**kwargs)\n",
    "                else:\n",
    "                    raise ValueError('Fused Adan does not support CPU')\n",
    "            else:\n",
    "                _single_tensor_adan(**kwargs)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def _single_tensor_adan(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    for i, param in enumerate(params):\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        exp_avg_diff = exp_avg_diffs[i]\n",
    "        neg_grad_or_diff = neg_pre_grads[i]\n",
    "\n",
    "        grad.mul_(clip_global_grad_norm)\n",
    "\n",
    "        # for memory saving, we use `neg_grad_or_diff`\n",
    "        # to get some temp variable in a inplace way\n",
    "        neg_grad_or_diff.add_(grad)\n",
    "\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  # m_t\n",
    "        exp_avg_diff.mul_(beta2).add_(neg_grad_or_diff,\n",
    "                                      alpha=1 - beta2)  # diff_t\n",
    "\n",
    "        neg_grad_or_diff.mul_(beta2).add_(grad)\n",
    "        exp_avg_sq.mul_(beta3).addcmul_(neg_grad_or_diff,\n",
    "                                        neg_grad_or_diff,\n",
    "                                        value=1 - beta3)  # n_t\n",
    "\n",
    "        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)\n",
    "        step_size_diff = lr * beta2 / bias_correction2\n",
    "        step_size = lr / bias_correction1\n",
    "\n",
    "        if no_prox:\n",
    "            param.mul_(1 - lr * weight_decay)\n",
    "            param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)\n",
    "        else:\n",
    "            param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)\n",
    "            param.div_(1 + lr * weight_decay)\n",
    "\n",
    "        neg_grad_or_diff.zero_().add_(grad, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _multi_tensor_adan(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    if len(params) == 0:\n",
    "        return\n",
    "\n",
    "    torch._foreach_mul_(grads, clip_global_grad_norm)\n",
    "\n",
    "    # for memory saving, we use `neg_pre_grads`\n",
    "    # to get some temp variable in a inplace way\n",
    "    torch._foreach_add_(neg_pre_grads, grads)\n",
    "\n",
    "    torch._foreach_mul_(exp_avgs, beta1)\n",
    "    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  # m_t\n",
    "\n",
    "    torch._foreach_mul_(exp_avg_diffs, beta2)\n",
    "    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,\n",
    "                        alpha=1 - beta2)  # diff_t\n",
    "\n",
    "    torch._foreach_mul_(neg_pre_grads, beta2)\n",
    "    torch._foreach_add_(neg_pre_grads, grads)\n",
    "    torch._foreach_mul_(exp_avg_sqs, beta3)\n",
    "    torch._foreach_addcmul_(exp_avg_sqs,\n",
    "                            neg_pre_grads,\n",
    "                            neg_pre_grads,\n",
    "                            value=1 - beta3)  # n_t\n",
    "\n",
    "    denom = torch._foreach_sqrt(exp_avg_sqs)\n",
    "    torch._foreach_div_(denom, bias_correction3_sqrt)\n",
    "    torch._foreach_add_(denom, eps)\n",
    "\n",
    "    step_size_diff = lr * beta2 / bias_correction2\n",
    "    step_size = lr / bias_correction1\n",
    "\n",
    "    if no_prox:\n",
    "        torch._foreach_mul_(params, 1 - lr * weight_decay)\n",
    "        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)\n",
    "        torch._foreach_addcdiv_(params,\n",
    "                                exp_avg_diffs,\n",
    "                                denom,\n",
    "                                value=-step_size_diff)\n",
    "    else:\n",
    "        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)\n",
    "        torch._foreach_addcdiv_(params,\n",
    "                                exp_avg_diffs,\n",
    "                                denom,\n",
    "                                value=-step_size_diff)\n",
    "        torch._foreach_div_(params, 1 + lr * weight_decay)\n",
    "    torch._foreach_zero_(neg_pre_grads)\n",
    "    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _fused_adan_multi_tensor(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    import fused_adan\n",
    "    multi_tensor_applier = MultiTensorApply(2048 * 32)\n",
    "    _dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
    "    multi_tensor_applier(\n",
    "        fused_adan.adan_multi_tensor, _dummy_overflow_buf,\n",
    "        [params, grads, exp_avgs, exp_avg_sqs, exp_avg_diffs, neg_pre_grads],\n",
    "        beta1, beta2, beta3, bias_correction1, bias_correction2,\n",
    "        bias_correction3_sqrt, lr, weight_decay, eps, no_prox,\n",
    "        clip_global_grad_norm)\n",
    "    torch._foreach_zero_(neg_pre_grads)\n",
    "    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _fused_adan_single_tensor(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    for i, param in enumerate(params):\n",
    "        p_data_fp32 = param.data.float()\n",
    "        out_p = param.data\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        exp_avg_diff = exp_avg_diffs[i]\n",
    "        neg_grad = neg_pre_grads[i]\n",
    "        with torch.cuda.device(param.device):\n",
    "            import fused_adan\n",
    "            fused_adan.adan_single_tensor(\n",
    "                p_data_fp32,\n",
    "                out_p,\n",
    "                grad,\n",
    "                exp_avg,\n",
    "                exp_avg_sq,\n",
    "                exp_avg_diff,\n",
    "                neg_grad,\n",
    "                beta1,\n",
    "                beta2,\n",
    "                beta3,\n",
    "                bias_correction1,\n",
    "                bias_correction2,\n",
    "                bias_correction3_sqrt,\n",
    "                lr,\n",
    "                weight_decay,\n",
    "                eps,\n",
    "                no_prox,\n",
    "                clip_global_grad_norm,\n",
    "            )\n",
    "        neg_grad.zero_().add_(grad, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _check_fused_available():\n",
    "    try:\n",
    "        import fused_adan\n",
    "    except ImportError as exc:\n",
    "        if torch.cuda.is_available():\n",
    "            # The module should be available but isn't. Try to\n",
    "            # help the user in this case.\n",
    "            raise ImportError((\n",
    "                str(exc)\n",
    "                + (\n",
    "                    '\\nThis could be caused by not having compiled '\n",
    "                    'the CUDA extension during package installation. '\n",
    "                    'Please try to re-install the package with '\n",
    "                    'the environment flag `FORCE_CUDA=1` set.'\n",
    "                )\n",
    "            ))\n",
    "        else:\n",
    "            raise ImportError(\n",
    "                str(exc) + '\\nFused Adan does not support CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Number of cores: 16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(f\"Number of cores: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the CNN-LSTM Forcasting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MainModel(\n",
      "  (cnn): CNN1D(\n",
      "    (conv_blocks): Sequential(\n",
      "      (0): Conv1d(31, 32, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "      (1): ReLU()\n",
      "      (2): Conv1d(32, 64, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "      (3): ReLU()\n",
      "      (4): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "      (6): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(64, 64, num_layers=3, batch_first=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 118497\n"
     ]
    }
   ],
   "source": [
    "cnn_params = [32, 64, \"MaxPool\", 64]\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_channels=31):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.conv_blocks = self.create_conv_blocks(cnn_params)\n",
    "        \n",
    "    def create_conv_blocks(self, architecture):\n",
    "        layers = []\n",
    "        input_channels = self.input_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if isinstance(x, int):  # Conv layer\n",
    "                layers.append(nn.Conv1d(in_channels=input_channels, out_channels=x, kernel_size=2, stride=1, padding=1))\n",
    "                layers.append(nn.ReLU())\n",
    "                input_channels = x  # Update input channels\n",
    "            elif x == \"MaxPool\":\n",
    "                layers.append(nn.MaxPool1d(kernel_size=2, stride=1))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (batch, features, time_steps) → (batch, channels, time_steps)\n",
    "        x = self.conv_blocks(x)\n",
    "        return x  # Output shape: (batch, channels, reduced_time_steps)\n",
    "\n",
    "class MainModel(nn.Module):\n",
    "    def __init__(self, input_size=31, hidden_size=64, num_layers=3, output_size=1):\n",
    "        super(MainModel, self).__init__()\n",
    "        self.num_layers = num_layers  \n",
    "        self.hidden_size = hidden_size\n",
    "        self.cnn = CNN1D(input_size)\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),   \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 2, 1)  # (batch, channels, time) → (batch, time, channels)\n",
    "        initial_hidden_state = torch.zeros(self.num_layers, x.size(0),self.hidden_size)\n",
    "        initial_cell_state = torch.zeros(self.num_layers, x.size(0),self.hidden_size)\n",
    "        x, _ = self.lstm(x, (initial_hidden_state, initial_cell_state))\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x) \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate Model\n",
    "model = MainModel(input_size=X_train_tensor.shape[2], hidden_size=64, num_layers=3, output_size=1).to(device)\n",
    "\n",
    "print(f\"{model}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([159671, 1])\n",
      "tensor([[ 0.0556],\n",
      "        [-0.0064],\n",
      "        [ 0.0217],\n",
      "        ...,\n",
      "        [ 0.0437],\n",
      "        [ 0.0332],\n",
      "        [ 0.0051]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.randn(X_train_tensor.shape[0], X_train_tensor.shape[1], X_train_tensor.shape[2])\n",
    "# Test Forward Pass\n",
    "y = model(dummy)  # Testing with dummy training data\n",
    "print(f\"{y.shape}\")\n",
    "print(f\"{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train and Test functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader:torch.utils.data.DataLoader,\n",
    "               model:torch.nn.Module,\n",
    "               loss_fn:torch.nn.Module,\n",
    "               optimizer:torch.optim.Optimizer,\n",
    "               calculate_accuracy,\n",
    "               device: torch.device = device,\n",
    "               loss_steps: int = 100,\n",
    "               seed: int = 25):\n",
    "  # Set seed for reproducibility\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "  #Training\n",
    "  train_loss, train_acc = 0, 0\n",
    "  #Put Data into training Mode\n",
    "  model.train()\n",
    "  for batch, (X, y) in enumerate(data_loader):\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    train_loss += loss.item()\n",
    "    train_acc += calculate_accuracy(y_true=y,\n",
    "                             y_pred=y_pred)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  train_loss = train_loss / len(data_loader)\n",
    "  train_acc = train_acc / len(data_loader)\n",
    "  if epoch % loss_steps == 0:\n",
    "    print(f\"Training Loss: {train_loss:.5f} | Training R2: {train_acc:.4f}%\")\n",
    "  return train_loss, train_acc\n",
    "\n",
    "def test(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              calculate_accuracy,\n",
    "              device: torch.device = device,\n",
    "              loss_steps: int = 100,\n",
    "              seed: int = 42):\n",
    "  # Set seed for reproducibility\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "  #Testing\n",
    "  test_loss, test_acc = 0, 0\n",
    "  model.to(device)\n",
    "  #Put Data into evaluation Mode\n",
    "  model.eval()\n",
    "  with torch.inference_mode():\n",
    "    for X, y in data_loader:\n",
    "      X,y = X.to(device), y.to(device)\n",
    "      test_pred = model(X)\n",
    "      loss = loss_fn(test_pred, y)\n",
    "      test_loss += loss.item()\n",
    "      test_acc += calculate_accuracy(y_true=y, y_pred=test_pred)\n",
    "\n",
    "    test_loss /= len(data_loader)\n",
    "    test_acc /= len(data_loader)\n",
    "    if epoch % loss_steps == 0:\n",
    "      print(f\"Test Loss {test_loss:.5f} | Test R2 {test_acc:5f}%\")\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parameters, objective function, and r2 metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes an accuracy-like metric for regression using R^2 Score.\n",
    "    \"\"\"\n",
    "    y_true = y_true.detach().cpu().numpy()\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)  # Best: 1, Worst: -∞\n",
    "    return r2  # Treat as \"accuracy\" for regression\n",
    "\n",
    "## Setup loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_of_epochs = 50\n",
    "num_loss_steps = 5\n",
    "seed_number = 42\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1979c164aa08420c9aaecbbab758b6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      " =====================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m num_loss_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m =====================================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m train_loss, train_accu \u001b[38;5;241m=\u001b[39m train(data_loader \u001b[38;5;241m=\u001b[39m train_dataloader,\n\u001b[1;32m     27\u001b[0m                                 model \u001b[38;5;241m=\u001b[39m model_Adan,\n\u001b[1;32m     28\u001b[0m                                 loss_fn \u001b[38;5;241m=\u001b[39m loss_fn,\n\u001b[1;32m     29\u001b[0m                                 optimizer \u001b[38;5;241m=\u001b[39m optimizer_Adan,\n\u001b[1;32m     30\u001b[0m                                 calculate_accuracy \u001b[38;5;241m=\u001b[39m calculate_accuracy,\n\u001b[1;32m     31\u001b[0m                                 device \u001b[38;5;241m=\u001b[39m device,\n\u001b[1;32m     32\u001b[0m                                 loss_steps \u001b[38;5;241m=\u001b[39m num_loss_steps,\n\u001b[1;32m     33\u001b[0m                                 seed \u001b[38;5;241m=\u001b[39m seed);\n\u001b[1;32m     35\u001b[0m test_loss, test_accu \u001b[38;5;241m=\u001b[39m test(data_loader \u001b[38;5;241m=\u001b[39m test_dataloader,\n\u001b[1;32m     36\u001b[0m                             model \u001b[38;5;241m=\u001b[39m model_Adan,\n\u001b[1;32m     37\u001b[0m                             loss_fn \u001b[38;5;241m=\u001b[39m loss_fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m                             loss_steps \u001b[38;5;241m=\u001b[39m num_loss_steps,\n\u001b[1;32m     41\u001b[0m                             seed\u001b[38;5;241m=\u001b[39mseed);\n\u001b[1;32m     43\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data_loader, model, loss_fn, optimizer, calculate_accuracy, device, loss_steps, seed)\u001b[0m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n\u001b[1;32m     22\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 23\u001b[0m train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m calculate_accuracy(y_true\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m     24\u001b[0m                          y_pred\u001b[38;5;241m=\u001b[39my_pred)\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m, in \u001b[0;36mcalculate_accuracy\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      6\u001b[0m y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m----> 9\u001b[0m r2 \u001b[38;5;241m=\u001b[39m r2_score(y_true, y_pred)  \u001b[38;5;66;03m# Best: 1, Worst: -∞\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r2\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1257\u001b[0m, in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, force_finite)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\":math:`R^2` (coefficient of determination) regression score function.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \n\u001b[1;32m   1135\u001b[0m \u001b[38;5;124;03mBest possible score is 1.0 and it can be negative (because the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;124;03m-inf\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m xp, _, device_ \u001b[38;5;241m=\u001b[39m get_namespace_and_device(\n\u001b[1;32m   1253\u001b[0m     y_true, y_pred, sample_weight, multioutput\n\u001b[1;32m   1254\u001b[0m )\n\u001b[1;32m   1256\u001b[0m _, y_true, y_pred, sample_weight, multioutput \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1257\u001b[0m     _check_reg_targets_with_floating_dtype(\n\u001b[1;32m   1258\u001b[0m         y_true, y_pred, sample_weight, multioutput, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[1;32m   1259\u001b[0m     )\n\u001b[1;32m   1260\u001b[0m )\n\u001b[1;32m   1262\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y_pred) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/sklearn/metrics/_regression.py:198\u001b[0m, in \u001b[0;36m_check_reg_targets_with_floating_dtype\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ensures that y_true, y_pred, and sample_weight correspond to the same\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03mregression task.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m dtype_name \u001b[38;5;241m=\u001b[39m _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m--> 198\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    199\u001b[0m     y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39mdtype_name, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[1;32m    200\u001b[0m )\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# _check_reg_targets does not accept sample_weight as input.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Convert sample_weight's data type separately to match dtype_name.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/sklearn/metrics/_regression.py:106\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m    104\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    105\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 106\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    109\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mreshape(y_true, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/sklearn/utils/validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1107\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1108\u001b[0m         array,\n\u001b[1;32m   1109\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1110\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1111\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mensure_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1112\u001b[0m     )\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    121\u001b[0m     X,\n\u001b[1;32m    122\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    123\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    124\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    125\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    126\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    127\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "model_Adan = MainModel(input_size=X_train_tensor.shape[2], \n",
    "                       hidden_size=64, num_layers=3, output_size=1).to(device)\n",
    "\n",
    "optimizer_Adan = Adan(params=model_Adan.parameters(), \n",
    "                      lr=learning_rate, \n",
    "                      betas=(0.95, 0.92, 0.99), \n",
    "                      weight_decay=1e-2)\n",
    "\n",
    "results = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accu\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"test_accu\": []\n",
    "}\n",
    "\n",
    "start_adan = time.perf_counter()\n",
    "for epoch in tqdm(range(0, num_of_epochs)):\n",
    "  if epoch % num_loss_steps == 0:\n",
    "    print(f\"Epoch: {epoch} \\n =====================================================================\")\n",
    "\n",
    "  train_loss, train_accu = train(data_loader = train_dataloader,\n",
    "                                  model = model_Adan,\n",
    "                                  loss_fn = loss_fn,\n",
    "                                  optimizer = optimizer_Adan,\n",
    "                                  calculate_accuracy = calculate_accuracy,\n",
    "                                  device = device,\n",
    "                                  loss_steps = num_loss_steps,\n",
    "                                  seed = seed);\n",
    "\n",
    "  test_loss, test_accu = test(data_loader = test_dataloader,\n",
    "                              model = model_Adan,\n",
    "                              loss_fn = loss_fn,\n",
    "                              calculate_accuracy = calculate_accuracy,\n",
    "                              device = device,\n",
    "                              loss_steps = num_loss_steps,\n",
    "                              seed=seed);\n",
    "\n",
    "  results[\"train_loss\"].append(train_loss)\n",
    "  results[\"train_accu\"].append(train_accu)\n",
    "  results[\"test_loss\"].append(test_loss)\n",
    "  results[\"test_accu\"].append(test_accu)\n",
    "  \n",
    "end_adan = time.perf_counter()\n",
    "total_time_adan = end_adan - start_adan\n",
    "print(f\"Total Runtime: {total_time_adan / 60: .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaNs in training data...\n",
      "tensor(85015) tensor(0)\n",
      "Checking for NaNs in test data...\n",
      "tensor(10739) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for NaNs in training data...\")\n",
    "print(torch.isnan(X_train_tensor).sum(), torch.isnan(y_train_tensor).sum())\n",
    "\n",
    "print(\"Checking for NaNs in test data...\")\n",
    "print(torch.isnan(X_test_tensor).sum(), torch.isnan(y_test_tensor).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2035,  0.3873,  1.4655,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [ 0.7407,  0.3538, -0.7903,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.2035, -0.1074,  0.0460,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.1361, -0.3332, -0.5716,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [ 0.4709,  0.1363, -0.6785,  ..., -0.1070, -0.0594, -0.1187]],\n",
       "\n",
       "        [[ 0.7407,  0.3538, -0.7903,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.2035, -0.1074,  0.0460,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.1361, -0.3332, -0.5716,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [ 0.4709,  0.1363, -0.6785,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.8105, -0.5763,  0.4137,  ..., -0.1070, -0.0594, -0.1187]],\n",
       "\n",
       "        [[-0.2035, -0.1074,  0.0460,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.1361, -0.3332, -0.5716,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [ 0.4709,  0.1363, -0.6785,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.8105, -0.5763,  0.4137,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [ 0.6058,  0.4917, -0.2479,  ..., -0.1070, -0.0594, -0.1187]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4058, -0.0595,  0.6985,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-2.7663, -2.4080,  0.9328,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.8779, -1.2377, -0.8726,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.6081, -0.8065, -0.4979,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.9454, -1.2103, -0.6401,  ..., -0.1070, -0.0594, -0.1187]],\n",
       "\n",
       "        [[-2.7663, -2.4080,  0.9328,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.8779, -1.2377, -0.8726,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.6081, -0.8065, -0.4979,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.9454, -1.2103, -0.6401,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.4733,  1.0674,  3.4442,  ..., -0.1070, -0.0594, -0.1187]],\n",
       "\n",
       "        [[-0.8779, -1.2377, -0.8726,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.6081, -0.8065, -0.4979,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.9454, -1.2103, -0.6401,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-0.4733,  1.0674,  3.4442,  ..., -0.1070, -0.0594, -0.1187],\n",
       "         [-1.4849, -0.9186,  1.1451,  ..., -0.1070, -0.0594, -0.1187]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
