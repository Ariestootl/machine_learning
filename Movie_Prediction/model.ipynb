{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: Index(['tconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult',\n",
      "       'startYear', 'runtimeMinutes', 'genres', 'averageRating', 'numVotes',\n",
      "       'HitScore'],\n",
      "      dtype='object')\n",
      "Shape: (199595, 11)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"movies_new.csv\")\n",
    "df = df.sort_values(by=\"startYear\")\n",
    "print(f\"Features: {df.columns}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "df['genres_list'] = df['genres'].apply(lambda x: x.split(','))\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_encoded = mlb.fit_transform(df['genres_list'])  # Fit & transform genres\n",
    "\n",
    "genre_columns = mlb.classes_  # Get genre names as column names\n",
    "df[genre_columns] = genre_encoded  # Add encoded genres to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "df['isAdult'] = df['isAdult'].astype(int)  \n",
    "df['numVotes_log'] = np.log1p(df['numVotes'])\n",
    "\n",
    "df['numVotes_log'] = np.log1p(df['numVotes'])\n",
    "numeric_columns = ['averageRating', 'HitScore', 'numVotes', 'numVotes_log', 'runtimeMinutes', 'startYear']\n",
    "df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "df.loc[:, 'runtimeMinutes'] = df['runtimeMinutes'].fillna(df['runtimeMinutes'].median())\n",
    "df.drop(columns=[\"originalTitle\"], inplace=True)\n",
    "\n",
    "# Define features and target\n",
    "X= df[['averageRating', 'HitScore', 'numVotes_log', 'runtimeMinutes']+ list(genre_columns)].to_numpy()\n",
    "y = df['HitScore'].to_numpy().reshape(-1,1)\n",
    "\n",
    "# Split data into training (80%) and temp (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split temp (20%) into validation (10%) and test (10%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"{X_train.shape[0] + X_val.shape[0] + X_test.shape[0] == df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerX = StandardScaler()\n",
    "X_train_scaled = scalerX.fit_transform(X_train)  # Fit on training set\n",
    "X_test_scaled = scalerX.transform(X_test)\n",
    "X_val_scaled = scalerX.transform(X_val)  # Apply on test set\n",
    "\n",
    "\n",
    "scalerY = StandardScaler()\n",
    "y_train_scaled = scalerY.fit_transform(y_train.reshape(-1, 1))  # Fit on training set\n",
    "y_test_scaled = scalerY.transform(y_test.reshape(-1, 1))  # Apply on test set\n",
    "y_val_scaled = scalerY.transform(y_val.reshape(-1, 1))  # Apply on validation test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame as a Time Series problem\n",
    "* Convert the data into a sequential type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (159671, 5, 31)\n",
      "Val shape: (19954, 5, 31)\n",
      "Test shape: (19955, 5, 31)\n"
     ]
    }
   ],
   "source": [
    "# Define time step (e.g., 5 years)\n",
    "TIME_STEP = 5  \n",
    "\n",
    "def create_sequences(X, y, time_step):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_step):\n",
    "        Xs.append(X[i:i+time_step])  # Past 5 years\n",
    "        ys.append(y[i+time_step])    # Target is the next year\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Convert datasets into 3D shape (samples, time steps, features)\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, TIME_STEP)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val, TIME_STEP)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, TIME_STEP)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Train shape:\", X_train_seq.shape)  # (samples, time_steps, features)\n",
    "print(\"Val shape:\", X_val_seq.shape)\n",
    "print(\"Test shape:\", X_test_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([159671, 5, 31]) torch.Size([159671, 1])\n",
      "torch.Size([19954, 5, 31]) torch.Size([19954, 1])\n",
      "torch.Size([19955, 5, 31]) torch.Size([19955, 1])\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_seq, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_seq, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Check shapes\n",
    "print(X_train_tensor.shape, y_train_tensor.shape)\n",
    "print(X_val_tensor.shape, y_val_tensor.shape)\n",
    "print(X_test_tensor.shape, y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Dataloaders from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders:\n",
      "Train dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f3d0663f620>\n",
      "Test dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f3d0d0e7770>\n",
      "Validation dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f3d0d91fa40>\n",
      "Length of train dataloader: 1248 batches of 128\n",
      "Length of validation dataloader: 156 batches of 128\n",
      "Length of test dataloader: 156 batches of 128\n",
      "========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 128  \n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) \n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Dataloaders:\\nTrain dataloader: {train_dataloader}\\nTest dataloader: {test_dataloader}\\nValidation dataloader: {val_dataloader}\")\n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {train_dataloader.batch_size}\")\n",
    "print(f\"Length of validation dataloader: {len(val_dataloader)} batches of {val_dataloader.batch_size}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {test_dataloader.batch_size}\")\n",
    "print(f\"==============================================================================\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adan: Adaptive Nesterov Momentum Algorithm\n",
    "\n",
    "import math\n",
    "from typing import List\n",
    "from torch import Tensor\n",
    "\n",
    "class MultiTensorApply(object):\n",
    "    available = False\n",
    "    warned = False\n",
    "\n",
    "    def __init__(self, chunk_size):\n",
    "        try:\n",
    "            MultiTensorApply.available = True\n",
    "            self.chunk_size = chunk_size\n",
    "        except ImportError as err:\n",
    "            MultiTensorApply.available = False\n",
    "            MultiTensorApply.import_err = err\n",
    "\n",
    "    def __call__(self, op, noop_flag_buffer, tensor_lists, *args):\n",
    "        return op(self.chunk_size, noop_flag_buffer, tensor_lists, *args)\n",
    "\n",
    "\n",
    "class Adan(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Implements a pytorch variant of Adan\n",
    "    Adan was proposed in\n",
    "    Adan: Adaptive Nesterov Momentum Algorithm for\n",
    "        Faster Optimizing Deep Models[J].arXiv preprint arXiv:2208.06677, 2022.\n",
    "    https://arxiv.org/abs/2208.06677\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or\n",
    "            dicts defining parameter groups.\n",
    "        lr (float, optional): learning rate. (default: 1e-3)\n",
    "        betas (Tuple[float, float, flot], optional): coefficients used for\n",
    "            first- and second-order moments. (default: (0.98, 0.92, 0.99))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability. (default: 1e-8)\n",
    "        weight_decay (float, optional): decoupled weight decay\n",
    "            (L2 penalty) (default: 0)\n",
    "        max_grad_norm (float, optional): value used to clip\n",
    "            global grad norm (default: 0.0 no clip)\n",
    "        no_prox (bool): how to perform the decoupled weight decay\n",
    "            (default: False)\n",
    "        foreach (bool): if True would use torch._foreach implementation.\n",
    "            It's faster but uses slightly more memory. (default: True)\n",
    "        fused (bool, optional): whether fused implementation is used.\n",
    "            (default: False)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-3,\n",
    "                 betas=(0.98, 0.92, 0.99),\n",
    "                 eps=1e-8,\n",
    "                 weight_decay=0.0,\n",
    "                 max_grad_norm=0.0,\n",
    "                 no_prox=False,\n",
    "                 foreach: bool = True,\n",
    "                 fused: bool = False):\n",
    "        if not 0.0 <= max_grad_norm:\n",
    "            raise ValueError('Invalid Max grad norm: {}'.format(max_grad_norm))\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError('Invalid learning rate: {}'.format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError('Invalid epsilon value: {}'.format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 0: {}'.format(\n",
    "                betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 1: {}'.format(\n",
    "                betas[1]))\n",
    "        if not 0.0 <= betas[2] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 2: {}'.format(\n",
    "                betas[2]))\n",
    "        if fused:\n",
    "            _check_fused_available()\n",
    "\n",
    "        defaults = dict(lr=lr,\n",
    "                        betas=betas,\n",
    "                        eps=eps,\n",
    "                        weight_decay=weight_decay,\n",
    "                        max_grad_norm=max_grad_norm,\n",
    "                        no_prox=no_prox,\n",
    "                        foreach=foreach,\n",
    "                        fused=fused)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adan, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('no_prox', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restart_opt(self):\n",
    "        for group in self.param_groups:\n",
    "            group['step'] = 0\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    state = self.state[p]\n",
    "                    # State initialization\n",
    "\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of gradient difference\n",
    "                    state['exp_avg_diff'] = torch.zeros_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\"\"\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        if self.defaults['max_grad_norm'] > 0:\n",
    "            device = self.param_groups[0]['params'][0].device\n",
    "            global_grad_norm = torch.zeros(1, device=device)\n",
    "\n",
    "            max_grad_norm = torch.tensor(self.defaults['max_grad_norm'],\n",
    "                                         device=device)\n",
    "            for group in self.param_groups:\n",
    "\n",
    "                for p in group['params']:\n",
    "                    if p.grad is not None:\n",
    "                        grad = p.grad\n",
    "                        global_grad_norm.add_(grad.pow(2).sum())\n",
    "\n",
    "            global_grad_norm = torch.sqrt(global_grad_norm)\n",
    "\n",
    "            clip_global_grad_norm = torch.clamp(\n",
    "                max_grad_norm / (global_grad_norm + group['eps']),\n",
    "                max=1.0).item()\n",
    "        else:\n",
    "            clip_global_grad_norm = 1.0\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            exp_avg_diffs = []\n",
    "            neg_pre_grads = []\n",
    "\n",
    "            beta1, beta2, beta3 = group['betas']\n",
    "            # assume same step across group now to simplify things\n",
    "            # per parameter step can be easily support\n",
    "            # by making it tensor, or pass list into kernel\n",
    "            if 'step' in group:\n",
    "                group['step'] += 1\n",
    "            else:\n",
    "                group['step'] = 1\n",
    "\n",
    "            bias_correction1 = 1.0 - beta1**group['step']\n",
    "            bias_correction2 = 1.0 - beta2**group['step']\n",
    "            bias_correction3 = 1.0 - beta3**group['step']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                params_with_grad.append(p)\n",
    "                grads.append(p.grad)\n",
    "\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "                    state['exp_avg_diff'] = torch.zeros_like(p)\n",
    "\n",
    "                if 'neg_pre_grad' not in state or group['step'] == 1:\n",
    "                    state['neg_pre_grad'] = p.grad.clone().mul_(\n",
    "                        -clip_global_grad_norm)\n",
    "\n",
    "                exp_avgs.append(state['exp_avg'])\n",
    "                exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "                exp_avg_diffs.append(state['exp_avg_diff'])\n",
    "                neg_pre_grads.append(state['neg_pre_grad'])\n",
    "\n",
    "            if not params_with_grad:\n",
    "                continue\n",
    "\n",
    "            kwargs = dict(\n",
    "                params=params_with_grad,\n",
    "                grads=grads,\n",
    "                exp_avgs=exp_avgs,\n",
    "                exp_avg_sqs=exp_avg_sqs,\n",
    "                exp_avg_diffs=exp_avg_diffs,\n",
    "                neg_pre_grads=neg_pre_grads,\n",
    "                beta1=beta1,\n",
    "                beta2=beta2,\n",
    "                beta3=beta3,\n",
    "                bias_correction1=bias_correction1,\n",
    "                bias_correction2=bias_correction2,\n",
    "                bias_correction3_sqrt=math.sqrt(bias_correction3),\n",
    "                lr=group['lr'],\n",
    "                weight_decay=group['weight_decay'],\n",
    "                eps=group['eps'],\n",
    "                no_prox=group['no_prox'],\n",
    "                clip_global_grad_norm=clip_global_grad_norm,\n",
    "            )\n",
    "\n",
    "            if group['foreach']:\n",
    "                if group['fused']:\n",
    "                    if torch.cuda.is_available():\n",
    "                        _fused_adan_multi_tensor(**kwargs)\n",
    "                    else:\n",
    "                        raise ValueError('Fused Adan does not support CPU')\n",
    "                else:\n",
    "                    _multi_tensor_adan(**kwargs)\n",
    "            elif group['fused']:\n",
    "                if torch.cuda.is_available():\n",
    "                    _fused_adan_single_tensor(**kwargs)\n",
    "                else:\n",
    "                    raise ValueError('Fused Adan does not support CPU')\n",
    "            else:\n",
    "                _single_tensor_adan(**kwargs)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def _single_tensor_adan(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    for i, param in enumerate(params):\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        exp_avg_diff = exp_avg_diffs[i]\n",
    "        neg_grad_or_diff = neg_pre_grads[i]\n",
    "\n",
    "        grad.mul_(clip_global_grad_norm)\n",
    "\n",
    "        # for memory saving, we use `neg_grad_or_diff`\n",
    "        # to get some temp variable in a inplace way\n",
    "        neg_grad_or_diff.add_(grad)\n",
    "\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  # m_t\n",
    "        exp_avg_diff.mul_(beta2).add_(neg_grad_or_diff,\n",
    "                                      alpha=1 - beta2)  # diff_t\n",
    "\n",
    "        neg_grad_or_diff.mul_(beta2).add_(grad)\n",
    "        exp_avg_sq.mul_(beta3).addcmul_(neg_grad_or_diff,\n",
    "                                        neg_grad_or_diff,\n",
    "                                        value=1 - beta3)  # n_t\n",
    "\n",
    "        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)\n",
    "        step_size_diff = lr * beta2 / bias_correction2\n",
    "        step_size = lr / bias_correction1\n",
    "\n",
    "        if no_prox:\n",
    "            param.mul_(1 - lr * weight_decay)\n",
    "            param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)\n",
    "        else:\n",
    "            param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)\n",
    "            param.div_(1 + lr * weight_decay)\n",
    "\n",
    "        neg_grad_or_diff.zero_().add_(grad, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _multi_tensor_adan(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    if len(params) == 0:\n",
    "        return\n",
    "\n",
    "    torch._foreach_mul_(grads, clip_global_grad_norm)\n",
    "\n",
    "    # for memory saving, we use `neg_pre_grads`\n",
    "    # to get some temp variable in a inplace way\n",
    "    torch._foreach_add_(neg_pre_grads, grads)\n",
    "\n",
    "    torch._foreach_mul_(exp_avgs, beta1)\n",
    "    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  # m_t\n",
    "\n",
    "    torch._foreach_mul_(exp_avg_diffs, beta2)\n",
    "    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,\n",
    "                        alpha=1 - beta2)  # diff_t\n",
    "\n",
    "    torch._foreach_mul_(neg_pre_grads, beta2)\n",
    "    torch._foreach_add_(neg_pre_grads, grads)\n",
    "    torch._foreach_mul_(exp_avg_sqs, beta3)\n",
    "    torch._foreach_addcmul_(exp_avg_sqs,\n",
    "                            neg_pre_grads,\n",
    "                            neg_pre_grads,\n",
    "                            value=1 - beta3)  # n_t\n",
    "\n",
    "    denom = torch._foreach_sqrt(exp_avg_sqs)\n",
    "    torch._foreach_div_(denom, bias_correction3_sqrt)\n",
    "    torch._foreach_add_(denom, eps)\n",
    "\n",
    "    step_size_diff = lr * beta2 / bias_correction2\n",
    "    step_size = lr / bias_correction1\n",
    "\n",
    "    if no_prox:\n",
    "        torch._foreach_mul_(params, 1 - lr * weight_decay)\n",
    "        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)\n",
    "        torch._foreach_addcdiv_(params,\n",
    "                                exp_avg_diffs,\n",
    "                                denom,\n",
    "                                value=-step_size_diff)\n",
    "    else:\n",
    "        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)\n",
    "        torch._foreach_addcdiv_(params,\n",
    "                                exp_avg_diffs,\n",
    "                                denom,\n",
    "                                value=-step_size_diff)\n",
    "        torch._foreach_div_(params, 1 + lr * weight_decay)\n",
    "    torch._foreach_zero_(neg_pre_grads)\n",
    "    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _fused_adan_multi_tensor(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    import fused_adan\n",
    "    multi_tensor_applier = MultiTensorApply(2048 * 32)\n",
    "    _dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
    "    multi_tensor_applier(\n",
    "        fused_adan.adan_multi_tensor, _dummy_overflow_buf,\n",
    "        [params, grads, exp_avgs, exp_avg_sqs, exp_avg_diffs, neg_pre_grads],\n",
    "        beta1, beta2, beta3, bias_correction1, bias_correction2,\n",
    "        bias_correction3_sqrt, lr, weight_decay, eps, no_prox,\n",
    "        clip_global_grad_norm)\n",
    "    torch._foreach_zero_(neg_pre_grads)\n",
    "    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _fused_adan_single_tensor(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    exp_avgs: List[Tensor],\n",
    "    exp_avg_sqs: List[Tensor],\n",
    "    exp_avg_diffs: List[Tensor],\n",
    "    neg_pre_grads: List[Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    beta3: float,\n",
    "    bias_correction1: float,\n",
    "    bias_correction2: float,\n",
    "    bias_correction3_sqrt: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    eps: float,\n",
    "    no_prox: bool,\n",
    "    clip_global_grad_norm: Tensor,\n",
    "):\n",
    "    for i, param in enumerate(params):\n",
    "        p_data_fp32 = param.data.float()\n",
    "        out_p = param.data\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        exp_avg_diff = exp_avg_diffs[i]\n",
    "        neg_grad = neg_pre_grads[i]\n",
    "        with torch.cuda.device(param.device):\n",
    "            import fused_adan\n",
    "            fused_adan.adan_single_tensor(\n",
    "                p_data_fp32,\n",
    "                out_p,\n",
    "                grad,\n",
    "                exp_avg,\n",
    "                exp_avg_sq,\n",
    "                exp_avg_diff,\n",
    "                neg_grad,\n",
    "                beta1,\n",
    "                beta2,\n",
    "                beta3,\n",
    "                bias_correction1,\n",
    "                bias_correction2,\n",
    "                bias_correction3_sqrt,\n",
    "                lr,\n",
    "                weight_decay,\n",
    "                eps,\n",
    "                no_prox,\n",
    "                clip_global_grad_norm,\n",
    "            )\n",
    "        neg_grad.zero_().add_(grad, alpha=-1.0)\n",
    "\n",
    "\n",
    "def _check_fused_available():\n",
    "    try:\n",
    "        import fused_adan\n",
    "    except ImportError as exc:\n",
    "        if torch.cuda.is_available():\n",
    "            # The module should be available but isn't. Try to\n",
    "            # help the user in this case.\n",
    "            raise ImportError((\n",
    "                str(exc)\n",
    "                + (\n",
    "                    '\\nThis could be caused by not having compiled '\n",
    "                    'the CUDA extension during package installation. '\n",
    "                    'Please try to re-install the package with '\n",
    "                    'the environment flag `FORCE_CUDA=1` set.'\n",
    "                )\n",
    "            ))\n",
    "        else:\n",
    "            raise ImportError(\n",
    "                str(exc) + '\\nFused Adan does not support CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Number of cores: 16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(f\"Number of cores: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the CNN-LSTM Forcasting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MainModel(\n",
      "  (cnn): CNN1D(\n",
      "    (conv_blocks): Sequential(\n",
      "      (0): Conv1d(31, 16, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "      (1): ReLU()\n",
      "      (2): Conv1d(16, 32, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "      (3): ReLU()\n",
      "      (4): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv1d(32, 64, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "      (6): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(128, 128, num_layers=4, batch_first=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 551249\n"
     ]
    }
   ],
   "source": [
    "cnn_params = [16, 32, \"MaxPool\", 64]\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_channels=31):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.conv_blocks = self.create_conv_blocks(cnn_params)\n",
    "        \n",
    "    def create_conv_blocks(self, architecture):\n",
    "        layers = []\n",
    "        input_channels = self.input_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if isinstance(x, int):  # Conv layer\n",
    "                layers.append(nn.Conv1d(in_channels=input_channels, out_channels=x, kernel_size=2, stride=1, padding=1))\n",
    "                layers.append(nn.ReLU())\n",
    "                input_channels = x  # Update input channels\n",
    "            elif x == \"MaxPool\":\n",
    "                layers.append(nn.MaxPool1d(kernel_size=2, stride=1))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (batch, features, time_steps) → (batch, channels, time_steps)\n",
    "        x = self.conv_blocks(x)\n",
    "        return x  # Output shape: (batch, channels, reduced_time_steps)\n",
    "\n",
    "class MainModel(nn.Module):\n",
    "    def __init__(self, input_size=64, hidden_size=128, num_layers=4, output_size=1):\n",
    "        super(MainModel, self).__init__()\n",
    "        self.num_layers = num_layers  \n",
    "        self.hidden_size = hidden_size\n",
    "        self.cnn = CNN1D(input_size)\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),   \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 2, 1)  # (batch, channels, time) → (batch, time, channels)\n",
    "        initial_hidden_state = torch.zeros(self.num_layers, x.size(0),self.hidden_size)\n",
    "        initial_cell_state = torch.zeros(self.num_layers, x.size(0),self.hidden_size)\n",
    "        x, _ = self.lstm(x, (initial_hidden_state, initial_cell_state))\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x) \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate Model\n",
    "model = MainModel(input_size=X_train_tensor.shape[2], hidden_size=128, num_layers=4, output_size=1).to(device)\n",
    "\n",
    "print(f\"{model}\")\n",
    "\n",
    "\n",
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 128, got 64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dummy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(X_train_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X_train_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], X_train_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Test Forward Pass\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m y \u001b[38;5;241m=\u001b[39m model(dummy)  \u001b[38;5;66;03m# Testing with dummy training data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m, in \u001b[0;36mMainModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m initial_hidden_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m     45\u001b[0m initial_cell_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[0;32m---> 46\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x, (initial_hidden_state, initial_cell_state))\n\u001b[1;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x) \n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1119\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1116\u001b[0m             hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   1117\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m-> 1119\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1000\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    997\u001b[0m     hidden: Tuple[Tensor, Tensor],  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    998\u001b[0m     batch_sizes: Optional[Tensor],\n\u001b[1;32m    999\u001b[0m ):\n\u001b[0;32m-> 1000\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[1;32m   1002\u001b[0m         hidden[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1003\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1005\u001b[0m     )\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[1;32m   1007\u001b[0m         hidden[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   1008\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m   1009\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1010\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/torch/nn/modules/rnn.py:312\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m     )\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 128, got 64"
     ]
    }
   ],
   "source": [
    "dummy = torch.randn(X_train_tensor.shape[0], X_train_tensor.shape[1], X_train_tensor.shape[2])\n",
    "# Test Forward Pass\n",
    "y = model(dummy)  # Testing with dummy training data\n",
    "print(f\"{y.shape}\")\n",
    "print(f\"{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train and Test functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader:torch.utils.data.DataLoader,\n",
    "               model:torch.nn.Module,\n",
    "               loss_fn:torch.nn.Module,\n",
    "               optimizer:torch.optim.Optimizer,\n",
    "               calculate_accuracy,\n",
    "               device: torch.device = device,\n",
    "               loss_steps: int = 100,\n",
    "               seed: int = 25):\n",
    "  # Set seed for reproducibility\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "  #Training\n",
    "  train_loss, train_acc = 0, 0\n",
    "  #Put Data into training Mode\n",
    "  model.train()\n",
    "  for batch, (X, y) in enumerate(data_loader):\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    train_loss += loss.item()\n",
    "    train_acc += calculate_accuracy(y_true=y,\n",
    "                             y_pred=y_pred)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  train_loss = train_loss / len(data_loader)\n",
    "  train_acc = train_acc / len(data_loader)\n",
    "  if epoch % loss_steps == 0:\n",
    "    print(f\"Training Loss: {train_loss:.5f} | Training R2: {train_acc:.4f}%\")\n",
    "  return train_loss, train_acc\n",
    "\n",
    "def test(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              calculate_accuracy,\n",
    "              device: torch.device = device,\n",
    "              loss_steps: int = 100,\n",
    "              seed: int = 42):\n",
    "  # Set seed for reproducibility\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "  #Testing\n",
    "  test_loss, test_acc = 0, 0\n",
    "  model.to(device)\n",
    "  #Put Data into evaluation Mode\n",
    "  model.eval()\n",
    "  with torch.inference_mode():\n",
    "    for X, y in data_loader:\n",
    "      X,y = X.to(device), y.to(device)\n",
    "      test_pred = model(X)\n",
    "      loss = loss_fn(test_pred, y)\n",
    "      test_loss += loss.item()\n",
    "      test_acc += calculate_accuracy(y_true=y, y_pred=test_pred)\n",
    "\n",
    "    test_loss /= len(data_loader)\n",
    "    test_acc /= len(data_loader)\n",
    "    if epoch % loss_steps == 0:\n",
    "      print(f\"Test Loss {test_loss:.5f} | Test R2 {test_acc:5f}%\")\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parameters, objective function, and r2 metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes an accuracy-like metric for regression using R^2 Score.\n",
    "    \"\"\"\n",
    "    y_true = y_true.detach().cpu().numpy()\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)  # Best: 1, Worst: -∞\n",
    "    return r2  # Treat as \"accuracy\" for regression\n",
    "\n",
    "## Setup loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_of_epochs = 100\n",
    "num_loss_steps = 10\n",
    "seed_number = 42\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6da23cf1466409f84fdb8f13db27907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      " =====================================================================\n",
      "Training Loss: 1.93442 | Training R2: -0.6342%\n",
      "Test Loss 1.25608 | Test R2 -0.057005%\n",
      "Epoch: 5 \n",
      " =====================================================================\n",
      "Training Loss: 1.36669 | Training R2: -0.1368%\n",
      "Test Loss 1.25345 | Test R2 -0.054380%\n",
      "Epoch: 10 \n",
      " =====================================================================\n",
      "Training Loss: 1.29186 | Training R2: -0.0698%\n",
      "Test Loss 1.24696 | Test R2 -0.047983%\n",
      "Epoch: 15 \n",
      " =====================================================================\n",
      "Training Loss: 1.27961 | Training R2: -0.0590%\n",
      "Test Loss 1.24704 | Test R2 -0.048022%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m num_loss_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m =====================================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m train_loss, train_accu \u001b[38;5;241m=\u001b[39m train(data_loader \u001b[38;5;241m=\u001b[39m train_dataloader,\n\u001b[1;32m     27\u001b[0m                                 model \u001b[38;5;241m=\u001b[39m model_Adan,\n\u001b[1;32m     28\u001b[0m                                 loss_fn \u001b[38;5;241m=\u001b[39m loss_fn,\n\u001b[1;32m     29\u001b[0m                                 optimizer \u001b[38;5;241m=\u001b[39m optimizer_Adan,\n\u001b[1;32m     30\u001b[0m                                 calculate_accuracy \u001b[38;5;241m=\u001b[39m calculate_accuracy,\n\u001b[1;32m     31\u001b[0m                                 device \u001b[38;5;241m=\u001b[39m device,\n\u001b[1;32m     32\u001b[0m                                 loss_steps \u001b[38;5;241m=\u001b[39m num_loss_steps,\n\u001b[1;32m     33\u001b[0m                                 seed \u001b[38;5;241m=\u001b[39m seed);\n\u001b[1;32m     35\u001b[0m test_loss, test_accu \u001b[38;5;241m=\u001b[39m test(data_loader \u001b[38;5;241m=\u001b[39m test_dataloader,\n\u001b[1;32m     36\u001b[0m                             model \u001b[38;5;241m=\u001b[39m model_Adan,\n\u001b[1;32m     37\u001b[0m                             loss_fn \u001b[38;5;241m=\u001b[39m loss_fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m                             loss_steps \u001b[38;5;241m=\u001b[39m num_loss_steps,\n\u001b[1;32m     41\u001b[0m                             seed\u001b[38;5;241m=\u001b[39mseed);\n\u001b[1;32m     43\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data_loader, model, loss_fn, optimizer, calculate_accuracy, device, loss_steps, seed)\u001b[0m\n\u001b[1;32m     23\u001b[0m   train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m calculate_accuracy(y_true\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m     24\u001b[0m                            y_pred\u001b[38;5;241m=\u001b[39my_pred)\n\u001b[1;32m     25\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 26\u001b[0m   loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader)\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/aries/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "model_Adan = MainModel(input_size=X_train_tensor.shape[2], \n",
    "                       hidden_size=64, num_layers=3, output_size=1).to(device)\n",
    "\n",
    "optimizer_Adan = Adan(params=model_Adan.parameters(), \n",
    "                      lr=learning_rate, \n",
    "                      betas=(0.95, 0.92, 0.99), \n",
    "                      weight_decay=1e-2)\n",
    "\n",
    "results = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accu\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"test_accu\": []\n",
    "}\n",
    "\n",
    "start_adan = time.perf_counter()\n",
    "for epoch in tqdm(range(0, num_of_epochs)):\n",
    "  if epoch % num_loss_steps == 0:\n",
    "    print(f\"Epoch: {epoch} \\n =====================================================================\")\n",
    "\n",
    "  train_loss, train_accu = train(data_loader = train_dataloader,\n",
    "                                  model = model_Adan,\n",
    "                                  loss_fn = loss_fn,\n",
    "                                  optimizer = optimizer_Adan,\n",
    "                                  calculate_accuracy = calculate_accuracy,\n",
    "                                  device = device,\n",
    "                                  loss_steps = num_loss_steps,\n",
    "                                  seed = seed);\n",
    "\n",
    "  test_loss, test_accu = test(data_loader = test_dataloader,\n",
    "                              model = model_Adan,\n",
    "                              loss_fn = loss_fn,\n",
    "                              calculate_accuracy = calculate_accuracy,\n",
    "                              device = device,\n",
    "                              loss_steps = num_loss_steps,\n",
    "                              seed=seed);\n",
    "\n",
    "  results[\"train_loss\"].append(train_loss)\n",
    "  results[\"train_accu\"].append(train_accu)\n",
    "  results[\"test_loss\"].append(test_loss)\n",
    "  results[\"test_accu\"].append(test_accu)\n",
    "  \n",
    "end_adan = time.perf_counter()\n",
    "total_time_adan = end_adan - start_adan\n",
    "print(f\"Total Runtime: {total_time_adan / 60: .2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
